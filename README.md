# üß¨ MTB-Illumina-WGS-Analysis  

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)  
![Conda](https://img.shields.io/badge/Conda-ready-blue)  
![GitHub last commit](https://img.shields.io/github/last-commit/betselotz/MTB-Illumina-WGS-Analysis)  

Bioinformatics workflow for analyzing *Mycobacterium tuberculosis* whole-genome sequences (WGS) generated by Illumina and available on NCBI.  

---  

Written by: [**Betselot Zerihun Ayano**](https://github.com/betselotz)  

---  

## Introduction ‚Äì *Mycobacterium tuberculosis* Complex (MTBC)  

- *Mycobacterium tuberculosis* complex (MTBC) comprises closely related bacterial species causing tuberculosis (TB) in humans and animals.  
- TB remains a leading cause of infectious disease morbidity and mortality worldwide.  
- MTBC is characterized by a **highly clonal genome**, with limited horizontal gene transfer, making genomic analysis crucial for understanding evolution, transmission, and drug resistance.  
- **Genome size:** ~4.4 Mbp  
- **GC content:** ~65%  
- Comparative genomics reveals **lineage-specific SNPs**, large sequence polymorphisms (LSPs), and regions of difference (RDs) that are useful for strain typing and epidemiology.  
- Whole-genome sequencing (WGS) enables:  
  - Identification of **drug-resistance mutations** in key genes (e.g., *rpoB*, *katG*, *inhA*).  
  - Phylogenetic analysis to trace transmission chains and outbreak sources.  
  - Detection of genomic diversity and microevolution within hosts and populations.  
- MTBC genomics informs public health strategies, including **molecular surveillance, outbreak investigation, and personalized TB treatment**.  

---  

### MTBC Species  

The *Mycobacterium tuberculosis* complex (MTBC) includes several closely related species:  

- *Mycobacterium tuberculosis* ‚Äì the main human pathogen  
- *Mycobacterium bovis* ‚Äì primarily infects cattle, can cause zoonotic TB in humans  
- *Mycobacterium africanum* ‚Äì restricted to West Africa, causes human TB  
- *Mycobacterium canettii* ‚Äì rare, mostly in East Africa, ancestral-like strains  
- *Mycobacterium microti* ‚Äì primarily infects voles, occasionally humans  
- *Mycobacterium pinnipedii* ‚Äì infects seals, rarely humans  
- *Mycobacterium caprae* ‚Äì mainly infects goats, occasionally humans  

---  

### *M. tuberculosis* Lineages  

Genomic studies have identified **10 major lineages** based on SNPs and phylogeny:  

1. **Lineage 1 (Indo-Oceanic)** ‚Äì East Africa, India, Southeast Asia  
2. **Lineage 2 (East-Asian / Beijing)** ‚Äì East Asia, associated with drug resistance  
3. **Lineage 3 (East-African-Indian)** ‚Äì South Asia, East Africa  
4. **Lineage 4 (Euro-American)** ‚Äì Worldwide, highly prevalent  
5. **Lineage 5 (West African 1 / *M. africanum*)** ‚Äì West Africa  
6. **Lineage 6 (West African 2 / *M. africanum*)** ‚Äì West Africa  
7. **Lineage 7** ‚Äì restricted to Ethiopia, ancient lineage with unique genomic markers  
8. **Lineage 8** ‚Äì discovered in Rwanda and Uganda, deep-branching, highly ancestral  
9. **Lineage 9** ‚Äì identified in East Africa, distinct SNP profile, rare  
10. **Lineage 10** ‚Äì recently reported, limited information, potentially from East/Central Africa  

> Understanding MTBC species and lineages is critical for **epidemiology, phylogenetics, and drug-resistance surveillance**.  

---  

### *Mycobacterium tuberculosis* Sublineages  

*M. tuberculosis* lineages are further divided into **sublineages** based on phylogenetic SNP markers:  

- **Lineage 1 (Indo-Oceanic):** L1.1, L1.2  
- **Lineage 2 (East-Asian / Beijing):** L2.1 (Proto-Beijing), L2.2 (Modern Beijing)  
- **Lineage 3 (East-African-Indian):** L3.1, L3.2  
- **Lineage 4 (Euro-American):** L4.1, L4.2, L4.3, L4.4, L4.5, etc.  
- **Lineage 5‚Äì10:** Mainly restricted to Africa, with limited sublineage descriptions due to low prevalence and recent discovery  

> Sublineages help in **tracking transmission chains, outbreak sources, and population structure**.  

---  

### *Mycobacterium tuberculosis* Drug-Resistance Types  

| Abbreviation | Full Name | Description |
|-------------|-----------|-------------|
| **HR-TB** | Isoniazid-Resistant TB | Resistant to **isoniazid** only |
| **RR-TB** | Rifampicin-Resistant TB | Resistant to **rifampicin**, with or without resistance to other drugs |
| **MDR-TB** | Multidrug-Resistant TB | Resistant to **both isoniazid and rifampicin** |
| **Pre-XDR-TB** | Pre-Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** or **second-line injectable** |
| **XDR-TB** | Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** + **injectable** |
| **Other** | Other Drug-Resistance Patterns | Includes mono-resistant TB, poly-resistant TB, and rare profiles |  

---  

### Drug-Resistance Mutations  

- **Rifampicin:** *rpoB* mutations (e.g., S450L)  
- **Isoniazid:** *katG* (S315T), *inhA* promoter mutations  
- **Ethambutol:** *embB* (M306V/I)  
- **Pyrazinamide:** *pncA* mutations  
- **Fluoroquinolones:** *gyrA/gyrB* mutations  

> Mutations are catalogued and updated by the **World Health Organization (WHO)** for standardized drug-resistance interpretation.  

- **Catalogue of mutations in Mycobacterium tuberculosis complex and their association with drug resistance, 2nd ed (2023):**  
  [WHO TB Mutation Catalogue 2023](https://github.com/GTB-tbsequencing/mutation-catalogue-2023/tree/main/Final%20Result%20Files)  

---  

## Bioinformatics Workflow  

The pipeline includes the following steps:  

1. **üì• Download raw FASTQ from NCBI SRA**  
2. **üßπ Quality control & trimming with `fastp`**  
3. **üéØ Mapping & variant calling with `Snippy`**  
4. **üß¨ Resistance & lineage typing with `tb-profiler`**  
5. **üß™ Variant filtering with `tb_variant_filter`**  
6. **üìä Summary QC with `MultiQC`**  
7. **üå≥ Phylogenetic tree construction & downstream analysis**  

```mermaid
flowchart TD
    A[üì• Download raw FASTQ] --> B[üßπ fastp - QC & trimming]
    B --> C[üéØ Snippy - mapping & variant calling]
    C --> D[üß¨ tb-profiler - lineage & resistance]
    D --> E[üß™ tb_variant_filter - SNP filtering]
    E --> F[üìä MultiQC - QC summary]
    F --> G[üå≥ Phylogenetic tree construction]



### Download Data from NCBI and EBI


### üì• Download Data from NCBI and EBI

---

#### Using SRA Explorer for few size samples 

1. Go to **[SRA Explorer](https://sra-explorer.info/#)**  
2. Search for:  Bioproject number  example PRJNA1201357
3. set Max Results into 500 

4. In the search results, check the all boxes:  
- ‚úÖ *WGS of micobacterium tuberculosis*  
- ‚úÖ *Add to collection*  
- ‚úÖ * go to data saved*  
- ‚úÖ *Bash script for downloading FastQ files*  

4. Download the generated Bash script (e.g., `sra_download.sh`)  
5. Run the script to download the FASTQ file(s):  

##  Get the Run Accessions

###  1. Get all ERR run IDs from ENA
```bash
curl -s "https://www.ebi.ac.uk/ena/portal/api/filereport?accession=PRJEB3334&result=read_run&fields=run_accession" | tail -n +2 > runs.txt
```
# Fetch all SRR run accessions for the BioProject
```bash
esearch -db sra -query PRJNA1201357 | efetch -format runinfo | cut -d',' -f1 | grep ^SRR > runs.txt
```
### Download using fastq-dump & Compress

#### Once runs.txt is ready:

```bash
nano download_sra.sh
```
```bash
#!/bin/bash
set -euo pipefail

# Number of threads for fasterq-dump / pigz
THREADS=4

# Directory for FASTQ files
OUTDIR="fastq_files"
mkdir -p "$OUTDIR"

# File containing run accessions
RUNS="SRR_Acc_List.txt"

# Directory where SRA files are stored by default
SRADIR=~/ncbi/public/sra

# Loop over each accession
while read -r ACC; do
    echo "==> Processing $ACC ..."

    # Skip download if SRA file already exists
    if [ -f "$SRADIR/$ACC.sra" ]; then
        echo "SRA file $ACC.sra already exists, skipping prefetch."
    else
        echo "Downloading $ACC.sra ..."
        prefetch --max-size 100G "$ACC"
    fi

    # Skip conversion if FASTQ already exists
    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        echo "FASTQ for $ACC already exists, skipping fasterq-dump."
    else
        # Convert to FASTQ (split paired-end reads)
        fasterq-dump "$ACC" --split-files -e "$THREADS" -O "$OUTDIR"

        # Compress FASTQ files
        if command -v pigz &> /dev/null; then
            pigz -p "$THREADS" "$OUTDIR"/${ACC}*.fastq
        else
            gzip "$OUTDIR"/${ACC}*.fastq
        fi
    fi

    # Optional: remove SRA file after successful FASTQ creation
    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        rm -f "$SRADIR/$ACC.sra"
    fi

    echo "==> Completed $ACC"
    echo
done < "$RUNS"

echo "üéâ All downloads and conversions completed!"
```

Run your script in the background with nohup

Save all output and errors to run.log.

Keep it running even if you close the terminal.
```bash
nohup bash download_sra.sh > run.log 2>&1 &
```
To check progress:
```bash
tail -f run.log
```
#To see if it‚Äôs still running:
```bash
ps aux | grep download_sra.sh
```

Checking if all of my fastq.gz file will pair correctly with out problem
Change directory into the working directory
```bash
cd ~/Genomics_project/TB/fastq_data/f_invio
```

1.  Open nano to create a new script
```bash
nano check_fastq_pairs.sh
```
2. Paste this inside nano
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"

# Only cd if not already in raw_data
if [[ "$(basename "$PWD")" != "raw_data" ]]; then
    cd "$INDIR" || { echo "‚ùå raw_data directory not found"; exit 1; }
fi

echo "üîç Checking FASTQ pairings in $PWD ..."

MISSING=false
PAIRED_COUNT=0
TOTAL_COUNT=0

# Loop over common R1 patterns
for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    TOTAL_COUNT=$((TOTAL_COUNT+1))

    # Extract sample name by removing R1/1/001 suffix
    SAMPLE=${R1%_1.fastq.gz}
    SAMPLE=${SAMPLE%_R1.fastq.gz}
    SAMPLE=${SAMPLE%_R1_*.fastq.gz}
    SAMPLE=${SAMPLE%_001.fastq.gz}
    SAMPLE=${SAMPLE%_R1_001.fastq.gz}

    # Look for corresponding R2
    if [[ -f "${SAMPLE}_2.fastq.gz" || -f "${SAMPLE}_R2.fastq.gz" || -f "${SAMPLE}_R2_*.fastq.gz" || -f "${SAMPLE}_002.fastq.gz" ]]; then
        echo "‚úÖ $SAMPLE ‚Äî paired"
        PAIRED_COUNT=$((PAIRED_COUNT+1))
    else
        echo "‚ùå $SAMPLE ‚Äî missing R2 file"
        MISSING=true
    fi
done

echo -e "\nTotal samples checked: $TOTAL_COUNT"
echo "Correctly paired samples: $PAIRED_COUNT"

if [ "$MISSING" = true ]; then
    echo "‚ö† Some samples are missing pairs. Fix before running fastp."
else
    echo "‚úÖ All FASTQ files are correctly paired."
fi
```


Make the script executable
```bash
chmod +x check_fastq_pairs.sh
```
# 5Ô∏è‚É£ Run the script
```bash
./check_fastq_pairs.sh
```

# minimum, maximum, and average read length for gzipped FASTQ files using awk

### calculating min, max,avg read lengths for both reads
 We can loop over all _1.fastq.gz files, find the corresponding _2.fastq.gz, calculate min, max, avg read lengths for both, and save the results as a CSV file.

1Ô∏è‚É£ Open nano to create a new script
```bash
nano fastq_read_length_summary.sh
```

2Ô∏è‚É£ Paste the following code into nano
```bash
#!/bin/bash
FASTQ_DIR="."                          # Current directory with FASTQ files
OUTDIR="read_length_summary"           # New output directory
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

# Create output directory if it doesn't exist
mkdir -p "$OUTDIR"

# CSV header
echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

# Loop over all _1.fastq.gz files
for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.trim.fastq.gz)
    R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        # Function to calculate min, max, avg read length
        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
        }

        # Calculate stats for R1 and R2
        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        # Append results to CSV
        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "‚ö† Missing R2 for $SAMPLE, skipping."
    fi
done

echo "‚úÖ Read length summary saved to $OUTPUT_CSV"

echo "‚úÖ Read length summary saved to $OUTPUT_CSV"
```
3Ô∏è‚É£ Save and exit nano

    Press Ctrl + O ‚Üí Enter (to write the file)

    Press Ctrl + X ‚Üí Exit nano

4Ô∏è‚É£ Make the script executable
```bash
chmod +x fastq_read_length_summary.sh
```
5Ô∏è‚É£ Run the script
```bash
./fastq_read_length_summary.sh
```

Visualize the readlength in some way


# FASTP 

1. Open nano and create the script
```bash
nano run_fastp.sh
```
2. Paste this inside nano
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_50"
mkdir -p "$OUTDIR"

SAMPLES=()

# Detect R1 files under common naming schemes
for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz "$INDIR"/*_001.fastq.gz "$INDIR"/*_R1_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1")
    SAMPLE=${SAMPLE%%_1.fastq.gz}
    SAMPLE=${SAMPLE%%_R1.fastq.gz}
    SAMPLE=${SAMPLE%%_001.fastq.gz}
    SAMPLE=${SAMPLE%%_R1_001.fastq.gz}

    # Find R2 match
    if   [[ -f "$INDIR/${SAMPLE}_2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_002.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_002.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2_001.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2_001.fastq.gz"
    else
        echo "‚ö† No R2 file found for $SAMPLE ‚Äî skipping."
        continue
    fi

    # Skip if already processed
    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "‚è© Skipping $SAMPLE (already processed)."
        continue
    fi

    SAMPLES+=("$SAMPLE,$R1,$R2")
done

if [[ ${#SAMPLES[@]} -eq 0 ]]; then
    echo "‚ùå No paired FASTQ files found in $INDIR"
    exit 1
fi

THREADS=$(nproc)
FASTP_THREADS=$(( THREADS / 2 ))

run_fastp() {
    SAMPLE=$1
    R1=$2
    R2=$3
    echo "‚úÖ Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 50 \
        --qualified_quality_phred 20 \
        --detect_adapter_for_pe \
        --thread $FASTP_THREADS \
        \
        # 1. UMI preprocessing (if UMI data) ‚Üí add: --umi
        # 2. Global trim front ‚Üí add: --trim_front1, --trim_front2
        # 3. Global trim tail ‚Üí add: --trim_tail1, --trim_tail2
        # 4. Quality prune 5' ‚Üí add: --cut_front
        # 5. Sliding window pruning ‚Üí add: --cut_right
        # 6. Quality prune 3' ‚Üí add: --cut_tail
        # 7. PolyG trimming (default for NovaSeq/NextSeq)
        # 8. Adapter trimming by overlap (default for PE data)
        # 9. Adapter trimming by sequence ‚Üí add: --adapter_sequence <seq>
        # 10. PolyX trimming ‚Üí add: --trim_poly_x
        # 11. Max length (if really needed) ‚Üí add: --max_len1, --max_len2 \
        &> "$OUTDIR/${SAMPLE}_fastp.log"
}

export -f run_fastp
export OUTDIR FASTP_THREADS

printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}

echo "üéâ Completed fastp for $(ls "$OUTDIR"/*_fastp.json | wc -l) samples."

```
3. Save & exit nano

    Press CTRL+O, Enter (save)

    Press CTRL+X (exit)
 4. Make the script executable
```bash
chmod +x run_fastp.sh
```
 5. Activate your conda env and run
```bash
conda activate fastp_env
./run_fastp.sh
```
# MultiQC
run MultiQC on all the files inside fastp_results directory, saving the report into a folder called multiqc_output.
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_50"
OUTPUT_DIR="multiqc_output"

mkdir -p "$OUTPUT_DIR"

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"
```

# SNIPPY
```bash
1. Open nano to create the script
```
nano run_snippy.sh

2. Paste this inside nano:
```bash
#!/bin/bash
set -euo pipefail

# ======== CONFIG ========
REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_70"
OUTDIR="snippy_results"
THREADS=8         # threads *inside* each Snippy job
BWA_THREADS=30
JOBS=4            # how many samples to run at once
# ========================

mkdir -p "$OUTDIR"

# Function: run snippy for a single sample
run_snippy_sample() {
    SAMPLE="$1"
    R1="${FASTP_DIR}/${SAMPLE}_1.trim.fastq.gz"
    R2="${FASTP_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R1" && -f "$R2" ]]; then
        echo "Running snippy on sample: $SAMPLE"
        TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
        mkdir -p "$TMP_DIR"

        snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
               --R1 "$R1" --R2 "$R2" --force --bwaopt "-T $BWA_THREADS"

        # Rename and move outputs
        for f in "$TMP_DIR"/*; do
            base=$(basename "$f")
            case "$base" in
                snps.vcf) newname="${SAMPLE}.snps.vcf" ;;
                snps.tab) newname="${SAMPLE}.snps.tab" ;;
                consensus.fa) newname="${SAMPLE}.consensus.fa" ;;
                *.bam) newname="${SAMPLE}.bam" ;;
                *.bam.bai) newname="${SAMPLE}.bam.bai" ;;
                *) continue ;;
            esac
            mv "$f" "${OUTDIR}/${newname}"
        done

        rm -rf "$TMP_DIR"

        if [[ -f "${OUTDIR}/${SAMPLE}.snps.vcf" ]]; then
            echo "‚úÖ VCF generated for $SAMPLE"
        else
            echo "‚ö† No VCF produced for $SAMPLE"
        fi
    else
        echo "‚ö† Missing R1/R2 for $SAMPLE"
    fi
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

# Run samples in parallel
ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' \
    | parallel -j "$JOBS" run_snippy_sample {}

# Step 2: Verification
echo "Verifying completeness..."
ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' | sort > fastq_samples.txt
ls "${OUTDIR}"/*.snps.vcf 2>/dev/null \
    | sed 's|.*/||; s/\.snps\.vcf//' | sort > snippy_samples.txt

echo "FASTQ pairs count: $(wc -l < fastq_samples.txt)"
echo "Snippy outputs count: $(wc -l < snippy_samples.txt)"

if diff fastq_samples.txt snippy_samples.txt >/dev/null; then
    echo "‚úÖ All FASTQ pairs have corresponding Snippy results."
else
    echo "‚ö† Missing samples detected:"
    diff fastq_samples.txt snippy_samples.txt || true
fi

rm -f fastq_samples.txt snippy_samples.txt

echo "üéØ All steps completed!"
echo "Snippy results are in: ${OUTDIR}/"

```
 3. Save and exit nano

    Press Ctrl + O, then Enter

    Press Ctrl + X

4. Make it executable:
```bash
chmod +x run_snippy.sh
```
 5. Activate your Snippy environment and run:
```bash
conda activate snippy_env
./run_snippy.sh
```

# qualimap BAM QC

 1. Open nano to create the script
```bash
nano run_qualimap.sh
```

2. Paste this inside nano:
```bash
#!/bin/bash
set -euo pipefail

# Directory with snippy BAM files
SNIPPY_DIR="all_bams"

# Output directory for qualimap reports
QUALIMAP_OUT="qualimap_reports"
mkdir -p "$QUALIMAP_OUT"

for bam in "$SNIPPY_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Running Qualimap BAM QC for sample: $sample"

    outdir="${QUALIMAP_OUT}/${sample}"
    mkdir -p "$outdir"

    # Run Qualimap (both HTML and PDF for completeness, lowercase format name)
    qualimap bamqc \
        -bam "$bam" \
        -outdir "$outdir" \
        -outformat pdf:html \
        --java-mem-size=4G

    # MultiQC will look inside "$outdir" for raw_data folder
done
```
3. Save and exit nano

    Press Ctrl + O, then Enter

    Press Ctrl + X
4. Make it executable:
```bash
chmod +x run_qualimap.sh
```
5. Activate your Snippy environment and run:
```bash
conda activate qualimap_env
./run_qualimap.sh
```

# MultiQC 
```bash
multiqc . -o multiqc_report
```
# tb variant filter 

1Ô∏è‚É£ Create or edit the script
```bash
nano run_tb_variant_filter.sh
```
2Ô∏è‚É£ Paste this code
```bash
#!/bin/bash

# Activate tb_variant_filter environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate tb_variant_filter_env

# Current directory
CURDIR=$(pwd)

# Output directory
OUTDIR="$CURDIR/tb_variant_filter_results"
mkdir -p "$OUTDIR"

# Loop through all VCF files in snippy_results
for vcf in "$CURDIR/snippy_results"/*.vcf; do
    sample=$(basename "$vcf")
    echo "Filtering $vcf ..."
    tb_variant_filter "$vcf" "$OUTDIR/${sample%.vcf}.filtered.vcf"
done

echo "All VCFs filtered and saved in $OUTDIR"


echo "All VCFs filtered and saved in $OUTDIR"
```
3Ô∏è‚É£ Make it executable
```bash
chmod +x run_tb_variant_filter.sh
```
4Ô∏è‚É£ Run the script
```bash
conda activate tb_variant_filter_env
./run_tb_variant_filter.sh
```

# TBPROFILER FROM BAM FILE

1Ô∏è‚É£ Create or edit the script
```bash
nano run_tbprofiler_on_snippy_bams.sh
```
2Ô∏è‚É£ Paste this code
```bash
#!/bin/bash

set -euo pipefail

BAM_DIR="snippy_results"

for bam in "$BAM_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Processing sample: $sample"

    # Run tb-profiler profile with spoligotype and TXT output
    tb-profiler profile \
        --bam "$bam" \
        --prefix "$sample" \
        --txt
done
```
3Ô∏è‚É£ Make it executable
```bash
chmod +x run_tbprofiler_on_snippy_bams.sh
```
4Ô∏è‚É£ Run the script
```bash
conda activate tbprofiler_env
./run_tbprofiler_on_snippy_bams.sh
```

# bcftools concensus generation

###  Compress and index each filtered VCF
```bash
for vcf in tb_variant_filter_results/*.vcf; do
    bgzip -c "$vcf" > "${vcf}.gz"
    bcftools index "${vcf}.gz"
done
```
### Create a script to generate consensus sequences:
```bash
nano generate_consensus_all.sh
```
 Paste this:
```bash
#!/bin/bash

# Activate bcftools environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate tb_consensus_env

# Current directory
CURDIR=$(pwd)

# Directories
VCFDIR="$CURDIR/tb_variant_filter_results"
OUTDIR="$CURDIR/consensus_sequences"
mkdir -p "$OUTDIR"

# Loop through all filtered VCFs
for vcf in "$VCFDIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)
    echo "Processing $sample ..."
    # Compress VCF with bgzip
    bgzip -c "$vcf" > "$vcf.gz"
    # Index the compressed VCF
    bcftools index "$vcf.gz"
    # Generate consensus FASTA
    bcftools consensus -f "$CURDIR/H37Rv.fasta" "$vcf.gz" \
        | sed "1s/.*/>$sample/" \
        > "$OUTDIR/${sample}.consensus.fasta"

    echo "$sample consensus generated with sample label."
done

echo "All consensus sequences saved in $OUTDIR with sample-based headers."
```

4Ô∏è‚É£ Make the script executable
```bash
chmod +x generate_consensus_all.sh
```

5Ô∏è‚É£ Run the script
```bash
conda activate tb_consensus_env
./generate_consensus_all.sh
```

check the length of each consensus FASTA using seqkit, bioawk, or even awk/grep. 
1Ô∏è‚É£ Using grep and wc
```bash
for f in consensus_sequences/*.fasta; do
    sample=$(basename "$f")
    # Remove header lines and count remaining characters
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample : $length bp"
done
```

# MAFFT
MAFFT v7.490 does not accept multiple input files on the command line. It expects a single FASTA file containing all sequences. Passing multiple paths causes it to think the filenames are options.

The correct approach is to merge all consensus FASTAs into one file, then run MAFFT on that single file.
Step 1: Merge all consensus FASTAs
```bash
cat consensus_sequences/*.fasta > consensus_sequences/all_consensus.fasta
```
Step 2: Run MAFFT
```bash
mafft --auto --reorder --thread -1 consensus_sequences/all_consensus.fasta > consensus_sequences/aligned_consensus.fasta
```
 Step 3: Verify
```bash
head consensus_sequences/aligned_consensus.fasta
```












üìñ References

WHO. Catalogue of mutations in MTBC and their association with drug resistance, 2nd ed, 2023.

Ngabonziza JCS, et al. A sister lineage of the Mycobacterium tuberculosis complex discovered in Rwanda. Nat Commun, 2020 (Lineage 8).

Coscolla M, et al. Phylogenomics of Mycobacterium africanum lineage 9. Nat Commun, 2021 (Lineage 9).

Phelan J, et al. Whole-genome sequencing for TB: current standards and challenges. Nat Rev Microbiol.

Li H, et al. Fast and accurate short read alignment with Burrows‚ÄìWheeler transform. Bioinformatics
