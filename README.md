# 🧬 MTB-Illumina-WGS-Analysis  

![GitHub last commit](https://img.shields.io/github/last-commit/betselotz/MTB-Illumina-WGS-Analysis)  

Bioinformatics workflow for analyzing *Mycobacterium tuberculosis* whole-genome sequences (WGS) generated by Illumina and available on NCBI.  

---  

Written by: [**Betselot Zerihun Ayano**](https://github.com/betselotz)  

---  

## Introduction – *Mycobacterium tuberculosis* Complex (MTBC)  

- *Mycobacterium tuberculosis* complex (MTBC) comprises closely related bacterial species causing tuberculosis (TB) in humans and animals.  
- TB remains a leading cause of infectious disease morbidity and mortality worldwide.  
- MTBC is characterized by a **highly clonal genome**, with limited horizontal gene transfer, making genomic analysis crucial for understanding evolution, transmission, and drug resistance.  
- **Genome size:** ~4.4 Mbp  
- **GC content:** ~65%  
- Comparative genomics reveals **lineage-specific SNPs**, large sequence polymorphisms (LSPs), and regions of difference (RDs) that are useful for strain typing and epidemiology.  
- Whole-genome sequencing (WGS) enables:  
  - Identification of **drug-resistance mutations** in key genes (e.g., *rpoB*, *katG*, *inhA*).  
  - Phylogenetic analysis to trace transmission chains and outbreak sources.  
  - Detection of genomic diversity and microevolution within hosts and populations.  
- MTBC genomics informs public health strategies, including **molecular surveillance, outbreak investigation, and personalized TB treatment**.  

---  

### MTBC Species  

The *Mycobacterium tuberculosis* complex (MTBC) includes several closely related species:  

- *Mycobacterium tuberculosis* – the main human pathogen  
- *Mycobacterium bovis* – primarily infects cattle, can cause zoonotic TB in humans  
- *Mycobacterium africanum* – restricted to West Africa, causes human TB  
- *Mycobacterium canettii* – rare, mostly in East Africa, ancestral-like strains  
- *Mycobacterium microti* – primarily infects voles, occasionally humans  
- *Mycobacterium pinnipedii* – infects seals, rarely humans  
- *Mycobacterium caprae* – mainly infects goats, occasionally humans  

---  

### *M. tuberculosis* Lineages  

Genomic studies have identified **10 major lineages** based on SNPs and phylogeny:  

1. **Lineage 1 (Indo-Oceanic)** – East Africa, India, Southeast Asia  
2. **Lineage 2 (East-Asian / Beijing)** – East Asia, associated with drug resistance  
3. **Lineage 3 (East-African-Indian)** – South Asia, East Africa  
4. **Lineage 4 (Euro-American)** – Worldwide, highly prevalent  
5. **Lineage 5 (West African 1 / *M. africanum*)** – West Africa  
6. **Lineage 6 (West African 2 / *M. africanum*)** – West Africa  
7. **Lineage 7** – restricted to Ethiopia, ancient lineage with unique genomic markers  
8. **Lineage 8** – discovered in Rwanda and Uganda, deep-branching, highly ancestral  
9. **Lineage 9** – identified in East Africa, distinct SNP profile, rare  
10. **Lineage 10** – recently reported, limited information, potentially from East/Central Africa  

> Understanding MTBC species and lineages is critical for **epidemiology, phylogenetics, and drug-resistance surveillance**.  

---  

### *Mycobacterium tuberculosis* Sublineages  

*M. tuberculosis* lineages are further divided into **sublineages** based on phylogenetic SNP markers:  

- **Lineage 1 (Indo-Oceanic):** L1.1, L1.2  
- **Lineage 2 (East-Asian / Beijing):** L2.1 (Proto-Beijing), L2.2 (Modern Beijing)  
- **Lineage 3 (East-African-Indian):** L3.1, L3.2  
- **Lineage 4 (Euro-American):** L4.1, L4.2, L4.3, L4.4, L4.5, etc.  
- **Lineage 5–10:** Mainly restricted to Africa, with limited sublineage descriptions due to low prevalence and recent discovery  

> Sublineages help in **tracking transmission chains, outbreak sources, and population structure**.  

---  

### *Mycobacterium tuberculosis* Drug-Resistance Types  

| Abbreviation | Full Name | Description |
|-------------|-----------|-------------|
| **HR-TB** | Isoniazid-Resistant TB | Resistant to **isoniazid** only |
| **RR-TB** | Rifampicin-Resistant TB | Resistant to **rifampicin**, with or without resistance to other drugs |
| **MDR-TB** | Multidrug-Resistant TB | Resistant to **both isoniazid and rifampicin** |
| **Pre-XDR-TB** | Pre-Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** or **second-line injectable** |
| **XDR-TB** | Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** + **injectable** |
| **Other** | Other Drug-Resistance Patterns | Includes mono-resistant TB, poly-resistant TB, and rare profiles |  

---  

### Drug-Resistance Mutations  

- **Rifampicin:** *rpoB* mutations (e.g., S450L)  
- **Isoniazid:** *katG* (S315T), *inhA* promoter mutations  
- **Ethambutol:** *embB* (M306V/I)  
- **Pyrazinamide:** *pncA* mutations  
- **Fluoroquinolones:** *gyrA/gyrB* mutations  

> Mutations are catalogued and updated by the **World Health Organization (WHO)** for standardized drug-resistance interpretation.  

- **Catalogue of mutations in Mycobacterium tuberculosis complex and their association with drug resistance, 2nd ed (2023):**  
  [WHO TB Mutation Catalogue 2023](https://github.com/GTB-tbsequencing/mutation-catalogue-2023/tree/main/Final%20Result%20Files)  

---  

## Bioinformatics Workflow  

The pipeline includes the following steps:  

1. **📥 Download raw FASTQ from NCBI SRA**  
2. **🧹 Quality control & trimming with `fastp`**  
3. **🎯 Mapping & variant calling with `Snippy`**  
4. **🧬 Resistance & lineage typing with `tb-profiler`**  
5. **🧪 Variant filtering with `tb_variant_filter`**  
6. **📊 Summary QC with `MultiQC`**  
7. **🌳 Phylogenetic tree construction & downstream analysis**  


### Download Data from NCBI and ENA

Raw sequencing data for *Mycobacterium tuberculosis* can be accessed from public repositories such as **NCBI Sequence Read Archive (SRA)** and **European Nucleotide Archive (ENA) / EBI**. These repositories provide high-throughput sequencing datasets submitted by researchers worldwide.  

Key points:  
- **NCBI SRA** (https://www.ncbi.nlm.nih.gov/sra): Provides raw FASTQ or SRA files that can be downloaded using the `sra-tools` (`prefetch`, `fasterq-dump`) or via FTP.  
- **EBI / ENA** (https://www.ebi.ac.uk/ena/browser/home): Offers raw sequencing files (FASTQ) and metadata for projects submitted to Europe’s archive. Supports both browser-based and command-line downloads.  
- Using accession lists (BioProject, Run IDs), data can be downloaded in bulk for large-scale analysis.  
- Metadata includes sample source, sequencing platform, and study information, which is essential for downstream analysis and reproducibility.  

> **Tip:** Always check the sequencing platform and read layout (paired-end vs single-end) to ensure correct processing in your pipeline.  

There are multiple ways to download *Mycobacterium tuberculosis* sequencing data, depending on the dataset size and source. Below are two practical methods.

---

#### Method 1: Using **SRA Explorer** (for small datasets)

1. Go to **[SRA Explorer](https://sra-explorer.info/#)**  
2. Search for a **BioProject number**, e.g., `PRJNA1201357`  
3. Set **Max Results** to `500`  

4. In the search results, check the following options:  
- ✅ *WGS of Mycobacterium tuberculosis*  
- ✅ *Add to collection*  
- ✅ *Go to data saved*  
- ✅ *Bash script for downloading FASTQ files*  

5. Download the generated Bash script (e.g., `sra_download.sh`)  
6. Run the script to download the FASTQ files:  

```bash
bash sra_download.sh
```
> **Tip:** Tip: This method is user-friendly and ideal for small projects (<500 samples). For large-scale datasets, use the command-line method below.

#### Method 2: Using SRA Toolkit / ENA Run Accessions (for large datasets)
##### A. Get all run accessions from ENA
```bash
curl -s "https://www.ebi.ac.uk/ena/portal/api/filereport?accession=PRJEB3334&result=read_run&fields=run_accession" | tail -n +2 > runs.txt
```
#####  B. Get SRR run accessions from NCBI SRA
```bash
esearch -db sra -query PRJNA1201357 | efetch -format runinfo | cut -d',' -f1 | grep ^SRR > runs.txt
```
Once `runs.txt` is ready, create a download script:
```bash
nano download_sra.sh
```
Paste the following into the file:

```bash
#!/bin/bash
set -euo pipefail

THREADS=4
OUTDIR="fastq_files"
mkdir -p "$OUTDIR"
RUNS="SRR_Acc_List.txt"
SRADIR=~/ncbi/public/sra

while read -r ACC; do
    echo "==> Processing $ACC ..."

    if [ -f "$SRADIR/$ACC.sra" ]; then
        echo "SRA file $ACC.sra already exists, skipping prefetch."
    else
        echo "Downloading $ACC.sra ..."
        prefetch --max-size 100G "$ACC"
    fi

    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        echo "FASTQ for $ACC already exists, skipping fasterq-dump."
    else
        fasterq-dump "$ACC" --split-files -e "$THREADS" -O "$OUTDIR"

        if command -v pigz &> /dev/null; then
            pigz -p "$THREADS" "$OUTDIR"/${ACC}*.fastq
        else
            gzip "$OUTDIR"/${ACC}*.fastq
        fi
    fi

    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        rm -f "$SRADIR/$ACC.sra"
    fi

    echo "==> Completed $ACC"
    echo
done < "$RUNS"

echo "🎉 All downloads and conversions completed!"

```

<details>
<summary>📖 Explanation of the FASTQ Download Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash shell.

- `set -euo pipefail`  
  Makes the script safer by exiting on errors, unset variables, or failed pipelines.

- `THREADS=4`  
  Number of CPU threads used by `fasterq-dump` and `pigz`.

- `OUTDIR="fastq_files"`  
  Directory where the FASTQ files will be saved.

- `RUNS="SRR_Acc_List.txt"`  
  File containing the list of SRR accession numbers to download.

- `SRADIR=~/ncbi/public/sra`  
  Default folder where `prefetch` stores downloaded `.sra` files.

- `while read -r ACC; do ... done < "$RUNS"`  
  Loops through each accession in the list.

- `if [ -f "$SRADIR/$ACC.sra" ]; then ... fi`  
  Skips downloading if the SRA file already exists, otherwise downloads with `prefetch`.

- `if ls "$OUTDIR"/${ACC}*.fastq.gz ...`  
  Skips FASTQ conversion if files already exist.

- `fasterq-dump "$ACC" --split-files -e "$THREADS" -O "$OUTDIR"`  
  Converts `.sra` to paired-end FASTQ files using multiple threads.

- Compression step:  
  - Uses `pigz` if available (multi-threaded gzip), else falls back to `gzip`.

- `rm -f "$SRADIR/$ACC.sra"`  
  Deletes the original `.sra` file after successful FASTQ creation.

- `echo` statements  
  Provide progress updates for each accession.

</details>

> **Tips for large-scale projects:**
> 
> - Adjust `THREADS` according to your CPU cores for faster downloads.
> - Ensure enough disk space; SRA files can be large (~10–50 GB per sample).
> - Use `pigz` for parallel compression to speed up gzip operations.
> - This method is reproducible and scalable for **hundreds to thousands of samples**.

### Running the Download Script in the Background

To run your script in the background and keep it running even if you close the terminal, use `nohup`. All output and errors will be saved to `run.log`.

```bash
nohup bash download_sra.sh > run.log 2>&1 &
```
Check the progress of your download
```bash
tail -f run.log
```
Check if the script is still running
```bash
ps aux | grep download_sra.sh
```


# 🧬 FASTQ Pair Checking and Download Workflow for MTB WGS

This guide demonstrates how to:

1. Check that all your FASTQ files are correctly paired.
2. Download *Mycobacterium tuberculosis* raw sequencing data using **SRA Explorer** or **SRA Toolkit / ENA**.
3. Run the download scripts in the background for large datasets.

---

## Initial Notes

Before starting, make sure you have:

- `sra-tools` installed (`prefetch`, `fasterq-dump`, `esearch`, `efetch`)
- `pigz` for parallel compression (optional, but recommended for large files)
- Enough disk space for raw and compressed FASTQ files
- Access to your working directory for downloads

---

## 1️⃣ Checking FASTQ Pairing

Ensure all FASTQ files are correctly paired before running any trimming or analysis.

##### Step 1: Change directory into your working directory
```bash
cd ~/Genomics_project/TB/fastq_data/f_invio
```
##### Step 2: Create the script
```bash
nano check_fastq_pairs.sh
```
##### Step 3: Paste the following into `check_fastq_pairs.sh`
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"

if [[ "$(basename "$PWD")" != "raw_data" ]]; then
    cd "$INDIR" || { echo "❌ raw_data directory not found"; exit 1; }
fi

echo "🔍 Checking FASTQ pairings in $PWD ..."

MISSING=false
PAIRED_COUNT=0
TOTAL_COUNT=0

for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    TOTAL_COUNT=$((TOTAL_COUNT+1))

    SAMPLE=${R1%_1.fastq.gz}
    SAMPLE=${SAMPLE%_R1.fastq.gz}
    SAMPLE=${SAMPLE%_R1_*.fastq.gz}
    SAMPLE=${SAMPLE%_001.fastq.gz}
    SAMPLE=${SAMPLE%_R1_001.fastq.gz}

    if [[ -f "${SAMPLE}_2.fastq.gz" || -f "${SAMPLE}_R2.fastq.gz" || -f "${SAMPLE}_R2_*.fastq.gz" || -f "${SAMPLE}_002.fastq.gz" ]]; then
        echo "✅ $SAMPLE — paired"
        PAIRED_COUNT=$((PAIRED_COUNT+1))
    else
        echo "❌ $SAMPLE — missing R2 file"
        MISSING=true
    fi
done

echo -e "\nTotal samples checked: $TOTAL_COUNT"
echo "Correctly paired samples: $PAIRED_COUNT"

if [ "$MISSING" = true ]; then
    echo "⚠ Some samples are missing pairs. Fix before running fastp."
else
    echo "✅ All FASTQ files are correctly paired."
fi

```
<details>
<summary>📖 Explanation of FASTQ Pairing Check Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash shell.

- `set -euo pipefail`  
  Makes the script safer by exiting on errors, unset variables, or failed pipelines.

- `INDIR="raw_data"`  
  Directory containing raw FASTQ files.

- `if [[ "$(basename "$PWD")" != "raw_data" ]]; then cd "$INDIR"; fi`  
  Changes to the `raw_data` directory if not already there. Exits with error if directory is missing.

- `MISSING=false; PAIRED_COUNT=0; TOTAL_COUNT=0`  
  Initializes variables to track missing pairs, number of paired samples, and total samples checked.

- `for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do ...`  
  Loops over common naming patterns for R1 FASTQ files.

- `SAMPLE=...`  
  Strips suffixes like `_1`, `_R1`, `_001` to get the base sample name.

- `if [[ -f "${SAMPLE}_2.fastq.gz" || ... ]]; then ... fi`  
  Checks if corresponding R2 file exists with several naming variations.

- `echo "✅ $SAMPLE — paired"`  
  Logs paired sample.

- `echo "❌ $SAMPLE — missing R2 file"`  
  Logs missing pair and sets `MISSING=true`.

- Summary output:  
  - `Total samples checked`  
  - `Correctly paired samples`  
  - Warning if any samples are missing pairs.

</details>

##### Step 4: Make the script executable
```bash
chmod +x check_fastq_pairs.sh
```
##### Step 5: Run the script
```bash
./check_fastq_pairs.sh
```
> **Tip:** Ensure all R1/R2 naming conventions in your directory match the patterns used in the script.  
> You can adjust the patterns (`*_1.fastq.gz`, `*_R1.fastq.gz`, etc.) if needed.


# 2️⃣Calculating Minimum, Maximum, and Average Read Lengths for Paired-End Reads

Before performing any downstream bioinformatics analysis, it is important to understand the quality and characteristics of your sequencing data. One key metric is the **read length** of your FASTQ files. 

- **Minimum read length:** Helps identify very short reads that may result from sequencing errors or trimming. Extremely short reads can cause mapping errors or low-quality variant calls.  
- **Maximum read length:** Confirms whether reads were sequenced to the expected length and identifies unusually long reads that may indicate adapter contamination or sequencing artifacts.  
- **Average read length:** Provides an overall measure of the sequencing quality and consistency across the dataset.

Calculating these metrics for **both R1 and R2 reads** is particularly important in paired-end sequencing:

- Ensures that both reads in a pair are of comparable lengths, which is crucial for accurate alignment and variant calling.  
- Detects any discrepancies between forward and reverse reads that could indicate technical issues during sequencing or library preparation.  
- Allows early filtering of problematic samples before running computationally intensive steps such as mapping, variant calling, or assembly.  

By summarizing read lengths in a **CSV file**, you can quickly inspect your dataset, compare samples, and make informed decisions on trimming, filtering, or quality control. This step improves the reliability and reproducibility of downstream analyses.

---

##### Step 1:  Open nano to create a new script
```bash
nano fastq_read_length_summary.sh
```
##### Step 2: Paste the following code into nano

```bash
#!/bin/bash

FASTQ_DIR="."
OUTDIR="read_length_summary"
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.trim.fastq.gz)
    R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
        }

        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "⚠ Missing R2 for $SAMPLE, skipping."
    fi
done

echo "✅ Read length summary saved to $OUTPUT_CSV"

```
<details>
<summary>📖 Explanation of Read Length Summary Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash shell.

- `FASTQ_DIR="."`  
  Directory containing FASTQ files (current directory by default).

- `OUTDIR="read_length_summary"`  
  Directory where the summary CSV will be saved.

- `OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"`  
  Path to the output CSV file.

- `mkdir -p "$OUTDIR"`  
  Creates the output directory if it doesn’t exist.

- `echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"`  
  Writes the header line of the CSV.

- `for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do ...`  
  Loops over all R1 FASTQ files matching the `_1.trim.fastq.gz` pattern.

- `SAMPLE=$(basename "$R1" _1.trim.fastq.gz)`  
  Extracts the sample name from the R1 filename.

- `R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"`  
  Constructs the corresponding R2 filename.

- `if [[ -f "$R2" ]]; then ... else ... fi`  
  Checks that the paired R2 file exists; skips the sample if missing.

- `calc_stats() { ... }`  
  Function that calculates **minimum, maximum, and average read length** for a given FASTQ file:
  - `NR%4==2` → only sequence lines in FASTQ.  
  - `min`, `max`, `avg` → computed for all reads.  
  - Output format: `min,max,avg`.

- `STATS_R1=$(calc_stats "$R1")`  
  Calculates read length stats for R1.

- `STATS_R2=$(calc_stats "$R2")`  
  Calculates read length stats for R2.

- `echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"`  
  Appends the sample stats as a new row in the CSV.

- `echo "⚠ Missing R2 for $SAMPLE, skipping."`  
  Warning if R2 is missing.

- `echo "✅ Read length summary saved to $OUTPUT_CSV"`  
  Final message after all samples are processed.

</details>

##### Step 3: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano
##### Step 4: Make the script executable
```bash
chmod +x fastq_read_length_summary.sh
```
##### Step 5: Run the script
```bash
./fastq_read_length_summary.sh
```


# 3️⃣ FASTP – Quality Control and Trimming

[`fastp`](https://github.com/OpenGene/fastp) is a widely used **FASTQ preprocessor** for quality control (QC), read trimming, and adapter removal. It is efficient, multithreaded, and provides both **JSON and HTML reports** for each sample.  
In this pipeline, `fastp` is used to ensure that only **high-quality reads** are retained before mapping and variant calling.  
High-quality read preprocessing is crucial in *Mycobacterium tuberculosis* (TB) whole-genome sequencing (WGS) analysis. Poorly trimmed or unfiltered reads can lead to:
- False-positive SNP calls
- Mapping errors, especially in repetitive regions (e.g., PE/PPE genes)
- Biased coverage, affecting downstream variant interpretation

### Advantages of fastp over Trimmomatic and other tools
- **All-in-one solution**: Unlike Trimmomatic (which often requires extra tools for QC reports), `fastp` handles trimming, adapter detection, filtering, and QC in a single step.  
- **Automatic adapter detection**: No need to manually supply adapter sequences, which reduces human error. This is especially helpful for large TB projects with mixed sequencing batches.  
- **Speed and multithreading**: `fastp` is written in C++ and is **much faster** than Trimmomatic (Java-based), making it well-suited for high-throughput TB datasets.  
- **Comprehensive QC output**: Generates both HTML (interactive) and JSON (machine-readable) reports, giving insights into quality distribution, duplication rates, adapter content, and polyG/polyX tails.  
- **Better polyG/polyX handling**: Important for Illumina NovaSeq/NextSeq data (common in TB studies), where artificial polyG tails are a known issue.  
- **UMI (Unique Molecular Identifier) support**: Useful in advanced TB sequencing protocols where UMIs are used for error correction.  
- **Minimal parameter tuning**: Default settings are optimized and generally require fewer manual adjustments than Trimmomatic, reducing complexity in standardized pipelines.  

### Why this matters for TB analysis
- **Accurate SNP calling**: TB drug-resistance prediction relies on high-confidence SNPs; poor-quality reads increase false positives.  
- **Low genetic diversity detection**: TB isolates often differ by only a handful of SNPs. Retaining true biological variants while filtering sequencing errors is critical.  
- **Scalability for large cohorts**: Public datasets (e.g., thousands of TB isolates from ENA/NCBI) require efficient, reproducible, and automated preprocessing — a role where `fastp` excels.  
---

### Steps to Run FASTP

##### Step 1: **Open nano to create the script**
```bash
nano run_fastp.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_50"
mkdir -p "$OUTDIR"

SAMPLES=()

for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz "$INDIR"/*_001.fastq.gz "$INDIR"/*_R1_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1")
    SAMPLE=${SAMPLE%%_1.fastq.gz}
    SAMPLE=${SAMPLE%%_R1.fastq.gz}
    SAMPLE=${SAMPLE%%_001.fastq.gz}
    SAMPLE=${SAMPLE%%_R1_001.fastq.gz}

    if   [[ -f "$INDIR/${SAMPLE}_2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_002.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_002.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2_001.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2_001.fastq.gz"
    else
        echo "⚠ No R2 file found for $SAMPLE — skipping."
        continue
    fi

    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "⏩ Skipping $SAMPLE (already processed)."
        continue
    fi

    SAMPLES+=("$SAMPLE,$R1,$R2")
done

if [[ ${#SAMPLES[@]} -eq 0 ]]; then
    echo "❌ No paired FASTQ files found in $INDIR"
    exit 1
fi

THREADS=$(nproc)
FASTP_THREADS=$(( THREADS / 2 ))

run_fastp() {
    SAMPLE=$1
    R1=$2
    R2=$3
    echo "✅ Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 50 \
        --qualified_quality_phred 20 \
        --detect_adapter_for_pe \
        --thread $FASTP_THREADS \
        &> "$OUTDIR/${SAMPLE}_fastp.log"
}

export -f run_fastp
export OUTDIR FASTP_THREADS

printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}

echo "🎉 Completed fastp for $(ls "$OUTDIR"/*_fastp.json | wc -l) samples."

```
<details>
<summary>📖 Explanation of fastp Trimming Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash shell.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `INDIR="raw_data"`  
  Directory containing raw FASTQ files.

- `OUTDIR="fastp_results_min_50"`  
  Directory for trimmed FASTQ files.

- `mkdir -p "$OUTDIR"`  
  Creates the output directory if it doesn’t exist.

- `SAMPLES=()`  
  Initializes an array to store sample names and FASTQ paths.

- `for R1 in ...`  
  Loops over common R1 naming patterns.

- `SAMPLE=...`  
  Strips suffixes from R1 filenames to get the sample name.

- `if ... elif ... else`  
  Searches for corresponding R2 file under multiple naming conventions.

- `if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && ... ]]`  
  Skips the sample if it has already been processed.

- `SAMPLES+=("$SAMPLE,$R1,$R2")`  
  Stores sample and file paths in the array for parallel processing.

- `THREADS=$(nproc)`  
  Detects the total number of CPU cores.

- `FASTP_THREADS=$(( THREADS / 2 ))`  
  Uses half the cores per fastp process to prevent overloading.

- `run_fastp() { ... }`  
  Function to run `fastp` for a single sample:
  - `-i` / `-I` → input R1/R2 files
  - `-o` / `-O` → output trimmed FASTQ files
  - `-h` → HTML report
  - `-j` → JSON report
  - `--length_required 50` → discard reads shorter than 50 bp
  - `--qualified_quality_phred 20` → minimum quality threshold
  - `--detect_adapter_for_pe` → automatic adapter trimming
  - `--thread` → number of threads

- `export -f run_fastp`  
  Makes the function available to GNU Parallel.

- `printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}`  
  Runs fastp in parallel for 3 samples at a time.

- `echo "🎉 Completed fastp ..."`  
  Prints total number of processed samples.

</details>

##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_fastp.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate fastp_env
./run_fastp.sh
```

# 4️⃣ MultiQC

After preprocessing with `fastp`, we often end up with dozens or even hundreds of per-sample QC reports (`.html` and `.json`). Instead of checking each report manually, **MultiQC** allows us to aggregate all results into a single interactive HTML report.  

### Why we use MultiQC in TB analysis
- **Aggregated QC overview**: Summarizes all `fastp` results in one place.  
- **Consistency check**: Helps us quickly detect outlier samples (e.g., unusually short reads, poor quality, or failed trimming).  
- **Scalable**: Works seamlessly for hundreds or thousands of TB isolates.  
- **Standardized reporting**: Essential for sharing results across teams or publications.  
---

### Script: Run MultiQC
##### Step 1: **Open nano to create the script `run_multiqc.sh`
```bash
nano run_multiqc.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_50"
OUTPUT_DIR="multiqc_output"

mkdir -p "$OUTPUT_DIR"
multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"
```
<details>
<summary>📖 Explanation of MultiQC Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash shell.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `INPUT_DIR="fastp_results_min_50"`  
  Directory containing input QC reports (fastp JSON/HTML outputs).

- `OUTPUT_DIR="multiqc_output"`  
  Directory where MultiQC will save the aggregated report.

- `mkdir -p "$OUTPUT_DIR"`  
  Creates the output directory if it doesn’t exist.

- `multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"`  
  Runs MultiQC on all files in `INPUT_DIR` and outputs the combined HTML/JSON report into `OUTPUT_DIR`.

</details>

##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_multiqc.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate multiqc_env
./run_multiqc.sh
```

# 5️⃣ Snippy

`Snippy` is a rapid variant calling and consensus generation pipeline designed for bacterial genomes.  
In **tuberculosis genomics**, Snippy is particularly useful because:  
- It maps reads directly to a reference genome (e.g., *M. tuberculosis* H37Rv).  
- It calls SNPs and produces high-quality consensus FASTA sequences.  
- It is reproducible and lightweight, making it ideal for large TB WGS datasets.  
- Outputs are standardized and easily used for downstream tools (phylogenetics, TBProfiler, etc.).  

---

##### Step 1: Create the script
```bash
nano run_snippy.sh
```
##### Step 2: Paste the following into `run_snippy.sh`
```bash
#!/bin/bash
set -euo pipefail

REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_70"
OUTDIR="snippy_results"
THREADS=8
BWA_THREADS=30
JOBS=4

mkdir -p "$OUTDIR"

run_snippy_sample() {
    SAMPLE="$1"
    R1="${FASTP_DIR}/${SAMPLE}_1.trim.fastq.gz"
    R2="${FASTP_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R1" && -f "$R2" ]]; then
        echo "Running snippy on sample: $SAMPLE"
        TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
        mkdir -p "$TMP_DIR"

        snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
               --R1 "$R1" --R2 "$R2" --force --bwaopt "-T $BWA_THREADS"

        for f in "$TMP_DIR"/*; do
            base=$(basename "$f")
            case "$base" in
                snps.vcf) newname="${SAMPLE}.snps.vcf" ;;
                snps.tab) newname="${SAMPLE}.snps.tab" ;;
                consensus.fa) newname="${SAMPLE}.consensus.fa" ;;
                *.bam) newname="${SAMPLE}.bam" ;;
                *.bam.bai) newname="${SAMPLE}.bam.bai" ;;
                *) continue ;;
            esac
            mv "$f" "${OUTDIR}/${newname}"
        done

        rm -rf "$TMP_DIR"

        if [[ -f "${OUTDIR}/${SAMPLE}.snps.vcf" ]]; then
            echo "✅ VCF generated for $SAMPLE"
        else
            echo "⚠ No VCF produced for $SAMPLE"
        fi
    else
        echo "⚠ Missing R1/R2 for $SAMPLE"
    fi
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' \
    | parallel -j "$JOBS" run_snippy_sample {}

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' | sort > fastq_samples.txt
ls "${OUTDIR}"/*.snps.vcf 2>/dev/null \
    | sed 's|.*/||; s/\.snps\.vcf//' | sort > snippy_samples.txt

echo "FASTQ pairs count: $(wc -l < fastq_samples.txt)"
echo "Snippy outputs count: $(wc -l < snippy_samples.txt)"

if diff fastq_samples.txt snippy_samples.txt >/dev/null; then
    echo "✅ All FASTQ pairs have corresponding Snippy results."
else
    echo "⚠ Missing samples detected:"
    diff fastq_samples.txt snippy_samples.txt || true
fi

rm -f fastq_samples.txt snippy_samples.txt

echo "🎯 All steps completed!"
echo "Snippy results are in: ${OUTDIR}/"
```
<details>
<summary>📖 Explanation of Snippy Pipeline Script</summary>

- `#!/bin/bash`  
  Runs the script using Bash shell.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `REF="H37Rv.fasta"`  
  Reference genome for Snippy variant calling.

- `FASTP_DIR="fastp_results_min_70"`  
  Directory containing trimmed FASTQ files.

- `OUTDIR="snippy_results"`  
  Directory for Snippy outputs.

- `THREADS` and `BWA_THREADS`  
  Threads for Snippy internal processes and BWA alignment.

- `JOBS=4`  
  Number of samples to run in parallel.

- `run_snippy_sample() { ... }`  
  Function to run Snippy for a single sample:
  - Checks if R1/R2 FASTQ files exist.
  - Creates a temporary directory for Snippy outputs.
  - Runs Snippy with specified threads and BWA options.
  - Renames and moves output files (`*.vcf`, `*.bam`, `consensus.fa`) to a standardized format with the sample name.
  - Deletes temporary files.
  - Verifies that the VCF was generated.

- `export -f run_snippy_sample`  
  Makes the function available to GNU Parallel.

- `ls "${FASTP_DIR}"/*_1.trim.fastq.gz | sed ... | parallel -j "$JOBS" run_snippy_sample {}`  
  Detects all R1 FASTQ files and runs Snippy in parallel for multiple samples.

- Verification steps:  
  - Lists FASTQ samples and generated VCFs.  
  - Compares both lists to ensure all samples were processed.  
  - Prints counts and warnings if samples are missing.

- `rm -f fastq_samples.txt snippy_samples.txt`  
  Cleans up temporary verification files.

- `echo "🎯 All steps completed!"`  
  Prints completion message with output directory location.

</details>

##### Step 3: Save and exit nano
Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)

##### Step 4: Make the script executable
```bash
chmod +x run_snippy.sh
```
##### Step 5: Activate environment and run
```bash
conda activate snippy_env
./run_snippy.sh
```

# 6️⃣ Qualimap BAM QC

Once we generate BAM files with Snippy, it is important to assess their quality before moving to downstream analyses.  
**Qualimap** is widely used for this because:  
- It evaluates mapping quality, coverage distribution, and GC content.  
- It produces both **HTML** and **PDF** reports with easy-to-interpret QC metrics.  
- In TB genomics, it helps us quickly identify problematic samples with low coverage, uneven depth, or poor alignment to the H37Rv reference.  
- The results can later be aggregated with MultiQC for batch reporting.  

---

##### Step 1: Create the script
```bash
nano run_qualimap.sh
```
#####  Step 2: Paste the following into `run_qualimap.sh`
```bash
#!/bin/bash
set -euo pipefail

SNIPPY_DIR="all_bams"
QUALIMAP_OUT="qualimap_reports"
mkdir -p "$QUALIMAP_OUT"

for bam in "$SNIPPY_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Running Qualimap BAM QC for sample: $sample"

    outdir="${QUALIMAP_OUT}/${sample}"
    mkdir -p "$outdir"

    qualimap bamqc \
        -bam "$bam" \
        -outdir "$outdir" \
        -outformat pdf:html \
        --java-mem-size=4G
done
```
<details>
<summary>📖 Explanation of Qualimap BAM QC Script</summary>

- `#!/bin/bash`  
  Runs the script using Bash.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `SNIPPY_DIR="all_bams"`  
  Directory containing Snippy-generated BAM files.

- `QUALIMAP_OUT="qualimap_reports"`  
  Directory to store Qualimap QC outputs.

- `mkdir -p "$QUALIMAP_OUT"`  
  Creates output directory if it does not exist.

- `for bam in "$SNIPPY_DIR"/*.bam; do ... done`  
  Loops over all BAM files in the Snippy directory.

- `sample=$(basename "$bam" .bam)`  
  Extracts the sample name from the BAM filename.

- `outdir="${QUALIMAP_OUT}/${sample}"`  
  Creates a unique output folder for each sample.

- `qualimap bamqc -bam "$bam" -outdir "$outdir" -outformat pdf:html --java-mem-size=4G`  
  Runs Qualimap QC on the BAM file:
  - Generates both PDF and HTML reports.
  - Allocates 4 GB of Java memory for processing.

- `echo "Running Qualimap BAM QC for sample: $sample"`  
  Prints status messages.

</details>


##### Step 3: Save and exit nano

Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_qualimap.sh
```
##### Step 5: Activate environment and install GNU Parallel into your `qualimap_env`:
```bash
conda activate qualimap_env
conda install -c conda-forge parallel
```
##### Step 6: run:
```bash
./run_qualimap.sh
```

# 7️⃣ MultiQC after Qualimap

Once we finish running **Qualimap BAM QC**, each sample has its own folder with detailed HTML and PDF reports.  
While these per-sample reports are useful, it becomes inefficient to open them one by one when working with many TB isolates.  
This is where **MultiQC** is especially powerful:  
- It scans all the Qualimap output folders.  
- It aggregates mapping quality, depth, and coverage statistics across all samples.  
- It provides an **at-a-glance overview** of problematic samples (e.g., low coverage, uneven depth, or poor alignment).  
- It ensures results are **standardized and shareable** across teams.  
---

### Run MultiQC on Qualimap outputs
```bash
multiqc qualimap_reports -o multiqc_report
```
📊 The final summary will be available in:  multiqc_report/multiqc_report.html

# 8️⃣ TB Variant Filter
The **tb_variant_filter** tool is a specialized filtering framework designed for **Mycobacterium tuberculosis (M. tb)** sequencing data. Unlike generic variant filtering tools (e.g., GATK VariantFiltration, bcftools filter), this tool leverages TB-specific genomic features and known problematic regions of the **H37Rv reference genome** to ensure only **high-confidence variants** are kept for downstream analysis.

---
## ✨ Why Filtering is Important
Raw variant calls from Snippy (or any variant caller) often include:
- **False positives** due to sequencing errors or mapping ambiguity.  
- Variants in **repetitive regions**, where reads cannot be uniquely aligned.  
- Low-quality calls with insufficient coverage or poor base quality.  
If left unfiltered, these can:
- Lead to **spurious drug resistance predictions**.  
- Introduce **noise in phylogenetic analyses**, distorting transmission clustering.  
- Inflate **SNP distances**, causing misinterpretation of outbreaks.  
---

## ⚙️ What `tb_variant_filter` Does
This tool applies a series of filters tailored to *M. tuberculosis*. The main steps include:

1. **Masking problematic genomic regions**  
   - Excludes SNPs in highly repetitive or poorly mappable regions (e.g., **PE/PPE gene families**, mobile genetic elements).  
   - These regions are known to cause alignment artifacts and unreliable variant calls.  
2. **Depth-based filtering**  
   - Removes variants with **insufficient coverage** (low read depth).  
   - Ensures only variants supported by enough reads are retained.  
3. **Quality-based filtering**  
   - Filters out variants with **low base or mapping quality**.  
   - Retains only high-confidence SNPs/indels.  
4. **Retains core genome SNPs**  
   - Focuses on SNPs in **conserved coding regions**, which are more reliable for drug resistance and phylogeny.  
   - This ensures consistency across isolates for comparative analysis.  
---

## ✅ Benefits of Using `tb_variant_filter`
- **TB-specific**: Tailored to the H37Rv reference genome.  
- **Improved accuracy**: Removes false positives that generic tools often miss.  
- **Reliable resistance prediction**: Only keeps SNPs truly linked to drug resistance.  
- **Cleaner phylogenies**: Results in more robust clustering and outbreak inference.  
- **Standardization**: Widely adopted in TB genomic epidemiology pipelines, making results comparable across studies.  
---

## Steps

##### Step 1: Create or edit the script  
```bash
nano run_tb_variant_filter.sh
```
#####  Step 2: Paste the following into `run_tb_variant_filter.sh`
```bash
#!/bin/bash
set -euo pipefail

# Activate tb_variant_filter environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate tb_variant_filter_env

CURDIR=$(pwd)
OUTDIR="$CURDIR/tb_variant_filter_results"
mkdir -p "$OUTDIR"

for vcf in "$CURDIR/snippy_results"/*.vcf; do
    sample=$(basename "$vcf")
    echo "Filtering $vcf ..."
    tb_variant_filter "$vcf" "$OUTDIR/${sample%.vcf}.filtered.vcf"
done

echo "✅ All VCFs filtered and saved in $OUTDIR"

```
<details>
<summary>📖 Explanation of TB Variant Filter Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `source $(conda info --base)/etc/profile.d/conda.sh`  
  Loads Conda shell functions to allow environment activation within a script.

- `conda activate tb_variant_filter_env`  
  Activates the environment containing `tb_variant_filter`.

- `CURDIR=$(pwd)`  
  Saves the current working directory.

- `OUTDIR="$CURDIR/tb_variant_filter_results"`  
  Sets the output directory for filtered VCFs.

- `mkdir -p "$OUTDIR"`  
  Creates the output directory if it does not exist.

- `for vcf in "$CURDIR/snippy_results"/*.vcf; do ... done`  
  Loops through all VCF files generated by Snippy.

- `sample=$(basename "$vcf")`  
  Extracts the filename of the VCF to use for naming outputs.

- `tb_variant_filter "$vcf" "$OUTDIR/${sample%.vcf}.filtered.vcf"`  
  Runs TB variant filtering and saves the filtered VCF in the output folder.

- `echo "✅ All VCFs filtered and saved in $OUTDIR"`  
  Prints a completion message.

</details>

##### Step 3: Make the script executable
```bash
chmod +x run_tb_variant_filter.sh
```
##### Step 4: Activate environment and run
```bash
conda activate tb_variant_filter_env
./run_tb_variant_filter.sh
```
# 9️⃣ TB-Profiler from FASTQ Files
---

**TB-Profiler** is a specialized bioinformatics tool designed for *Mycobacterium tuberculosis* whole-genome sequencing (WGS) data.  
It performs **variant calling, lineage determination, and drug resistance prediction** in a single streamlined pipeline.  

### Why we use TB-Profiler in our workflow
- 🧪 **Drug Resistance Prediction** → Screens for known resistance mutations against both first- and second-line TB drugs.  
- 🌍 **Lineage Typing** → Classifies isolates into globally recognized TB lineages (e.g., Lineage 1–7).  
- 🔄 **Flexible Input** → Accepts FASTQ, BAM, or VCF files depending on our analysis stage.  
- 📊 **Clear Outputs** → Generates both human-readable reports (`.txt`) and structured machine-readable files (`.json`).  
- ⚡ **Speed & Integration** → Built on efficient tools like `bcftools` and integrates easily into TB genomics pipelines.  

### Why it matters
- Provides **actionable insights for public health**: drug resistance, lineage distribution, and outbreak tracking.  
- Avoids the need to manually cross-reference mutations with multiple TB resistance databases.  
- Standardized and widely adopted in the TB research community, making our results **comparable across studies**.  
---

## Steps

##### Step 1: Create or edit the script
```bash
nano run_tbprofiler.sh
```
##### Step 2: Paste the following code
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"
OUTDIR="tbprofiler_results"

mkdir -p "$OUTDIR"

for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.fastq.gz)
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"

    if [[ ! -f "$R2" ]]; then
        echo "❌ Missing pair for $SAMPLE (no $R2)"
        continue
    fi

    echo "Processing sample: $SAMPLE"

    tb-profiler profile \
        --read1 "$R1" \
        --read2 "$R2" \
        --prefix "$OUTDIR/$SAMPLE" \
        --txt
done

echo "🎯 All tb-profiler runs completed. Results saved in $OUTDIR"
```
<details>
<summary>📖 Explanation of TB-Profiler Script</summary>

- `#!/bin/bash`  
  Runs the script using Bash.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `FASTQ_DIR="raw_data"`  
  Directory containing paired-end FASTQ files.

- `OUTDIR="tbprofiler_results"`  
  Directory where TB-Profiler outputs will be stored.

- `mkdir -p "$OUTDIR"`  
  Creates the output directory if it does not exist.

- `for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do ... done`  
  Loops through all R1 FASTQ files.

- `SAMPLE=$(basename "$R1" _1.fastq.gz)`  
  Extracts sample name from the R1 filename.

- `R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"`  
  Constructs the path to the corresponding R2 file.

- `if [[ ! -f "$R2" ]]; then ... fi`  
  Skips the sample if the R2 file is missing.

- `tb-profiler profile --read1 "$R1" --read2 "$R2" --prefix "$OUTDIR/$SAMPLE" --txt`  
  Runs TB-Profiler on the paired-end reads, outputting results in TXT format.

- `echo "🎯 All tb-profiler runs completed. Results saved in $OUTDIR"`  
  Prints a completion message.

</details>

##### Step 3: Make the script executable
```bash
chmod +x run_tbprofiler.sh
```
##### Step 4: Activate environment and run
```bash
conda activate tbprofiler_env
./run_tbprofiler.sh
```


# 10️⃣ BCFTools Consensus Generation

After filtering VCFs with **tb_variant_filter**, we generate **sample-specific consensus sequences**.  
This allows us to produce FASTA sequences representing the **full genome of each isolate**, incorporating only **high-confidence variants** relative to the reference genome (*H37Rv*).  

## Why consensus sequences are important
- Provide a **single representative genome** per sample for downstream analyses.  
- Used in **phylogenetic reconstruction**, outbreak investigation, and comparative genomics.  
- Incorporates **only reliable SNPs and indels**, reducing noise from sequencing errors.  
- Standardizes genome representations across multiple isolates.  
---

##### Step 1: Compress and index each filtered VCF
```bash
for vcf in tb_variant_filter_results/*.vcf; do
    bgzip -c "$vcf" > "${vcf}.gz"
    bcftools index "${vcf}.gz"
done
```
##### Step 2: Create a script to generate consensus sequences
```bash
nano generate_consensus_all.sh
```
##### Step 3: Paste this code
```bash
#!/bin/bash
set -euo pipefail

CURDIR=$(pwd)
VCFDIR="$CURDIR/tb_variant_filter_results"
OUTDIR="$CURDIR/consensus_sequences"
mkdir -p "$OUTDIR"

for vcf in "$VCFDIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)
    echo "Processing $sample ..."
    bgzip -c "$vcf" > "$vcf.gz"
    bcftools index "$vcf.gz"
    bcftools consensus -f "$CURDIR/H37Rv.fasta" "$vcf.gz" | sed "1s/.*/>$sample/" > "$OUTDIR/${sample}.consensus.fasta"
    echo "✅ $sample consensus generated with sample-based header."
done

echo "🎉 All consensus sequences saved in $OUTDIR."

```
<details>
<summary>📖 Explanation of VCF-to-Consensus Script</summary>

- `#!/bin/bash`  
  Runs the script with Bash.

- `set -euo pipefail`  
  Exits on errors, undefined variables, or pipeline failures.

- `CURDIR=$(pwd)`  
  Saves the current working directory.

- `VCFDIR="$CURDIR/tb_variant_filter_results"`  
  Directory containing filtered VCFs.

- `OUTDIR="$CURDIR/consensus_sequences"`  
  Directory for storing consensus FASTA sequences.

- `mkdir -p "$OUTDIR"`  
  Creates the output directory if it does not exist.

- `for vcf in "$VCFDIR"/*.vcf; do ... done`  
  Loops through all filtered VCF files.

- `sample=$(basename "$vcf" .vcf)`  
  Extracts sample name from the VCF filename.

- `bgzip -c "$vcf" > "$vcf.gz"`  
  Compresses the VCF file with `bgzip`.

- `bcftools index "$vcf.gz"`  
  Indexes the compressed VCF for consensus generation.

- `bcftools consensus -f "$CURDIR/H37Rv.fasta" "$vcf.gz" | sed "1s/.*/>$sample/" > "$OUTDIR/${sample}.consensus.fasta"`  
  Generates a consensus FASTA using the reference genome and replaces the FASTA header with the sample name.

- `echo "✅ $sample consensus generated with sample-based header."`  
  Prints a confirmation for each sample.

- `echo "🎉 All consensus sequences saved in $OUTDIR."`  
  Prints a final completion message.

**Note:** Make sure the `tb_consensus_env` (or equivalent) is already activated before running this script.
</details>

##### Step 4: Make the script executable
```bash
chmod +x generate_consensus_all.sh
```
##### Step 5: Run the script
```bash
conda activate tb_consensus_env
./generate_consensus_all.sh
```

# 1️⃣1️⃣ Check Consensus FASTA Lengths

After generating consensus sequences, it's important to **verify the genome length** for each sample.  
This ensures no sequences are truncated or incomplete due to missing coverage or filtering.

---

### 📏 Calculating Consensus Genome Lengths

We can check the length of each consensus FASTA sequence to ensure completeness and consistency.  
This helps verify that consensus sequences cover the full *M. tuberculosis* genome (~4.4 Mbp) and can reveal missing regions.

### Using `grep` and `wc`
We remove the FASTA headers (`>` lines) and count the remaining nucleotides to get the total genome length:

```bash
for f in consensus_sequences/*.fasta; do
    sample=$(basename "$f")
    # Remove header lines and count remaining characters
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample : $length bp"
done
```

to save the result in csv file 
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"
OUTPUT_CSV="consensus_lengths.csv"
echo "Sample,Length_bp" > "$OUTPUT_CSV"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample,$length" >> "$OUTPUT_CSV"
done

echo "✅ Consensus genome lengths saved to $OUTPUT_CSV"

```
<details>
<summary>📖 Explanation of the calculating consensus genome lengths calculation and saving the result in csv</summary>

- `FASTA_DIR="consensus_sequences"` → sets the directory containing consensus FASTA files.  
- `OUTPUT_CSV="consensus_lengths.csv"` → defines the CSV file to save lengths.  
- `echo "Sample,Length_bp" > "$OUTPUT_CSV"` → creates the CSV file and writes the header line.  
- `for f in "$FASTA_DIR"/*.fasta; do ... done` → loops over all FASTA files in the directory.  
- `sample=$(basename "$f" .fasta)` → extracts the sample name from the FASTA filename.  
- `length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)` → removes header lines (`>`), concatenates sequences into one line, and counts nucleotides.  
- `echo "$sample,$length" >> "$OUTPUT_CSV"` → appends the sample name and its sequence length to the CSV file.  
- `echo "✅ Consensus genome lengths saved to $OUTPUT_CSV"` → prints a completion message when finished.  

</details>

### Rename the FASTA files

```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"
for f in "$FASTA_DIR"/*.snps.filtered.consensus.fasta; do
    mv "$f" "${f/.snps.filtered.consensus/}"
done
echo "✅ All consensus FASTA files have been renamed."

```
<details> <summary>📖 Explanation of rename the FASTA files</summary>

FASTA_DIR="consensus_sequences" → sets the directory containing consensus FASTA files.

for f in "$FASTA_DIR"/*.snps.filtered.consensus.fasta; do ... done → loops over all FASTA files ending with .snps.filtered.consensus.fasta.

mv "$f" "${f/.snps.filtered.consensus/}" → renames each file by removing the .snps.filtered.consensus part from its filename.

echo "✅ All consensus FASTA files have been renamed." → prints a confirmation message when renaming is complete.

</details>

###  Update headers inside the FASTA files
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    awk -v s="$sample" '/^>/{print ">" s; next} {print}' "$f" > "${f}.tmp" && mv "${f}.tmp" "$f"
    echo "✅ Updated header in: $(basename "$f")"
done
echo "🎉 All FASTA headers have been successfully updated."
```
<details> 
<summary>📖 Explanation of rename headers inside the FASTA files</summary>

- `FASTA_DIR="consensus_sequences"` → sets the directory containing FASTA files.

- `for f in "$FASTA_DIR"/*.fasta; do ... done` → loops over all FASTA files in the directory.

- `sample=$(basename "$f" .fasta)` → extracts the sample name from the filename (without `.fasta`).

- `awk -v s="$sample" '/^>/{print ">" s; next} {print}' "$f" > "${f}.tmp" && mv "${f}.tmp" "$f"` → replaces the header line in each FASTA with `>sample` while keeping the sequence lines unchanged, writes to a temporary file, and replaces the original.

- `echo "✅ Updated header in: $(basename "$f")"` → prints which file was updated.

- `echo "🎉 All FASTA headers have been successfully updated."` → prints a completion message when all headers are updated.

</details>

# 1️⃣2️⃣ Multiple Sequence Alignment with MAFFT

MAFFT v7.490 requires **a single FASTA file** as input.  
It **cannot take multiple FASTA files** on the command line directly, otherwise it interprets filenames as options.

---

##### Step 1: Merge all consensus FASTAs
Combine all individual consensus sequences into one multi-FASTA file:

```bash
cat consensus_sequences/*.fasta > consensus_sequences/all_consensus.fasta
```
##### Step 2: Run MAFFT
Perform multiple sequence alignment on the merged file:
Use a faster algorithm than --auto

--auto lets MAFFT decide, but for hundreds/thousands of sequences, it often picks a slower iterative refinement.
You can explicitly pick faster modes:

Since Mycobacterium tuberculosis genomes are highly conserved and we care more about speed than very small accuracy gains, we can safely drop the expensive iterative refinement steps in MAFFT.
fast and TB-suitable command

```bash
mafft --retree 2 --maxiterate 0 --thread -1 consensus_sequences/all_consensus.fasta > consensus_sequences/aligned_consensus.fasta
```
<details>
<summary>🔹 Why this is good for TB</summary>

- `--retree 2` → guide tree rebuilt twice (enough for closely related genomes).  
- `--maxiterate 0` → skips iterative refinement (much faster).  
- `--thread -1` → automatically uses **all available CPU cores**.  
- Accuracy loss is negligible for TB because sequences are >99% identical.  

</details>

If we have very many genomes ( >1000 ) and want maximum speed, switch to:
```bash
mafft --parttree --retree 2 --maxiterate 0 --thread -1 consensus_sequences/all_consensus.fasta > consensus_sequences/aligned_consensus.fasta
```

##### Step 3: Verify the alignment
Quickly inspect the top of the aligned FASTA:
```bash
head consensus_sequences/aligned_consensus.fasta
```

# Shovill
Shovill is a fast and easy-to-use **bacterial genome assembler** designed for Illumina short-read data. It wraps around popular assemblers like **SPAdes** or **SKESA**, streamlining the process of genome assembly from paired-end reads.  

Key points for TB genomes:

- Optimized for **small bacterial genomes** (~4–5 Mb).  
- Uses multiple threads for faster assembly (`--cpus`) and can manage RAM efficiently (`--ram`).  
- Allows customization of **minimum contig length** (`--minlen`) and **coverage depth** (`--mincov` / `--depth`).  
- Automatically renames output contigs for clarity and downstream analyses.  
- Suitable for pipelines with **already trimmed FASTQ files** from fastp.  

> ⚠ Note: Shovill is best used with **high-quality, paired-end Illumina reads**. Low-quality or highly fragmented data may require additional QC before assembly.

```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_50"
OUTDIR="shovill_results"
mkdir -p "$OUTDIR"

GSIZE=4411532

shopt -s nullglob
for R1 in "$INPUT_DIR"/*_1.trim.fastq.gz; do
  [[ -e "$R1" ]] || continue

  R2="${R1/_1.trim.fastq.gz/_2.trim.fastq.gz}"

  if [[ ! -f "$R2" ]]; then
    echo ">> Skipping $(basename "$R1") (no matching R2 found)" >&2
    continue
  fi

  sample=$(basename "$R1" _1.trim.fastq.gz)
  sample_out="$OUTDIR/$sample"

  if [[ -f "$sample_out/${sample}_contigs.fa" ]]; then
    echo ">> Skipping $sample (already assembled)"
    continue
  fi

  echo "==> Running Shovill on: $sample"
  mkdir -p "$sample_out"

  shovill \
    --R1 "$R1" \
    --R2 "$R2" \
    --gsize "$GSIZE" \
    --outdir "$sample_out" \
    --assembler skesa \
    --minlen 500 \
    --mincov 5 \
    --depth 100 \
    --namefmt "${sample}_%05d" \
    --cpus 4 \
    --ram 16 \
    --tmpdir "${TMPDIR:-/tmp}" \
    --force

  echo "==> Renaming output files in: $sample_out"
  for f in "$sample_out"/*; do
    base=$(basename "$f")
    mv "$f" "$sample_out/${sample}_$base"
  done
done
```
<details> <summary>📖 Explanation of Shovill Pipeline Script</summary>

INPUT_DIR="fastp_results_min_50" → directory with preprocessed FASTQ files.

OUTDIR="shovill_results" → directory to store Shovill assemblies.

mkdir -p "$OUTDIR" → creates output directory if it doesn’t exist.

GSIZE=4411532 → approximate genome size for M. tuberculosis (~4.41 Mb).

shopt -s nullglob → ensures the for loop skips if no matching files exist.

for R1 in "$INPUT_DIR"/*_1.trim.fastq.gz; do ... done → loops over all R1 FASTQ files.

R2="${R1/_1.trim.fastq.gz/_2.trim.fastq.gz}" → guesses the corresponding R2 file name.

if [[ ! -f "$R2" ]]; then ... fi → skips the sample if R2 is missing.

sample=$(basename "$R1" _1.trim.fastq.gz) → extracts sample name from R1 file.

sample_out="$OUTDIR/$sample" → defines output folder for the sample.

if [[ -f "$sample_out/${sample}_contigs.fa" ]]; then ... fi → skips assembly if contigs already exist.

shovill \ ... → runs Shovill assembler with parameters:

--R1/--R2 → paired-end reads

--gsize → genome size

--outdir → output directory

--assembler skesa → chooses SKESA assembler

--minlen 500 → minimum contig length

--mincov 5 → minimum coverage

--depth 100 → target depth

--namefmt "${sample}_%05d" → output naming

--cpus 4 → CPU threads

--ram 16 → RAM in GB

--tmpdir → temporary directory

--force → overwrite existing files

for f in "$sample_out"/*; do ... done → renames all files in output folder to include sample prefix.

</details>


### Checking the output
View large files page by page
```bash
less shovill_results/ET1135_S12/ET1135_S12_contigs.fa
```
View the firt ten lines
```bash
head -n 20 shovill_results/ET1135_S12/ET1135_S12_contigs.fa
```
Counted how many contigs we have
```bash
grep -c ">" ./shovill_results/ET1135_S12/ET1135_S12_contigs.fa
```

To get contig name, length, coverage from the FASTA headers:
```bash
grep ">" ./shovill_results/ET1135_S12/ET1135_S12_contigs.fa | \
awk -F'[ =]' '{print $1, $2, $3, $4, $5}'
```
#  Assembly Evaluation

Before downstream analyses, it is important to verify the quality of the assembled genome. This ensures reliable results in variant calling, consensus generation, and phylogenetic analyses.
### 1. Quick assembly stats using `stats.sh`

The `stats.sh` script from BBMap provides basic assembly statistics such as total length, N50, number of contigs, and GC content.
###### Activate the environment
```bash
conda activate bbmap_env
```
###### Run stats.sh to Verify Installation
```bash
stats.sh
```

######  run `stats.sh`
```bash
stats.sh in=./shovill_results/ET1135_S12/ET1135_S12_contigs.fa
```
> **Tip:** This command will display key statistics including:
> 
> - Total bases
> - Number of contigs
> - Minimum, maximum, and N50 contig lengths
> - GC content


loop over multiple assemblies:
```bash
for f in ./shovill_results/*/*_contigs.fa; do
    stats.sh in="$f"
done
```
script that will loop over all contig FASTA files, run stats.sh from BBMap, and save the results to a CSV file:
##### Step 1: Create or edit the script
```bash
nano run_assembly_stats.sh
```
##### Step 2: Paste the following into the script
```bash
#!/bin/bash

CONTIG_DIR="./shovill_results"
OUTPUT_CSV="assembly_stats.csv"

echo "Sample,Total_Bases,Num_Contigs,Min_Contig,Max_Contig,N50,GC_Content" > "$OUTPUT_CSV"

for f in "$CONTIG_DIR"/*/*_contigs.fa; do
    sample=$(basename "$f" _contigs.fa)
    
    stats_output=$(stats.sh in="$f" format=tsv 2>/dev/null | tail -n 1)
    
    total=$(echo "$stats_output" | cut -f1)
    num=$(echo "$stats_output" | cut -f3)
    min=$(echo "$stats_output" | cut -f4)
    max=$(echo "$stats_output" | cut -f5)
    n50=$(echo "$stats_output" | cut -f6)
    gc=$(echo "$stats_output" | cut -f8)
    
    echo "$sample,$total,$num,$min,$max,$n50,$gc" >> "$OUTPUT_CSV"
done

echo "✅ Assembly stats saved to $OUTPUT_CSV"

```
###### Step 3: Make the script executable
``` bash
chmod +x run_assembly_stats.sh
```
###### Step 4: Activate environment and run
``` bash
conda activate bbmap_env
./run_assembly_stats.sh
```

### 2. Using seqkit to explore assembly
###### Activate the environment
``` bash
conda activate seqkit_env
```
###### Display help
``` bash
seqkit -h
```
###### Convert FASTA to tab-delimited table (sequence length and name)
``` bash
seqkit fx2tab -nl ./shovill_results/ET1135_S12/ET1135_S12_contigs.fa
```
### run QUAST on all Shovill assemblies
Collect the key statistics in a single CSV file
##### Step 1: Create the script
```bash
nano run_seqkit_on_shovill.sh
```
#####  Step 2: Paste the following into `run_seqkit_on_shovill.sh`

``` bash
#!/bin/bash
set -euo pipefail

# Directories
SHOVILL_DIR="shovill_results"
QUAST_DIR="quast_results"
mkdir -p "$QUAST_DIR"

# CSV output
CSV_FILE="quast_summary.csv"
echo "Sample,NumContigs,TotalLength,MinLen,MaxLen,AverageLen,N50,N75,GC%" > "$CSV_FILE"

# Loop over all samples
for sample_out in "$SHOVILL_DIR"/*; do
  [[ -d "$sample_out" ]] || continue

  sample=$(basename "$sample_out")
  contigs=$(ls "$sample_out"/*_contigs.fa 2>/dev/null | head -n 1)
  if [[ -z "$contigs" ]]; then
    echo ">> Skipping $sample (no contigs found)" >&2
    continue
  fi

  # Output folder for QUAST
  outdir="$QUAST_DIR/$sample"
  mkdir -p "$outdir"

  echo "==> Running QUAST on sample: $sample"
  quast "$contigs" -o "$outdir" --csv

  # Extract statistics from QUAST CSV
  stats_file="$outdir/report.tsv"
  if [[ -f "$stats_file" ]]; then
    # QUAST tsv has header, take the second line
    stats=$(sed -n '2p' "$stats_file" | tr '\t' ',')
    echo "${sample},${stats}" >> "$CSV_FILE"
  else
    echo ">> Warning: QUAST report missing for $sample" >&2
  fi
done

echo "✅ All QUAST stats saved in $CSV_FILE"
```
<details> <summary>Click to expand explanation</summary>

CONTIG_DIR="./shovill_results" → directory containing contig FASTA files.

OUTPUT_CSV="assembly_stats.csv" → CSV file to save assembly statistics.

echo "Sample,Total_Bases,...,GC_Content" > "$OUTPUT_CSV" → writes CSV header.

for f in "$CONTIG_DIR"/*/*_contigs.fa; do ... done → loops over all contig FASTA files in subdirectories.

sample=$(basename "$f" _contigs.fa) → extracts sample name from filename.

stats_output=$(stats.sh in="$f" format=tsv 2>/dev/null | tail -n 1) → runs stats.sh to get assembly metrics in TSV format and takes the last line (numeric stats).

total=$(echo "$stats_output" | cut -f1) → total bases in contigs.

num=$(echo "$stats_output" | cut -f3) → number of contigs.

min=$(echo "$stats_output" | cut -f4) → minimum contig length.

max=$(echo "$stats_output" | cut -f5) → maximum contig length.

n50=$(echo "$stats_output" | cut -f6) → N50 statistic.

gc=$(echo "$stats_output" | cut -f8) → GC content percentage.

echo "$sample,$total,$num,$min,$max,$n50,$gc" >> "$OUTPUT_CSV" → appends the stats to the CSV.

echo "✅ Assembly stats saved to $OUTPUT_CSV" → prints completion message.

</details>
##### Step 3: Make the script executable
```bash
chmod +x run_seqkit_on_shovill.sh
```
##### Step 4: Activate environment and run
```bash
conda activate tbprofiler_env
./run_seqkit_on_shovill.sh
```

#### 3. Assembly summary with assembly-scan
We can use another tool assembly-scan to generate summary statistics of the assembly.
###### Activate the environment
``` bash
conda activate assembly_scan_env
```
###### Verify the Installation
``` bash
assembly-scan --version
```
######  Generate summary statistics
``` bash
assembly-scan ./shovill_results/ET1135_S12/ET1135_S12_contigs.fa \
  --transpose \
  | tee ./shovill_results/ET1135_S12/ET1135_S12-assembly-scan.tsv
```
##### 4. Compute GC content from assembly-scan output
``` bash
grep 'contig_percent_[cg]' \
  ./shovill_results/ET1135_S12/ET1135_S12-assembly-scan.tsv \
  | awk -F '\t' '{sum+=$3} END {print "GC%=",sum}'
```

# Prokka
Prokka is a rapid **prokaryotic genome annotation tool** that predicts genes, coding sequences (CDS), rRNAs, tRNAs, and other genomic features from assembled contigs or genomes.  

Key points for TB genomes:

- Annotates **Mycobacterium tuberculosis** genomes with correct taxonomy using `--genus` and `--species`.  
- Produces multiple output files, including **GFF3**, **FASTA of proteins**, and **GenBank format**, which are useful for downstream analysis.  
- Supports **multi-threading** (`--cpus`) to speed up processing of multiple genomes.  
- Works seamlessly with **Shovill-assembled contigs**.  
- Output files are organized per sample directory with a consistent naming prefix for easy pipeline integration.  

> ⚠ Note: Prokka relies on the quality of the assembly; fragmented or low-coverage assemblies may result in incomplete annotations.
##### Step 1: Create or edit the script
```bash
nano run_prokka.sh
```
##### Step 2: Paste the following into the script

```bash
#!/bin/bash
set -euo pipefail

SHOVILL_DIR="shovill_results"
PROKKA_DIR="prokka_results"
mkdir -p "$PROKKA_DIR"

for sample_out in "$SHOVILL_DIR"/*; do
  [[ -d "$sample_out" ]] || continue

  sample=$(basename "$sample_out")

  contigs=$(ls "$sample_out"/*_contigs.fa 2>/dev/null | head -n 1)
  if [[ -z "$contigs" ]]; then
    echo ">> Skipping $sample (no contigs.fa found)" >&2
    continue
  fi

  echo "==> Running Prokka on sample: $sample"

  outdir="$PROKKA_DIR/$sample"
  mkdir -p "$outdir"

  prokka \
    --outdir "$outdir" \
    --prefix "$sample" \
    --kingdom Bacteria \
    --genus Mycobacterium \
    --species tuberculosis \
    --cpus 4 \
    "$contigs"
done

```
<details><summary>📖 Explanation of Prokka Pipeline Script</summary>

SHOVILL_DIR="shovill_results" → directory containing Shovill assemblies.  
PROKKA_DIR="prokka_results" → directory to store Prokka annotation outputs.  
mkdir -p "$PROKKA_DIR" → ensures the Prokka results directory exists.  
for sample_out in "$SHOVILL_DIR"/*; do ... done → loops over each sample folder.  
[[ -d "$sample_out" ]] || continue → skips files, only process directories.  
sample=$(basename "$sample_out") → extracts sample name from directory.  
contigs=$(ls "$sample_out"/*_contigs.fa 2>/dev/null | head -n 1) → finds contigs FASTA.  
if [[ -z "$contigs" ]]; then ... fi → skip sample if no contigs found.  
outdir="$PROKKA_DIR/$sample" → separate output directory per sample.  
mkdir -p "$outdir" → ensures Prokka output directory exists.  
prokka --outdir "$outdir" --prefix "$sample" --kingdom Bacteria --genus Mycobacterium --species tuberculosis --cpus 4 "$contigs" → runs Prokka with TB-specific settings.

</details>


###### Step 3: Make the script executable
``` bash
chmod +x run_prokka.sh
```
###### Step 4: Activate environment and run
``` bash
conda activate prokka_env
./run_prokka.sh






# 📖 References

WHO. Catalogue of mutations in MTBC and their association with drug resistance, 2nd ed, 2023.

Ngabonziza JCS, et al. A sister lineage of the Mycobacterium tuberculosis complex discovered in Rwanda. Nat Commun, 2020 (Lineage 8).

Coscolla M, et al. Phylogenomics of Mycobacterium africanum lineage 9. Nat Commun, 2021 (Lineage 9).

Phelan J, et al. Whole-genome sequencing for TB: current standards and challenges. Nat Rev Microbiol.

Li H, et al. Fast and accurate short read alignment with Burrows–Wheeler transform. Bioinformatics
