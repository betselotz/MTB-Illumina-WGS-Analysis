# MTB-Illumina-WGS-Analysis

Bioinformatics workflow for analyzing *Mycobacterium tuberculosis* whole-genome sequences (WGS) generated by Illumina and available on NCBI. This workflow covers the full pipeline: downloading raw sequencing data, quality control, read mapping, variant calling and annotation, prediction of drug-resistance mutations, lineage typing, and phylogenetic analysis.

---


Written by: [Betselot Zerihun Ayano ](https://github.com/betselotz)
## Introduction â€“ *Mycobacterium tuberculosis* Complex (MTBC)
- *Mycobacterium tuberculosis* complex (MTBC) comprises closely related bacterial species causing tuberculosis (TB) in humans and animals.
- TB remains a leading cause of infectious disease morbidity and mortality worldwide.
- MTBC is characterized by a **highly clonal genome**, with limited horizontal gene transfer, making genomic analysis crucial for understanding evolution, transmission, and drug resistance.
- **Genome size:** ~4.4 Mbp  
- **GC content:** ~65%
- Comparative genomics reveals **lineage-specific SNPs**, large sequence polymorphisms (LSPs), and regions of difference (RDs) that are useful for strain typing and epidemiology.
- Whole-genome sequencing (WGS) enables:
  - Identification of **drug-resistance mutations** in key genes (e.g., *rpoB*, *katG*, *inhA*).
  - Phylogenetic analysis to trace transmission chains and outbreak sources.
  - Detection of genomic diversity and microevolution within hosts and populations.
- MTBC genomics informs public health strategies, including **molecular surveillance, outbreak investigation, and personalized TB treatment**.

### MTBC Species and *M. tuberculosis* Lineages
#### MTBC Species
The *Mycobacterium tuberculosis* complex (MTBC) includes several closely related species:  
- *Mycobacterium tuberculosis* â€“ the main human pathogen  
- *Mycobacterium bovis* â€“ primarily infects cattle, can cause zoonotic TB in humans  
- *Mycobacterium africanum* â€“ restricted to West Africa, causes human TB  
- *Mycobacterium canettii* â€“ rare, mostly in East Africa, ancestral-like strains  
- *Mycobacterium microti* â€“ primarily infects voles, occasionally humans  
- *Mycobacterium pinnipedii* â€“ infects seals, rarely humans  
- *Mycobacterium caprae* â€“ mainly infects goats, occasionally humans  
#### *M. tuberculosis* Lineages
Genomic studies have identified **lineages based on SNPs and phylogeny**:  
1. **Lineage 1 (Indo-Oceanic)** â€“ East Africa, India, Southeast Asia  
2. **Lineage 2 (East-Asian / Beijing)** â€“ East Asia, associated with drug resistance  
3. **Lineage 3 (East-African-Indian)** â€“ South Asia, East Africa  
4. **Lineage 4 (Euro-American)** â€“ Worldwide, highly prevalent  
5. **Lineage 5 (West African 1 / *M. africanum*)** â€“ West Africa  
6. **Lineage 6 (West African 2 / *M. africanum*)** â€“ West Africa  
7. **Lineage 7** â€“ restricted to Ethiopia  

> Understanding MTBC species and lineages is critical for **epidemiology, phylogenetics, and drug-resistance surveillance**.

#### *Mycobacterium tuberculosis* Sublineages
*M. tuberculosis* lineages are further divided into **sublineages** based on phylogenetic SNP markers:  
- **Lineage 1 (Indo-Oceanic)**  
  - Sublineages: L1.1, L1.2  
- **Lineage 2 (East-Asian / Beijing)**  
  - Sublineages: L2.1 (Proto-Beijing), L2.2 (Modern Beijing)  
- **Lineage 3 (East-African-Indian)**  
  - Sublineages: L3.1, L3.2  
- **Lineage 4 (Euro-American)**  
  - Sublineages: L4.1, L4.2, L4.3, L4.4, L4.5, etc.  
- **Lineage 5â€“7**  
  - Mainly restricted to West Africa and Ethiopia, with fewer sublineages described  

> Sublineages help in **tracking transmission chains, outbreak sources, and population structure**.

#### *Mycobacterium tuberculosis* Drug-Resistance Types

| Abbreviation | Full Name | Description |
|-------------|-----------|-------------|
| **HR-TB** | Isoniazid-Resistant TB | Resistant to **isoniazid** only. |
| **RR-TB** | Rifampicin-Resistant TB | Resistant to **rifampicin**, with or without resistance to other drugs. |
| **MDR-TB** | Multidrug-Resistant TB | Resistant to **both isoniazid and rifampicin**, the two key first-line drugs. |
| **Pre-XDR-TB** | Pre-Extensively Drug-Resistant TB | MDR TB that is additionally resistant to **any fluoroquinolone** or **any second-line injectable drug**. |
| **XDR-TB** | Extensively Drug-Resistant TB | MDR TB that is resistant to **both fluoroquinolones and at least one second-line injectable drug**. |
| **Other** | Other Drug-Resistance Patterns | Includes **mono-resistant TB, poly-drug resistant TB**, or rare resistance profiles not covered above. |

> These categories are essential for **treatment planning, surveillance, and genomic interpretation** of TB drug resistance.

#### Drug-Resistance Mutations
> Genomic analysis can identify specific **mutations associated with these resistance types**, enabling rapid and precise drug-resistance prediction.
- Key genes associated with resistance:  
  - **Rifampicin:** *rpoB* mutations (e.g., S450L)  
  - **Isoniazid:** *katG* (S315T), *inhA* promoter mutations  
  - **Ethambutol:** *embB* (M306V/I)  
  - **Pyrazinamide:** *pncA* mutations  
  - **Fluoroquinolones:** *gyrA/gyrB* mutations  

- Mutations are catalogued and updated by the **World Health Organization (WHO)** for standardized drug-resistance interpretation.  

- **Catalogue of mutations in Mycobacterium tuberculosis complex and their association with drug resistance, 2nd ed (2023):  
  [https://github.com/GTB-tbsequencing/mutation-catalogue-2023/tree/main/Final%20Result%20Files]



### Download Data from NCBI and EBI


### ðŸ“¥ Download Data from NCBI and EBI

---

#### Using SRA Explorer for few size samples 

1. Go to **[SRA Explorer](https://sra-explorer.info/#)**  
2. Search for:  Bioproject number
3. set Max Results into 500 

4. In the search results, check the all boxes:  
- âœ… *WGS of Klebsiella pneumoniae isolate*  
- âœ… *Add to collection*  
- âœ… * go to data saved*  
- âœ… *Bash script for downloading FastQ files*  

4. Download the generated Bash script (e.g., `sra_download.sh`)  
5. Run the script to download the FASTQ file(s):  

##  Get the Run Accessions

###  1. Get all ERR run IDs from ENA
```bash
curl -s "https://www.ebi.ac.uk/ena/portal/api/filereport?accession=PRJEB3334&result=read_run&fields=run_accession" | tail -n +2 > runs.txt
```
# Fetch all SRR run accessions for the BioProject
```bash
esearch -db sra -query PRJNA787062 | efetch -format runinfo | cut -d',' -f1 | grep ^SRR > runs.txt
```
# Download & Compress

# Once runs.txt is ready:

```bash
nano download_sra.sh
```
```bash
#!/bin/bash
set -euo pipefail

# Number of threads for fasterq-dump / pigz
THREADS=4

# Directory for FASTQ files
OUTDIR="fastq_files"
mkdir -p "$OUTDIR"

# File containing run accessions
RUNS="SRR_Acc_List.txt"

# Directory where SRA files are stored by default
SRADIR=~/ncbi/public/sra

# Loop over each accession
while read -r ACC; do
    echo "==> Processing $ACC ..."

    # Skip download if SRA file already exists
    if [ -f "$SRADIR/$ACC.sra" ]; then
        echo "SRA file $ACC.sra already exists, skipping prefetch."
    else
        echo "Downloading $ACC.sra ..."
        prefetch --max-size 100G "$ACC"
    fi

    # Skip conversion if FASTQ already exists
    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        echo "FASTQ for $ACC already exists, skipping fasterq-dump."
    else
        # Convert to FASTQ (split paired-end reads)
        fasterq-dump "$ACC" --split-files -e "$THREADS" -O "$OUTDIR"

        # Compress FASTQ files
        if command -v pigz &> /dev/null; then
            pigz -p "$THREADS" "$OUTDIR"/${ACC}*.fastq
        else
            gzip "$OUTDIR"/${ACC}*.fastq
        fi
    fi

    # Optional: remove SRA file after successful FASTQ creation
    if ls "$OUTDIR"/${ACC}*.fastq.gz 1> /dev/null 2>&1; then
        rm -f "$SRADIR/$ACC.sra"
    fi

    echo "==> Completed $ACC"
    echo
done < "$RUNS"

echo "ðŸŽ‰ All downloads and conversions completed!"
```

Run your script in the background with nohup

Save all output and errors to run.log.

Keep it running even if you close the terminal.
```bash
nohup bash download_sra.sh > run.log 2>&1 &
```
To check progress:
```bash
tail -f run.log
```
#To see if itâ€™s still running:
```bash
ps aux | grep download_sra.sh
```
#
#
#
#
#
#
#
#
#
#
#
Checking if all of my fastq.gz file will pair correctly with out problem

cd ~/Genomics_project/TB/fastq_data/f_invio


# 1ï¸âƒ£ Open nano to create a new script

nano check_fastq_pairs.sh

# 2. Paste this inside nano
#!/bin/bash
set -euo pipefail

INDIR="raw_data"

# Only cd if not already in raw_data
if [[ "$(basename "$PWD")" != "raw_data" ]]; then
    cd "$INDIR" || { echo "âŒ raw_data directory not found"; exit 1; }
fi

echo "ðŸ” Checking FASTQ pairings in $PWD ..."

MISSING=false
PAIRED_COUNT=0
TOTAL_COUNT=0

# Loop over common R1 patterns
for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    TOTAL_COUNT=$((TOTAL_COUNT+1))

    # Extract sample name by removing R1/1/001 suffix
    SAMPLE=${R1%_1.fastq.gz}
    SAMPLE=${SAMPLE%_R1.fastq.gz}
    SAMPLE=${SAMPLE%_R1_*.fastq.gz}
    SAMPLE=${SAMPLE%_001.fastq.gz}
    SAMPLE=${SAMPLE%_R1_001.fastq.gz}

    # Look for corresponding R2
    if [[ -f "${SAMPLE}_2.fastq.gz" || -f "${SAMPLE}_R2.fastq.gz" || -f "${SAMPLE}_R2_*.fastq.gz" || -f "${SAMPLE}_002.fastq.gz" ]]; then
        echo "âœ… $SAMPLE â€” paired"
        PAIRED_COUNT=$((PAIRED_COUNT+1))
    else
        echo "âŒ $SAMPLE â€” missing R2 file"
        MISSING=true
    fi
done

echo -e "\nTotal samples checked: $TOTAL_COUNT"
echo "Correctly paired samples: $PAIRED_COUNT"

if [ "$MISSING" = true ]; then
    echo "âš  Some samples are missing pairs. Fix before running fastp."
else
    echo "âœ… All FASTQ files are correctly paired."
fi



# 4ï¸âƒ£ Make the script executable
chmod +x check_fastq_pairs.sh

# 5ï¸âƒ£ Run the script
./check_fastq_pairs.sh


# minimum, maximum, and average read length for gzipped FASTQ files using awk
# We can loop over all _1.fastq.gz files, find the corresponding _2.fastq.gz, calculate min, max, avg read lengths for both, and save the results as a CSV file.

# 1ï¸âƒ£ Open nano to create a new script

nano fastq_read_length_summary.sh

# 2ï¸âƒ£ Paste the following code into nano
#!/bin/bash
FASTQ_DIR="."                          # Current directory with FASTQ files
OUTDIR="read_length_summary"           # New output directory
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

# Create output directory if it doesn't exist
mkdir -p "$OUTDIR"

# CSV header
echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

# Loop over all _1.fastq.gz files
for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.trim.fastq.gz)
    R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        # Function to calculate min, max, avg read length
        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
        }

        # Calculate stats for R1 and R2
        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        # Append results to CSV
        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "âš  Missing R2 for $SAMPLE, skipping."
    fi
done

echo "âœ… Read length summary saved to $OUTPUT_CSV"

echo "âœ… Read length summary saved to $OUTPUT_CSV"
# 3ï¸âƒ£ Save and exit nano

    Press Ctrl + O â†’ Enter (to write the file)

    Press Ctrl + X â†’ Exit nano

# 4ï¸âƒ£ Make the script executable

chmod +x fastq_read_length_summary.sh

# 5ï¸âƒ£ Run the script

./fastq_read_length_summary.sh

# Visualize the readlength i some way
#
#
#
#
#
#
#
#
#
#

# FASTP 

# 1. Open nano and create the script

nano run_fastp.sh

# 2. Paste this inside nano

#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_70"
mkdir -p "$OUTDIR"

SKIPPED=()      # Array to track skipped samples
PROCESSED=0     # Counter for processed samples
ALREADY_DONE=() # Samples skipped because results already exist

# Number of parallel jobs (process 2 samples at a time)
JOBS=2

process_sample() {
    local R1="$1"
    local SAMPLE="$2"
    local R2="$3"

    # Skip if already processed (check trimmed file or report)
    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "â© Skipping $SAMPLE (already analyzed)."
        ALREADY_DONE+=("$SAMPLE")
        return
    fi

    echo "âœ… Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --detect_adapter_for_pe \
        --thread 8 \
        --cut_mean_quality 20 \
        --cut_front \
        --cut_tail \
        --cut_window_size 4 \
        --qualified_quality_phred 25 \
        --unqualified_percent_limit 40 \
        --length_required 70

    PROCESSED=$((PROCESSED + 1))
}

# Loop over all R1 files in raw_data
for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz "$INDIR"/*_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1")
    SAMPLE=${SAMPLE%%_1.fastq.gz}
    SAMPLE=${SAMPLE%%_R1.fastq.gz}
    SAMPLE=${SAMPLE%%_001.fastq.gz}

    # Find matching R2 file
    if [[ -f "$INDIR/${SAMPLE}_2.fastq.gz" ]]; then
        R2="$INDIR/${SAMPLE}_2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2.fastq.gz" ]]; then
        R2="$INDIR/${SAMPLE}_R2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_002.fastq.gz" ]]; then
        R2="$INDIR/${SAMPLE}_002.fastq.gz"
    else
        echo "âš  No R2 file found for $SAMPLE â€” skipping."
        SKIPPED+=("$SAMPLE")
        continue
    fi

    # Run in background to allow parallel processing
    process_sample "$R1" "$SAMPLE" "$R2" &

    # Limit jobs to $JOBS at a time
    if [[ $(jobs -r -p | wc -l) -ge $JOBS ]]; then
        wait -n
    fi
done

# Wait for any remaining jobs
wait

# Summary
echo -e "\n===== Summary ====="
echo "Total samples processed: $PROCESSED"
if [ ${#ALREADY_DONE[@]} -ne 0 ]; then
    echo "Samples skipped (already analyzed):"
    for s in "${ALREADY_DONE[@]}"; do
        echo "  - $s"
    done
fi
if [ ${#SKIPPED[@]} -ne 0 ]; then
    echo "Samples skipped (missing R2 file):"
    for s in "${SKIPPED[@]}"; do
        echo "  - $s"
    done
fi


# 3. Save & exit nano

    Press CTRL+O, Enter (save)

    Press CTRL+X (exit)

# 4. Make the script executable

chmod +x run_fastp.sh

# 5. Activate your conda env and run

conda activate fastp_env
./run_fastp.sh


#run MultiQC on all the files inside fastp_results directory, saving the report into a folder called multiqc_output.

#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_70"
OUTPUT_DIR="multiqc_output"

mkdir -p "$OUTPUT_DIR"

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"




# Sometimes not all fastq.gz file in the directory processed by fastp so another code prepared to run for it 

# 1. Open nano and create the script

nano run_fastp2.sh

# 2. Paste this inside nano

#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_70"
mkdir -p "$OUTDIR"

SAMPLES=()

# Detect R1 files under common naming schemes
for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz "$INDIR"/*_001.fastq.gz "$INDIR"/*_R1_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1")
    SAMPLE=${SAMPLE%%_1.fastq.gz}
    SAMPLE=${SAMPLE%%_R1.fastq.gz}
    SAMPLE=${SAMPLE%%_001.fastq.gz}
    SAMPLE=${SAMPLE%%_R1_001.fastq.gz}

    # Find R2 match
    if   [[ -f "$INDIR/${SAMPLE}_2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_002.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_002.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2_001.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2_001.fastq.gz"
    else
        echo "âš  No R2 file found for $SAMPLE â€” skipping."
        continue
    fi

    # Skip if already processed
    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "â© Skipping $SAMPLE (already processed)."
        continue
    fi

    # Save sample info for parallel execution
    SAMPLES+=("$SAMPLE,$R1,$R2")
done

# Export function to run fastp on one sample
run_fastp() {
    SAMPLE=$1
    R1=$2
    R2=$3
    echo "âœ… Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 70 \
        --max_len1 150 \
        --max_len2 150 \
        --qualified_quality_phred 20 \
        --detect_adapter_for_pe \
        --thread 8
}

export -f run_fastp
export OUTDIR

# Run in parallel: adjust -j to control how many samples run at once
printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}


# 3. Save & exit nano

    Press CTRL+O, Enter (save)

    Press CTRL+X (exit)

# 4. Make the script executable

chmod +x run_fastp2.sh

# 5. Activate your conda env and run

conda activate fastp_env
./run_fastp2.sh

# ðŸ”§ Notes

# This script will:

# Detect R1/R2 pairs

# Skip unpaired samples

# Skip already processed samples

# Run multiple fastp jobs at the same time

# The flag -j 3 means process 3 samples at once.
# Since each fastp is using --thread 8, this means youâ€™d use 24 threads total.
# Adjust -j based on how many CPU cores you have.




#run MultiQC on all the files inside fastp_results directory, saving the report into a folder called multiqc_output.

#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_70"
OUTPUT_DIR="multiqc_output"

mkdir -p "$OUTPUT_DIR"

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"
#
#
#
#
#
#
#
#
#
#
#
#
#
#

#SNIPPY

#1. Open nano to create the script

nano run_snippy.sh

# 2. Paste this inside nano:

#!/bin/bash
set -euo pipefail

# ======== CONFIG ========
REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_70"
OUTDIR="snippy_results"
THREADS=8         # threads *inside* each Snippy job
BWA_THREADS=30
JOBS=4            # how many samples to run at once
# ========================

mkdir -p "$OUTDIR"

# Function: run snippy for a single sample
run_snippy_sample() {
    SAMPLE="$1"
    R1="${FASTP_DIR}/${SAMPLE}_1.trim.fastq.gz"
    R2="${FASTP_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R1" && -f "$R2" ]]; then
        echo "Running snippy on sample: $SAMPLE"
        TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
        mkdir -p "$TMP_DIR"

        snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
               --R1 "$R1" --R2 "$R2" --force --bwaopt "-T $BWA_THREADS"

        # Rename and move outputs
        for f in "$TMP_DIR"/*; do
            base=$(basename "$f")
            case "$base" in
                snps.vcf) newname="${SAMPLE}.snps.vcf" ;;
                snps.tab) newname="${SAMPLE}.snps.tab" ;;
                consensus.fa) newname="${SAMPLE}.consensus.fa" ;;
                *.bam) newname="${SAMPLE}.bam" ;;
                *.bam.bai) newname="${SAMPLE}.bam.bai" ;;
                *) continue ;;
            esac
            mv "$f" "${OUTDIR}/${newname}"
        done

        rm -rf "$TMP_DIR"

        if [[ -f "${OUTDIR}/${SAMPLE}.snps.vcf" ]]; then
            echo "âœ… VCF generated for $SAMPLE"
        else
            echo "âš  No VCF produced for $SAMPLE"
        fi
    else
        echo "âš  Missing R1/R2 for $SAMPLE"
    fi
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

# Run samples in parallel
ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' \
    | parallel -j "$JOBS" run_snippy_sample {}

# Step 2: Verification
echo "Verifying completeness..."
ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' | sort > fastq_samples.txt
ls "${OUTDIR}"/*.snps.vcf 2>/dev/null \
    | sed 's|.*/||; s/\.snps\.vcf//' | sort > snippy_samples.txt

echo "FASTQ pairs count: $(wc -l < fastq_samples.txt)"
echo "Snippy outputs count: $(wc -l < snippy_samples.txt)"

if diff fastq_samples.txt snippy_samples.txt >/dev/null; then
    echo "âœ… All FASTQ pairs have corresponding Snippy results."
else
    echo "âš  Missing samples detected:"
    diff fastq_samples.txt snippy_samples.txt || true
fi

rm -f fastq_samples.txt snippy_samples.txt

echo "ðŸŽ¯ All steps completed!"
echo "Snippy results are in: ${OUTDIR}/"


# 3. Save and exit nano

    Press Ctrl + O, then Enter

    Press Ctrl + X

# 4. Make it executable:

chmod +x run_snippy.sh

# 5. Activate your Snippy environment and run:

conda activate snippy_env
./run_snippy.sh


#
#
#
#
#
#
#
#
#

# qualimap BAM QC

# 1. Open nano to create the script

nano run_qualimap.sh

# 2. Paste this inside nano:
#!/bin/bash
set -euo pipefail

# Directory with snippy BAM files
SNIPPY_DIR="all_bams"

# Output directory for qualimap reports
QUALIMAP_OUT="qualimap_reports"
mkdir -p "$QUALIMAP_OUT"

for bam in "$SNIPPY_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Running Qualimap BAM QC for sample: $sample"

    outdir="${QUALIMAP_OUT}/${sample}"
    mkdir -p "$outdir"

    # Run Qualimap (both HTML and PDF for completeness, lowercase format name)
    qualimap bamqc \
        -bam "$bam" \
        -outdir "$outdir" \
        -outformat pdf:html \
        --java-mem-size=4G

    # MultiQC will look inside "$outdir" for raw_data folder
done

3. Save and exit nano

    Press Ctrl + O, then Enter

    Press Ctrl + X
4. Make it executable:

chmod +x run_qualimap.sh

5. Activate your Snippy environment and run:

conda activate qualimap_env
./run_qualimap.sh
#
#
#
#
#
#
#
#
#
#
# MultiQC 
multiqc . -o multiqc_report

#1ï¸âƒ£ Create or edit the script

nano run_tb_variant_filter.sh

# 2ï¸âƒ£ Paste this code

#!/bin/bash

# Activate tb_variant_filter environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate tb_variant_filter_env

# Current directory
CURDIR=$(pwd)

# Output directory
OUTDIR="$CURDIR/tb_variant_filter_results"
mkdir -p "$OUTDIR"

# Loop through all VCF files in snippy_results
for vcf in "$CURDIR/snippy_results"/*.vcf; do
    sample=$(basename "$vcf")
    echo "Filtering $vcf ..."
    tb_variant_filter "$vcf" "$OUTDIR/${sample%.vcf}.filtered.vcf"
done

echo "All VCFs filtered and saved in $OUTDIR"


echo "All VCFs filtered and saved in $OUTDIR"

3ï¸âƒ£ Make it executable

chmod +x run_tb_variant_filter.sh

4ï¸âƒ£ Run the script
conda activate tb_variant_filter_env
./run_tb_variant_filter.sh

#
#
#
#
#
#
#
#
#
#
# TBPROFILER FROM BAM FILE

#1ï¸âƒ£ Create or edit the script

nano run_tbprofiler_on_snippy_bams.sh

# 2ï¸âƒ£ Paste this code

#!/bin/bash

set -euo pipefail

BAM_DIR="snippy_results"

for bam in "$BAM_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Processing sample: $sample"

    # Run tb-profiler profile with spoligotype and TXT output
    tb-profiler profile \
        --bam "$bam" \
        --prefix "$sample" \
        --txt
done
3ï¸âƒ£ Make it executable

chmod +x run_tbprofiler_on_snippy_bams.sh

4ï¸âƒ£ Run the script
conda activate tbprofiler_env
./run_tbprofiler_on_snippy_bams.sh

#
#
#
#
#
#
#
#
#
#
## bcftools concensus generation

#  Compress and index each filtered VCF
for vcf in tb_variant_filter_results/*.vcf; do
    bgzip -c "$vcf" > "${vcf}.gz"
    bcftools index "${vcf}.gz"
done
# Create a script to generate consensus sequences:
nano generate_consensus_all.sh

# Paste this:
#!/bin/bash

# Activate bcftools environment
source $(conda info --base)/etc/profile.d/conda.sh
conda activate tb_consensus_env

# Current directory
CURDIR=$(pwd)

# Directories
VCFDIR="$CURDIR/tb_variant_filter_results"
OUTDIR="$CURDIR/consensus_sequences"
mkdir -p "$OUTDIR"

# Loop through all filtered VCFs
for vcf in "$VCFDIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)
    echo "Processing $sample ..."
    # Compress VCF with bgzip
    bgzip -c "$vcf" > "$vcf.gz"
    # Index the compressed VCF
    bcftools index "$vcf.gz"
    # Generate consensus FASTA
    bcftools consensus -f "$CURDIR/H37Rv.fasta" "$vcf.gz" \
        | sed "1s/.*/>$sample/" \
        > "$OUTDIR/${sample}.consensus.fasta"

    echo "$sample consensus generated with sample label."
done

echo "All consensus sequences saved in $OUTDIR with sample-based headers."



4ï¸âƒ£ Make the script executable

chmod +x generate_consensus_all.sh


5ï¸âƒ£ Run the script
conda activate tb_consensus_env
./generate_consensus_all.sh


#  check the length of each consensus FASTA using seqkit, bioawk, or even awk/grep. 
#  1ï¸âƒ£ Using grep and wc
for f in consensus_sequences/*.fasta; do
    sample=$(basename "$f")
    # Remove header lines and count remaining characters
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample : $length bp"
done


# MAFFT v7.490 does not accept multiple input files on the command line. It expects a single FASTA file containing all sequences. Passing multiple paths causes it to think the filenames are options.

# The correct approach is to merge all consensus FASTAs into one file, then run MAFFT on that single file.
# Step 1: Merge all consensus FASTAs

cat consensus_sequences/*.fasta > consensus_sequences/all_consensus.fasta

# Step 2: Run MAFFT

mafft --auto --reorder --thread -1 consensus_sequences/all_consensus.fasta > consensus_sequences/aligned_consensus.fasta
#  Step 3: Verify

head consensus_sequences/aligned_consensus.fasta
