# 🧬 MTB-Illumina-WGS-Analysis  

![GitHub last commit](https://img.shields.io/github/last-commit/betselotz/MTB-Illumina-WGS-Analysis)  

Bioinformatics workflow for analyzing *Mycobacterium tuberculosis* whole-genome sequences (WGS) generated by Illumina and available on NCBI.  

---  

Written by: [**Betselot Zerihun Ayano**](https://github.com/betselotz)  

---  

## Introduction – *Mycobacterium tuberculosis* Complex (MTBC)  

<details>
  <summary>🦠 MTBC & TB Genomics Overview</summary>

- **🧬 Mycobacterium tuberculosis complex (MTBC):** Closely related species causing TB in humans & animals.  
- **🌍 Global Impact:** TB remains a leading cause of infectious disease morbidity & mortality.  
- **🔒 Highly Clonal Genome:** Limited horizontal gene transfer → genomic analysis critical for evolution, transmission & drug resistance.  
- **📏 Genome Size:** ~4.4 Mbp  
- **💎 GC Content:** ~65%  
- **🔍 Comparative Genomics:** Identifies lineage-specific SNPs, large sequence polymorphisms (LSPs), and regions of difference (RDs) → useful for strain typing & epidemiology.  
- **💻 Whole-Genome Sequencing (WGS) Enables:**  
  - 💊 Detect drug-resistance mutations (e.g., *rpoB*, *katG*, *inhA*)  
  - 🌳 Phylogenetic analysis to trace transmission & outbreaks  
  - 🧩 Detect genomic diversity & microevolution within hosts/populations  
- **🏥 Public Health Impact:** MTBC genomics informs molecular surveillance, outbreak investigation, and personalized TB treatment.

</details>

---  

### MTBC Species  
<details>
  <summary>🦠 MTBC Species Overview</summary>

The **Mycobacterium tuberculosis complex (MTBC)** consists of several genetically related species that cause tuberculosis in humans and animals:  

- 🧍 **Mycobacterium tuberculosis** – the primary human pathogen; responsible for the majority of TB cases globally; primarily infects the lungs but can cause extrapulmonary TB.  
- 🐄 **Mycobacterium bovis** – mainly infects cattle; can cause zoonotic TB in humans through unpasteurized dairy or close contact; used in BCG vaccine derivation.  
- 🌍 **Mycobacterium africanum** – largely restricted to West Africa; causes human TB; less virulent than *M. tuberculosis*, but important regionally.  
- 🌿 **Mycobacterium canettii** – rare; mostly reported in East Africa; considered ancestral-like, showing higher genetic diversity; primarily human infections.  
- 🐀 **Mycobacterium microti** – primarily infects wild rodents (voles); occasionally causes TB in humans; used in some experimental TB models.  
- 🦭 **Mycobacterium pinnipedii** – infects seals and other pinnipeds; zoonotic infections are rare in humans.  
- 🐐 **Mycobacterium caprae** – mainly infects goats and other livestock; can occasionally infect humans; prevalent in parts of Europe.  
- 🐖 **Mycobacterium orygis** – infects various wild and domestic animals including antelopes, cattle, and occasionally humans; mainly reported in South Asia.  
- 🐂 **Mycobacterium mungi** – infects banded mongooses in Africa; causes wildlife TB; rare human relevance.  
- 🐘 **Mycobacterium suricattae** – infects meerkats in Southern Africa; wildlife pathogen with ecological importance.  
- 🐏 **Mycobacterium caprae-like strains** – occasionally distinguished in livestock or wildlife; emphasizes host-adapted diversity within MTBC.  

These species differ in **host preference, geographic distribution, virulence, and zoonotic potential**, highlighting the importance of genomic surveillance and comparative studies for TB control.  

</details>

---  

### *M. tuberculosis* Lineages  

<details>
  <summary>🌐 MTBC Lineages Overview</summary>

Genomic studies have identified **10 major lineages** of MTBC based on SNPs, large sequence polymorphisms (LSPs), and phylogeny. These lineages show differences in **geographic distribution, host adaptation, virulence, and drug-resistance association**:  

1. 🟢 **Lineage 1 (Indo-Oceanic / EAI – East African-Indian)** – Predominantly found in East Africa, India, and Southeast Asia; associated with slow progression of disease in some regions.  
2. 🔴 **Lineage 2 (East-Asian / Beijing)** – Widespread in East Asia and parts of Russia; often associated with multidrug resistance and high virulence.  
3. 🔵 **Lineage 3 (East-African-Indian / CAS – Central Asian Strain)** – Found in South Asia and East Africa; linked to moderate transmission and some drug-resistance profiles.  
4. 🟡 **Lineage 4 (Euro-American / LAM, Haarlem, X-type, T-type)** – Globally widespread; highly successful in transmission; includes multiple sublineages.  
5. 🟣 **Lineage 5 (West African 1 / *M. africanum* type 1)** – Mostly restricted to West Africa; slower progression of TB; less virulent than L4.  
6. 🟤 **Lineage 6 (West African 2 / *M. africanum* type 2)** – West Africa; similar epidemiology to Lineage 5; historically associated with specific ethnic populations.  
7. 🟠 **Lineage 7** – Restricted to Ethiopia; ancient lineage with unique SNP markers; less studied, may have local host adaptation.  
8. 🟣 **Lineage 8** – Discovered in Rwanda and Uganda; deep-branching, highly ancestral; limited prevalence, provides insight into MTBC evolution.  
9. ⚫ **Lineage 9** – Identified in East Africa; rare; distinct SNP profile; contributes to regional TB genetic diversity.  
10. ⚪ **Lineage 10** – Recently reported in East/Central Africa; very limited information; potentially ancestral lineage with unique genomic characteristics.  

> Understanding MTBC species and lineages is critical for **epidemiology, phylogenetics, outbreak tracing, and drug-resistance surveillance**, as different lineages can show varying transmission dynamics and treatment outcomes.

</details>

---  

### *Mycobacterium tuberculosis* Sublineages  

<details>
  <summary>🔍 MTBC Sublineages</summary>
*M. tuberculosis* lineages are further divided into **sublineages** based on phylogenetic SNP markers, large sequence polymorphisms, and other genomic signatures. Sublineages reflect **microevolution, genomic diversity, and lineage-specific traits**:  

- 🟢 **Lineage 1 (Indo-Oceanic / EAI – East African-Indian):**  
  - Sublineages: L1.1, L1.2, L1.3  
  - Also called “East African-Indian” sublineages  
  - Show differences in SNP density, virulence markers, and drug-resistance loci  
- 🔴 **Lineage 2 (East-Asian / Beijing):**  
  - Sublineages: L2.1 (Proto-Beijing), L2.2 (Modern Beijing), L2.2.1, L2.2.2  
  - “Beijing” sublineages have distinct genomic deletions, IS6110 insertion patterns, and higher mutation rates contributing to drug resistance  
- 🔵 **Lineage 3 (East-African-Indian / CAS – Central Asian Strain):**  
  - Sublineages: L3.1, L3.2, L3.3  
  - Also called “CAS” strains  
  - Genomic variation includes lineage-specific SNPs and polymorphic loci influencing host-pathogen interactions  
- 🟡 **Lineage 4 (Euro-American / LAM, Haarlem, X-type, T-type):**  
  - Sublineages: L4.1–L4.10 (includes LAM, Haarlem, X-type, T-type sublineages)  
  - Exhibits high genomic diversity, including large sequence polymorphisms (LSPs) and sublineage-specific SNP clusters  
  - Provides resolution for fine-scale phylogenetic studies and outbreak tracing  
- 🟣 **Lineage 5 (West African 1 / *M. africanum* type 1):**  
  - Sublineages: L5.1, L5.2 (where described)  
  - Genomic differences include lineage-specific SNPs and regions of difference (RDs)  
  - Less virulent than Lineage 4, with specific adaptation markers  
- 🟤 **Lineage 6 (West African 2 / *M. africanum* type 2):**  
  - Sublineages: L6.1, L6.2 (limited description)  
  - Distinct SNP patterns and genomic deletions separate it from L5  
  - Important for studying regional adaptation and genomic evolution  
- 🟠 **Lineage 7:**  
  - Ancient lineage restricted to Ethiopia  
  - Unique SNP markers and genomic signatures distinguish it from other lineages  
  - Limited sublineage information due to low prevalence  
- 🟡 **Lineage 8:**  
  - Deep-branching, recently discovered in Rwanda and Uganda  
  - Shows ancestral genomic characteristics  
  - Genomic diversity includes lineage-specific SNPs and RDs  
- ⚫ **Lineage 9:**  
  - Rare, identified in East Africa  
  - Distinct SNP profile, showing microevolutionary divergence from other lineages  
  - Limited sublineage information  
- ⚪ **Lineage 10:**  
  - Recently reported in East/Central Africa  
  - Genomic features are ancestral with unique SNP and RD patterns  
  - Very limited prevalence; data still emerging  
> Sublineages capture **microevolutionary changes, genomic diversity, and lineage-specific features**, making them essential for high-resolution phylogenetics, drug-resistance studies, and molecular epidemiology.  

</details>


---  

### *Mycobacterium tuberculosis* Drug-Resistance Types  
<details>
  <summary>💊 TB Drug-Resistance Definitions</summary>

| Abbreviation | Full Name | Description |
|-------------|-----------|-------------|
| **HR-TB** | Isoniazid-Resistant TB | Resistant to **isoniazid** but susceptible to **rifampicin**, with or without resistance to other drugs |
| **RR-TB** | Rifampicin-Resistant TB | Resistant to **rifampicin**, with or without resistance to other drugs |
| **MDR-TB** | Multidrug-Resistant TB | Resistant to **both isoniazid and rifampicin** |
| **Pre-XDR-TB** | Pre-Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** or **second-line injectable** |
| **XDR-TB** | Extensively Drug-Resistant TB | MDR TB + resistance to **fluoroquinolone** + **injectable** |
| **Other** | Other Drug-Resistance Patterns | Includes mono-resistant TB, poly-resistant TB, and rare profiles |

> Understanding these resistance patterns is crucial for **treatment decisions and epidemiological surveillance**.

</details>

---  

### Drug-Resistance Mutations  

<details>
  <summary>🧬 Key TB Drug-Resistance Mutations</summary>

- **Rifampicin:** *rpoB* mutations (e.g., S450L)  
- **Isoniazid:** *katG* (S315T), *inhA* promoter mutations  
- **Ethambutol:** *embB* (M306V/I)  
- **Pyrazinamide:** *pncA* mutations  
- **Fluoroquinolones:** *gyrA/gyrB* mutations  

> Mutations are catalogued and updated by the **World Health Organization (WHO)** for standardized drug-resistance interpretation.

</details>

- **Catalogue of mutations in Mycobacterium tuberculosis complex and their association with drug resistance, 2nd ed (2023):**  
  [WHO TB Mutation Catalogue 2023](https://github.com/GTB-tbsequencing/mutation-catalogue-2023/tree/main/Final%20Result%20Files)  

---  



# 1️⃣ Download and Inspect FASTQ Data

##  Download Data from NCBI and ENA
Raw sequencing data for *Mycobacterium tuberculosis* can be accessed from public repositories such as **NCBI Sequence Read Archive (SRA)** and **European Nucleotide Archive (ENA) / EBI**. These repositories provide high-throughput sequencing datasets submitted by researchers worldwide.  
Key points:  
- **NCBI SRA** (https://www.ncbi.nlm.nih.gov/sra): Provides raw FASTQ or SRA files that can be downloaded using the `sra-tools` (`prefetch`, `fasterq-dump`) or via FTP.  
- **EBI / ENA** (https://www.ebi.ac.uk/ena/browser/home): Offers raw sequencing files (FASTQ) and metadata for projects submitted to Europe’s archive. Supports both browser-based and command-line downloads.  
- Using accession lists (BioProject, Run IDs), data can be downloaded in bulk for large-scale analysis.  
- Metadata includes sample source, sequencing platform, and study information, which is essential for downstream analysis and reproducibility.  

> **Tip:** Always check the sequencing platform and read layout (paired-end vs single-end) to ensure correct processing in your pipeline.  

There are multiple ways to download *Mycobacterium tuberculosis* sequencing data, depending on the dataset size and source. Below are two practical methods.

---

##### Change directory into your working directory
in our setting `~/Genomics_project/TB/fastq_data/f_invio` is our working directory all bioinformatics analysis conducted from this directory 
```bash
/media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1104194
```

#### Method 1: Using **SRA Explorer** (for small datasets)
1. Go to **[SRA Explorer](https://sra-explorer.info/#)**  
2. Search for a **BioProject number**, e.g., `PRJNA1201357`  
3. Set **Max Results** to `500`  
4. In the search results, check the following options:  
- ✅ *WGS of Mycobacterium tuberculosis*  
- ✅ *Add to collection*  
- ✅ *Go to data saved*  
- ✅ *Bash script for downloading FASTQ files*  
5. Download the generated Bash script (e.g., `sra_download.sh`)  
6. Run the script to download the FASTQ files:  

```bash
bash sra_download.sh
```
> **Tip:** Tip: This method is user-friendly and ideal for small projects (<500 samples). For large-scale datasets, use the command-line method below.
> **Tip:** Tip: Sometimes, we may need to download data directly from a BioSample list. In such cases, we can manually search for specific samples based on their metadata, then use the BioSample ID in SRA Explorer to download the corresponding fastq.gz files
#### Method 2: Using SRA Toolkit / ENA Run Accessions (for large datasets)

##### A. Get all run accessions from ENA

```bash
curl -s "https://www.ebi.ac.uk/ena/portal/api/filereport?accession=PRJEB3334&result=read_run&fields=run_accession" | tail -n +2 > runs.txt
```
<details>
<summary>🌍 Get Run Accessions from ENA</summary>

For **single-end trimmed reads**:

## Checking FASTQ

### FASTQ summary

This repository contains scripts and commands to **explore and summarize paired-end FASTQ files** for multiple samples, as well as individual single samples, in a bioinformatically meaningful way.

> ⚠️ Note: We do **not** need to run these scripts for single-end sequence datasets.

</details>

  
### 1. Peek at the first few reads
```bash
zcat raw_data/SRR28821350_1.fastq.gz | head -n 16
zcat raw_data/SRR28821350_2.fastq.gz | head -n 16
```
<details>
<summary>🔍 Inspect First Reads of FASTQ Files</summary>
- `zcat` is a Linux/Unix command used to **view the contents of compressed files** without manually uncompressing them.  
- Works primarily with `.gz` files (gzip-compressed).  
- Unlike `gunzip`, it **prints the uncompressed data to standard output** instead of creating a new file.  
- Example workflow:  
  - `zcat raw_data/SRR28821350_1.fastq.gz | head -n 16` →  Show the first 16 lines of R1 (forward) FASTQ file of a compressed file without extracting it. 


- Each read in FASTQ format consists of 4 lines:  
  1. Header line (`@`) with read ID  
  2. Sequence line  
  3. Separator line (`+`)  
  4. Quality scores line  
  - Useful for:  
  - Checking file format  
  - Inspecting sequences and quality  
  - Ensuring R1 and R2 files are correctly paired  
</details>


### 2. Count total reads
For single sample
```bash
echo $(( $(zcat raw_data/SRR28821350_1.fastq.gz | wc -l) / 4 ))
echo $(( $(zcat raw_data/SRR28821350_2.fastq.gz | wc -l) / 4 ))
```
<details>
<summary>📊 Counting Reads in FASTQ Files</summary>

- `zcat raw_data/SRR28821350_1.fastq.gz` → Decompresses the R1 FASTQ file and sends its content to standard output.  
- `| wc -l` → Counts the total number of lines in the decompressed file.  
- `$(( ... / 4 ))` → **Arithmetic expansion** in Bash: evaluates the expression inside `$(( ))`.  
  - Divides the total number of lines by 4 because **each sequencing read occupies 4 lines** in a FASTQ file:  
    1. Header line (`@`)  
    2. Sequence line  
    3. Separator line (`+`)  
    4. Quality scores line  
- `echo` → Prints the resulting number of reads.  
- The second command works the same for the R2 (reverse) FASTQ file.  
- Overall, these commands give a **quick read count** for paired-end sequencing files.  

</details>


For batch processing 
##### Step 1: Open a new script
```bash
nano count_reads.sh
```
##### Step 2: Paste the following code
This script counts reads in paired-end FASTQ files and saves results to a CSV.

```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"

OUTFILE="$OUTDIR/fastq_read_counts.csv"
echo "Sample,R1_reads,R2_reads" > "$OUTFILE"
echo "📊 Counting reads in FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.fastq\.gz//')
    R2=""
    for suffix in "_2.fastq.gz" "_R2.fastq.gz" "_R2_*.fastq.gz"; do
        [[ -f "$INDIR/${SAMPLE}${suffix}" ]] && R2="$INDIR/${SAMPLE}${suffix}" && break
    done
    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
    R2_COUNT=$([[ -n "$R2" ]] && echo $(( $(zcat "$R2" | wc -l) / 4 )) || echo "NA")
    echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTFILE"
    echo "✅ $SAMPLE → R1: $R1_COUNT | R2: $R2_COUNT"
done

echo "🎉 All done! Read counts saved to '$OUTFILE'"

```

<details>
<summary>📊 FASTQ Read Count Script Overview</summary>

- `INDIR="raw_data"` → Directory containing input FASTQ files.  
- `OUTDIR="csv_output"` → Directory where the CSV file will be saved; created automatically if it doesn’t exist.  
- `OUTFILE="$OUTDIR/fastq_read_counts.csv"` → CSV file to store read counts.  
- `echo "Sample,R1_reads,R2_reads" > "$OUTFILE"` → Creates the CSV header.  
- `echo "📊 Counting reads in FASTQ files from '$INDIR'..."` → Prints starting message.  
- `for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz; do ... done` → Loops through all R1 FASTQ files.  
- `[[ -f "$R1" ]] || continue` → Skips if the R1 file does not exist.  
- `SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.fastq\.gz//')` → Extracts sample name from the file name.  
- `for suffix in "_2.fastq.gz" "_R2.fastq.gz" "_R2_*.fastq.gz"; do ... done` → Finds the corresponding R2 file if it exists.  
- `R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))` → Counts reads in R1 by dividing total lines by 4.  
- `R2_COUNT=$([[ -n "$R2" ]] && echo $(( $(zcat "$R2" | wc -l) / 4 )) || echo "NA")` → Counts reads in R2 if present; otherwise outputs "NA".  
- `echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTFILE"` → Appends counts to the CSV file.  
- `echo "✅ $SAMPLE → R1: $R1_COUNT | R2: $R2_COUNT"` → Prints progress for each sample.  
- `echo "🎉 All done! Read counts saved to '$OUTFILE'"` → Prints completion message.  

</details>
If it is for **single read** we have replaced the following code instead of step 2

```bash
#!/bin/bash
set -euo pipefail

# Input and output directories
INDIR="raw_data"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"

# Output CSV file
OUTFILE="$OUTDIR/fastq_read_counts_single.csv"
echo "Sample,R1_reads" > "$OUTFILE"

echo "📊 Counting reads in single-end FASTQ files from '$INDIR'..."

# Loop over all FASTQ files
for R1 in "$INDIR"/*.fastq.gz; do
    [[ -f "$R1" ]] || continue

    # Extract sample name (remove .fastq.gz)
    SAMPLE=$(basename "$R1" | sed -E 's/\.fastq\.gz$//')

    # Count reads (4 lines per read in FASTQ)
    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))

    # Append counts to CSV
    echo "$SAMPLE,$R1_COUNT" >> "$OUTFILE"
    echo "✅ $SAMPLE → R1: $R1_COUNT"
done

echo "🎉 All done! Read counts saved to '$OUTFILE'"
```

##### Step 3: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano

##### Step 3: Make the script executable
```bash
chmod +x count_reads.sh
```
##### Step 4: Run the script
```bash
./count_reads.sh
```

### 3. Base composition

- **Assess sequencing quality** → Balanced A, T, G, C indicates good data.  
- **Detect contamination or bias** → Overrepresented bases may signal adapters, low-complexity regions, or contaminants.  
- **Guide quality control** → Helps decide if serious trimming or filtering is needed before analysis.  
- **Prevent downstream errors** → Ensures accurate mapping, variant calling, or assembly.  
✅ **Base composition check is a quick QC step that safeguards analysis quality.**

```bash
#!/bin/bash
for fq in raw_data/SRR28821350_1.fastq.gz raw_data/SRR28821350_2.fastq.gz; do
    [ -f "$fq" ] || continue
    echo "Counting bases in $fq..."
    zcat "$fq" | awk 'NR%4==2 { for(i=1;i<=length($0);i++) b[substr($0,i,1)]++ } 
    END { for(base in b) print base, b[base] }'
    echo "----------------------"
done
```
<details>
<summary>🧬 Nucleotide Counting Script Explanation</summary>

- `for fq in raw_data/SRR28821350_1.fastq.gz raw_data/SRR28821350_2.fastq.gz; do ... done` → Loops over the two specified FASTQ files (R1 and R2).  
- `[ -f "$fq" ] || continue` → Skips the iteration if the file does not exist.  
- `echo "Counting bases in $fq..."` → Prints which file is being processed.  
- `zcat "$fq"` → Decompresses the FASTQ file and streams its content to standard output.  
- `awk 'NR%4==2 { for(i=1;i<=length($0);i++) b[substr($0,i,1)]++ } END { for(base in b) print base, b[base] }'` → Counts nucleotides:  
  - `NR%4==2` → Only processes the **sequence line** of each FASTQ read.  
  - `for(i=1;i<=length($0);i++)` → Iterates over each nucleotide in the sequence line.  
  - `b[substr($0,i,1)]++` → Increments a counter for each base (A, T, G, C, N, or other).  
  - `END { for(base in b) print base, b[base] }` → Prints the total counts for each base after processing the file.  
- `echo "----------------------"` → Adds a visual separator between files for readability.  

</details>


### 4. Quality score summary
its good practice to quickly inspect base quality scores for the first few reads before full QC or analysis
- We may spot unusual patterns in base quality that may indicate issues with the sequencing run.
```bash
 First 10 quality lines
zcat raw_data/SRR28821350_1.fastq.gz | sed -n '4~4p' | head -n 10
zcat raw_data/SRR28821350_2.fastq.gz | sed -n '4~4p' | head -n 10
```
<details>
<summary>🔍 View FASTQ Quality Scores</summary>

- `zcat raw_data/SRR28821350_1.fastq.gz | sed -n '4~4p' | head -n 10`  
  - Decompresses R1 FASTQ.  
  - `sed -n '4~4p'` → Prints every 4th line starting from line 4 (the **quality score line** for each read).  
  - `head -n 10` → Shows only the first 10 quality lines for quick inspection.  

- `zcat raw_data/SRR28821350_2.fastq.gz | sed -n '4~4p' | head -n 10`  
  - Same as above, but for R2 FASTQ.  
</details>

 Count ASCII characters in quality lines
```bash
zcat raw_data/SRR28821350_1.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
zcat raw_data/SRR28821350_2.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
```
<details>
<summary>🔢 Count Quality Score Frequencies</summary>

- `zcat raw_data/SRR28821350_1.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'`  
  - Decompresses R1 FASTQ.  
  - `sed -n '4~4p'` → Selects every 4th line (the **quality line**).  
  - `awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'` → Counts occurrences of each quality score character.  

- `zcat raw_data/SRR28821350_2.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'`  
  - Same as above, but for R2 FASTQ.  
</details>


### 5.   Checking FASTQ Pairing 

We ensured all our FASTQ files are correctly paired before running any bioinformatics analysis.


##### Step 1: Create the script
```bash
nano check_fastq_pairs.sh
```
##### Step 2: Paste the following into `check_fastq_pairs.sh`
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
[[ "$(basename "$PWD")" != "raw_data" ]] && cd "$INDIR" || { echo "❌ raw_data directory not found"; exit 1; }

echo "🔍 Checking FASTQ pairings in $PWD ..."

MISSING=false
PAIRED_COUNT=0
TOTAL_COUNT=0

for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do
    [[ -f "$R1" ]] || continue
    TOTAL_COUNT=$((TOTAL_COUNT+1))
    SAMPLE=${R1%_1.fastq.gz}; SAMPLE=${SAMPLE%_R1.fastq.gz}; SAMPLE=${SAMPLE%_R1_*.fastq.gz}; SAMPLE=${SAMPLE%_001.fastq.gz}; SAMPLE=${SAMPLE%_R1_001.fastq.gz}

    if [[ -f "${SAMPLE}_2.fastq.gz" || -f "${SAMPLE}_R2.fastq.gz" || -f "${SAMPLE}_R2_*.fastq.gz" || -f "${SAMPLE}_002.fastq.gz" ]]; then
        echo "✅ $SAMPLE — paired"
        PAIRED_COUNT=$((PAIRED_COUNT+1))
    else
        echo "❌ $SAMPLE — missing R2 file"
        MISSING=true
    fi
done

echo -e "\nTotal samples checked: $TOTAL_COUNT"
echo "Correctly paired samples: $PAIRED_COUNT"
$MISSING && echo "⚠ Some samples are missing pairs. Fix before running fastp." || echo "✅ All FASTQ files are correctly paired."
```
<details>
<summary>🔗 FASTQ Pairing Check Script Explanation</summary>

- `#!/bin/bash` → Runs the script using Bash.  
- `set -euo pipefail` → Exits on errors, unset variables, or failed commands.  
- `INDIR="raw_data"` → Directory containing the FASTQ files.  
- `[[ "$(basename "$PWD")" != "raw_data" ]] && cd "$INDIR" ...` → Changes to `raw_data` directory if not already there.  
- `MISSING=false` → Flag to track if any R2 files are missing.  
- `PAIRED_COUNT=0` / `TOTAL_COUNT=0` → Counters for paired samples and total samples checked.  
- `for R1 in *_1.fastq.gz *_R1.fastq.gz *_R1_*.fastq.gz *_001.fastq.gz; do ... done` → Loops over all R1 FASTQ files.  
- `[[ -f "$R1" ]] || continue` → Skips if the file does not exist.  
- `SAMPLE=...` → Strips common R1 suffixes to extract the sample name.  
- `if [[ -f "${SAMPLE}_2.fastq.gz" || ... ]]; then ... fi` → Checks if a corresponding R2 file exists.  
- `echo "✅ $SAMPLE — paired"` → Prints a message if the pair is found.  
- `echo "❌ $SAMPLE — missing R2 file"` → Prints a message if the pair is missing and sets `MISSING=true`.  
- `TOTAL_COUNT` and `PAIRED_COUNT` → Track the total and successfully paired samples.  
- Final messages:  
  - `⚠ Some samples are missing pairs` → Warns user if any R2 files are missing.  
  - `✅ All FASTQ files are correctly paired` → Confirms all samples are paired.  

</details>


##### Step 4: Make the script executable
```bash
chmod +x check_fastq_pairs.sh
```
##### Step 5: Run the script
```bash
./check_fastq_pairs.sh
```
> **Tip:** Ensure all R1/R2 naming conventions in your directory match the patterns used in the script.  
> You can adjust the patterns (`*_1.fastq.gz`, `*_R1.fastq.gz`, etc.) if needed.


Calculating Minimum, Maximum, and Average Read Lengths for Paired-End Reads

### 5.   Checking raw FASTQ Read Length Summary

<details>
<summary>📏 Read Length Summary – Importance and Benefits</summary>

Before performing any downstream bioinformatics analysis, it is important to understand the quality and characteristics of your sequencing data. One key metric is the **read length** of FASTQ files.  

## 🔹 Why read length matters  
- **Minimum read length:** Identifies very short reads that may result from sequencing errors or trimming. Extremely short reads can cause mapping errors or low-quality variant calls.  
- **Maximum read length:** Confirms whether reads were sequenced to the expected length and detects unusually long reads that may indicate adapter contamination or sequencing artifacts.  
- **Average read length:** Provides an overall measure of sequencing quality and consistency across the dataset.  

## 🔹 Importance in paired-end sequencing  
Calculating these metrics for **both R1 and R2 reads** is crucial:  
- Ensures both reads in a pair are of comparable lengths → essential for accurate alignment and variant calling.  
- Detects discrepancies between forward and reverse reads that may indicate technical issues during sequencing or library preparation.  
- Allows early filtering of problematic samples before computationally intensive steps such as mapping, variant calling, or assembly.  

## 🔹 Benefits of summarizing into a CSV  
By compiling read lengths into a **CSV file**, you can:  
- Quickly inspect and compare samples.  
- Identify outliers or problematic datasets.  
- Make informed decisions on trimming, filtering, or quality control.  
- Improve reliability and reproducibility of downstream analyses.  

</details>

---

##### Step 1:  Open nano to create a new script
```bash
nano fastq_read_length_summary.sh
```
##### Step 2: Paste the following code into nano

```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.fastq.gz "$FASTQ_DIR"/*_R1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.fastq\.gz//')
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"
    [[ -f "$R2" ]] || R2="$FASTQ_DIR/${SAMPLE}_R2.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
        }

        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "⚠ Missing R2 for $SAMPLE, skipping."
    fi
done

echo "✅ Read length summary saved to $OUTPUT_CSV"

```
<details>
<summary>📊 Read Length Summary Script Explanation</summary>

- `FASTQ_DIR="raw_data"` → Directory containing FASTQ files.  
- `OUTDIR="csv_output"` → Directory to save CSV output; created automatically if missing.  
- `OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"` → Output CSV file path.  
- `mkdir -p "$OUTDIR"` → Ensure output directory exists.  
- `echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"` → CSV header.  
- `for R1 in "$FASTQ_DIR"/*_1.fastq.gz "$FASTQ_DIR"/*_R1.fastq.gz; do ...` → Loop over all R1 FASTQ files.  
- `[[ -f "$R1" ]] || continue` → Skip if R1 file does not exist.  
- `SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.fastq\.gz//')` → Extract sample name.  
- `R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"` → Get paired R2 filename; also checks `_R2` naming.  
- `if [[ -f "$R2" ]]; then ... else ... fi` → Skip sample if R2 is missing.  
- `calc_stats() { ... }` → Function to calculate min, max, avg read lengths for a FASTQ file.  
- `STATS_R1=$(calc_stats "$R1")` → Stats for R1.  
- `STATS_R2=$(calc_stats "$R2")` → Stats for R2.  
- `echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"` → Append sample stats to CSV.  
- `echo "⚠ Missing R2 for $SAMPLE, skipping."` → Warning if R2 missing.  
- `echo "✅ Read length summary saved to $OUTPUT_CSV"` → Final confirmation message.

</details>

If it is for **single read** we have replaced the following code instead of step 2
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

mkdir -p "$OUTDIR"
echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.fastq.gz "$FASTQ_DIR"/*_R1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.fastq\.gz//')
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"
    [[ -f "$R2" ]] || R2="$FASTQ_DIR/${SAMPLE}_R2.fastq.gz"

    echo "Processing sample $SAMPLE"

    calc_stats() {
        zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
    }

    STATS_R1=$(calc_stats "$R1")
    
    if [[ -f "$R2" ]]; then
        STATS_R2=$(calc_stats "$R2")
    else
        STATS_R2="NA,NA,NA"
        echo "⚠ R2 not found for $SAMPLE, treating as single-end."
    fi

    echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
done

echo "✅ Read length summary saved to $OUTPUT_CSV"
```

##### Step 3: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano
##### Step 4: Make the script executable
```bash
chmod +x fastq_read_length_summary.sh
```
##### Step 5: Run the script
```bash
./fastq_read_length_summary.sh
```



# 3️⃣ FASTP – Quality Control and Trimming

<details>
<summary>⚡ FASTQ Preprocessing with fastp</summary>

[`fastp`](https://github.com/OpenGene/fastp) is a widely used **FASTQ preprocessor** for quality control (QC), read trimming, and adapter removal. It is efficient, multithreaded, and provides both **JSON and HTML reports** for each sample.  

In this pipeline, `fastp` ensures that only **high-quality reads** are retained before mapping and variant calling. High-quality read preprocessing is crucial in *Mycobacterium tuberculosis* (TB) WGS analysis because poorly trimmed or unfiltered reads can lead to:
- False-positive SNP calls  
- Mapping errors, especially in repetitive regions (e.g., PE/PPE genes)  
- Biased coverage, affecting downstream variant interpretation  

### Advantages of fastp over Trimmomatic and other tools
- **All-in-one solution**: Handles trimming, adapter detection, filtering, and QC in a single step.  
- **Automatic adapter detection**: Reduces human error, especially for large TB projects with mixed sequencing batches.  
- **Speed and multithreading**: Written in C++, much faster than Java-based Trimmomatic.  
- **Comprehensive QC output**: HTML (interactive) and JSON (machine-readable) reports for quality distribution, duplication rates, adapter content, and polyG/polyX tails.  
- **Better polyG/polyX handling**: Important for Illumina NovaSeq/NextSeq data.  
- **UMI support**: Useful for advanced TB sequencing protocols using Unique Molecular Identifiers.  
- **Minimal parameter tuning**: Default settings are optimized, reducing manual adjustments.  

### Why this matters for TB analysis
- **Accurate SNP calling**: TB drug-resistance prediction relies on high-confidence SNPs.  
- **Low genetic diversity detection**: TB isolates often differ by only a few SNPs. Filtering errors while retaining true variants is critical.  
- **Scalability for large cohorts**: Efficient, reproducible preprocessing is essential for thousands of public TB isolates from ENA/NCBI.  

</details>

---

### Steps to Run FASTP
##### Step 1: **Open nano to create the script**
```bash
nano run_fastp.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_50"
mkdir -p "$OUTDIR"

SAMPLES=()

for R1 in "$INDIR"/*_1.fastq.gz "$INDIR"/*_R1.fastq.gz "$INDIR"/*_001.fastq.gz "$INDIR"/*_R1_001.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1")
    SAMPLE=${SAMPLE%%_1.fastq.gz}
    SAMPLE=${SAMPLE%%_R1.fastq.gz}
    SAMPLE=${SAMPLE%%_001.fastq.gz}
    SAMPLE=${SAMPLE%%_R1_001.fastq.gz}

    if   [[ -f "$INDIR/${SAMPLE}_2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_002.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_002.fastq.gz"
    elif [[ -f "$INDIR/${SAMPLE}_R2_001.fastq.gz" ]]; then R2="$INDIR/${SAMPLE}_R2_001.fastq.gz"
    else
        echo "⚠ No R2 file found for $SAMPLE — skipping."
        continue
    fi

    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "⏩ Skipping $SAMPLE (already processed)."
        continue
    fi

    SAMPLES+=("$SAMPLE,$R1,$R2")
done

if [[ ${#SAMPLES[@]} -eq 0 ]]; then
    echo "❌ No paired FASTQ files found in $INDIR"
    exit 1
fi

THREADS=$(nproc)
FASTP_THREADS=$(( THREADS / 2 ))

run_fastp() {
    SAMPLE=$1
    R1=$2
    R2=$3
    echo "✅ Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 50 \
        --qualified_quality_phred 20 \
        --detect_adapter_for_pe \
        --thread $FASTP_THREADS \
        &> "$OUTDIR/${SAMPLE}_fastp.log"
}

export -f run_fastp
export OUTDIR FASTP_THREADS

printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}

echo "🎉 Completed fastp for $(ls "$OUTDIR"/*_fastp.json | wc -l) samples."

```
<details>
<summary>🧹 fastp Trimming Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors, undefined variables, or failed pipelines.  
- `INDIR="raw_data"` → Raw FASTQ directory.  
- `OUTDIR="fastp_results_min_50"` → Directory for trimmed FASTQs.  
- `mkdir -p "$OUTDIR"` → Create output directory.  
- `SAMPLES=()` → Initialize array to store sample info.  
- `for R1 in ...` → Loop over R1 files with common naming patterns.  
- `SAMPLE=...` → Extract sample name from R1 filename.  
- `if ... elif ... else` → Detect corresponding R2 under multiple naming conventions.  
- `if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && ... ]]` → Skip already processed samples.  
- `SAMPLES+=("$SAMPLE,$R1,$R2")` → Store sample info for parallel execution.  
- `THREADS=$(nproc)` → Detect total CPU cores.  
- `FASTP_THREADS=$(( THREADS / 2 ))` → Allocate threads per fastp process.  
- `run_fastp() { ... }` → Function to run fastp per sample:  
  - `-i / -I` → Input R1/R2  
  - `-o / -O` → Output trimmed FASTQs  
  - `-h / -j` → HTML and JSON reports  
  - `--length_required 50` → Minimum read length  
  - `--qualified_quality_phred 20` → Quality threshold  
  - `--detect_adapter_for_pe` → Auto adapter trimming  
  - `--thread` → Threads for fastp  
- `export -f run_fastp` → Make function available to GNU Parallel.  
- `printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}` → Run 3 fastp jobs in parallel.  
- `echo "🎉 Completed fastp ..."` → Display completion message.

</details>

If it is for **single read** we have replaced the following code instead of step 2

```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_50"
mkdir -p "$OUTDIR"

SAMPLES=()

# Loop over all .fastq.gz files
for R1 in "$INDIR"/*.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1" .fastq.gz)

    # Skip already processed samples
    if [[ -f "$OUTDIR/${SAMPLE}.trim.fastq.gz" ]]; then
        echo "⏩ Skipping $SAMPLE (already processed)."
        continue
    fi

    SAMPLES+=("$SAMPLE,$R1")
done

if [[ ${#SAMPLES[@]} -eq 0 ]]; then
    echo "❌ No single-end FASTQ files found in $INDIR"
    exit 1
fi

THREADS=$(nproc)
FASTP_THREADS=$(( THREADS / 2 ))

run_fastp() {
    SAMPLE=$1
    R1=$2
    echo "✅ Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -o "$OUTDIR/${SAMPLE}.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 50 \
        --qualified_quality_phred 20 \
        --thread $FASTP_THREADS \
        &> "$OUTDIR/${SAMPLE}_fastp.log"
}

export -f run_fastp
export OUTDIR FASTP_THREADS

printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2}

echo "🎉 Completed fastp for $(ls "$OUTDIR"/*_fastp.json | wc -l) samples."

```
##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_fastp.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate fastp_env
./run_fastp.sh
```

Count R1 trimmed files
```bash
ls -lth fastp_results_min_50/*_1.trim.fastq.gz | wc -l
```
Count R2 trimmed files
```bash
ls -lth fastp_results_min_50/*_2.trim.fastq.gz | wc -l
```

View first 10 quality lines in trimmed FASTQ

```bash
zcat fastp_results_min_50/ET3_S55_1.trim.fastq.gz | sed -n '4~4p' | head -n 10
zcat fastp_results_min_50/ET3_S55_2.trim.fastq.gz | sed -n '4~4p' | head -n 10
```
<details>
  <summary>🔍 How it works</summary>

- `zcat` → Decompresses the trimmed FASTQ.  
- `sed -n '4~4p'` → Prints every 4th line starting from line 4 (the quality score line of each read).  
- `head -n 10` → Shows the first 10 quality lines for quick inspection.  

</details>

Count ASCII characters in quality lines:
```bash
zcat fastp_results_min_50/ET3_S55_1.trim.fastq.gz | sed -n '4~4p' | \
awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
zcat fastp_results_min_50/ET3_S55_2.trim.fastq.gz | sed -n '4~4p' | \
awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
```
<details>
  <summary>🔢 How it works</summary>

- `zcat` → Decompresses trimmed FASTQ.  
- `sed -n '4~4p'` → Selects every 4th line (quality score line).  
- `awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'` → Counts occurrences of each ASCII character in the quality scores.  

This helps quickly identify if the quality encoding is correct (usually Phred+33 for Illumina) and whether trimming improved overall quality.

</details>

For batch processing trimmed FASTQ
##### Step 1: Open a new script
```bash
nano count_trimmed_reads.sh
```
##### Step 2: Paste the following code
This script counts reads in trimmed paired-end FASTQ files and saves results to a CSV.
```bash
#!/bin/bash
set -euo pipefail

INDIR="fastp_results_min_50"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_reads,R2_reads" > "$OUTPUT_CSV"
echo "📊 Counting reads in trimmed FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*_1.trim.fastq.gz "$INDIR"/*_R1.trim.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.trim\.fastq\.gz//')
    R2=""
    for suffix in "_2.trim.fastq.gz" "_R2.trim.fastq.gz" "_R2_*.trim.fastq.gz"; do
        [[ -f "$INDIR/${SAMPLE}${suffix}" ]] && R2="$INDIR/${SAMPLE}${suffix}" && break
    done
    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
    R2_COUNT=$([[ -n "$R2" ]] && echo $(( $(zcat "$R2" | wc -l) / 4 )) || echo "NA")
    echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTPUT_CSV"
    echo "✅ $SAMPLE → R1: $R1_COUNT | R2: $R2_COUNT"
done

echo "🎉 All done! Read counts saved to '$OUTPUT_CSV'"

```
<details>
  <summary>📊 Trimmed FASTQ Read Count Script Explanation</summary>

- `#!/bin/bash` → Runs the script using Bash.  
- `set -euo pipefail` → Exits on errors, unset variables, or failed commands.  
- `INDIR="fastp_results_min_50"` → Directory containing trimmed FASTQ files.  
- `OUTDIR="csv_output"` → Directory to save the output CSV file.  
- `OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"` → Path of the output CSV file.  
- `mkdir -p "$OUTDIR"` → Creates the output directory if it doesn’t exist.  
- `echo "Sample,R1_reads,R2_reads" > "$OUTPUT_CSV"` → Writes the CSV header.  
- `echo "📊 Counting reads in trimmed FASTQ files from '$INDIR'..."` → Prints starting message.  
- `for R1 in "$INDIR"/*_1.trim.fastq.gz "$INDIR"/*_R1.trim.fastq.gz; do ... done` → Iterates over all R1 trimmed FASTQ files.  
- `[[ -f "$R1" ]] || continue` → Skips if the R1 file does not exist.  
- `SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.trim\.fastq\.gz//')` → Extracts the sample name from the filename.  
- `for suffix in "_2.trim.fastq.gz" "_R2.trim.fastq.gz" "_R2_*.trim.fastq.gz"; do ... done` → Searches for the corresponding R2 file.  
- `R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))` → Counts number of reads in R1 (lines divided by 4).  
- `R2_COUNT=$([[ -n "$R2" ]] && echo $(( $(zcat "$R2" | wc -l) / 4 )) || echo "NA")` → Counts number of reads in R2 if present, otherwise outputs "NA".  
- `echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTPUT_CSV"` → Appends results to the CSV file.  
- `echo "✅ $SAMPLE → R1: $R1_COUNT | R2: $R2_COUNT"` → Prints per-sample progress.  
- `echo "🎉 All done! Read counts saved to '$OUTPUT_CSV'"` → Prints final completion message.  

</details>

for single-end trimmed reads:
```bash
#!/bin/bash
set -euo pipefail

INDIR="fastp_results_min_50"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_reads" > "$OUTPUT_CSV"
echo "📊 Counting reads in trimmed FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*.trim.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/\.trim\.fastq\.gz//')
    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
    echo "$SAMPLE,$R1_COUNT" >> "$OUTPUT_CSV"
    echo "✅ $SAMPLE → R1: $R1_COUNT"
done

echo "🎉 All done! Read counts saved to '$OUTPUT_CSV'"
```

##### Step 3: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano

##### Step 4: Make the script executable
```bash
chmod +x count_trimmed_reads.sh
```
##### Step 5: Run the script
```bash
./count_trimmed_reads.sh
```
Using zcat and wc (line counts)
Each read in FASTQ has 4 lines, so dividing by 4 gives read count. 
```bash
RAW_R1_COUNT=$(( $(zcat raw_data/ET3_S55_1.fastq.gz | wc -l) / 4 ))
RAW_R2_COUNT=$(( $(zcat raw_data/ET3_S55_2.fastq.gz | wc -l) / 4 ))

TRIM_R1_COUNT=$(( $(zcat fastp_results_min_50/ET3_S55_1.trim.fastq.gz | wc -l) / 4 ))
TRIM_R2_COUNT=$(( $(zcat fastp_results_min_50/ET3_S55_2.trim.fastq.gz | wc -l) / 4 ))

echo "R1 trimmed reads: $(( RAW_R1_COUNT - TRIM_R1_COUNT ))"
echo "R2 trimmed reads: $(( RAW_R2_COUNT - TRIM_R2_COUNT ))"
```
read length summary on trimmed FASTQ files
##### Step 1: Open nano to create a new script
```bash
nano trimmed_fastq_read_length_summary.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="fastp_results_min_50"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.trim.fastq.gz)
    R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {
                len=length($0)
                sum+=len
                if(min==""){min=len}
                if(len<min){min=len}
                if(len>max){max=len}
                count++
            } END {
                avg=sum/count
                printf "%d,%d,%.2f", min, max, avg
            }'
        }

        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "⚠ Missing R2 for $SAMPLE, skipping."
    fi
done

echo "✅ Trimmed read length summary saved to $OUTPUT_CSV"

```
<details>
  <summary>📊 Trimmed Read Length Summary Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `FASTQ_DIR="fastp_results_min_50"` → Directory containing trimmed FASTQ files.  
- `OUTDIR="csv_output"` → Directory to save CSV output.  
- `OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"` → Output CSV file path.  
- `mkdir -p "$OUTDIR"` → Create output directory if missing.  
- `echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"` → Write CSV header.  
- `for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do ...` → Loop over all R1 trimmed FASTQ files.  
- `SAMPLE=$(basename "$R1" _1.trim.fastq.gz)` → Extract sample name.  
- `R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"` → Find corresponding R2 file.  
- `if [[ -f "$R2" ]]; then ... else ... fi` → Skip sample if R2 is missing.  
- `calc_stats() { ... }` → Function to calculate min, max, and average read lengths.  
- `STATS_R1=$(calc_stats "$R1")` → Compute stats for R1.  
- `STATS_R2=$(calc_stats "$R2")` → Compute stats for R2.  
- `echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"` → Append results to CSV.  
- `echo "⚠ Missing R2 for $SAMPLE, skipping."` → Print warning if R2 not found.  
- `echo "✅ Trimmed read length summary saved to $OUTPUT_CSV"` → Final confirmation message.  

</details>

If it is for **single read** we have replaced the following code instead of step 2
```bash
#!/bin/bash
set -euo pipefail

INDIR="fastp_results_min_50"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"

mkdir -p "$OUTDIR"
echo "Sample,R1_reads,R2_reads" > "$OUTPUT_CSV"
echo "📊 Counting reads in trimmed FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*_1.trim.fastq.gz "$INDIR"/*_R1.trim.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_R?1.*\.trim\.fastq\.gz//')
    R2=""
    for suffix in "_2.trim.fastq.gz" "_R2.trim.fastq.gz" "_R2_*.trim.fastq.gz"; do
        [[ -f "$INDIR/${SAMPLE}${suffix}" ]] && R2="$INDIR/${SAMPLE}${suffix}" && break
    done
    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
    R2_COUNT=$([[ -n "$R2" ]] && echo $(( $(zcat "$R2" | wc -l) / 4 )) || echo "NA")
    echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTPUT_CSV"
    echo "✅ $SAMPLE → R1: $R1_COUNT | R2: $R2_COUNT"
done

echo "🎉 All done! Read counts saved to '$OUTPUT_CSV'"
```

##### Step 3: Save and exit nano
Press Ctrl + O → Enter
Press Ctrl + X → Exit
##### Step 4: Make the script executable
```
chmod +x trimmed_fastq_read_length_summary.sh
```
##### Step 5: Run the script
```bash
./trimmed_fastq_read_length_summary.sh
```

# 4️⃣ MultiQC

<details>
<summary>📊 Aggregating QC with MultiQC</summary>

After preprocessing with `fastp`, we often generate dozens or hundreds of per-sample QC reports (`.html` and `.json`). Instead of checking each report manually, **MultiQC** aggregates all results into a single interactive HTML report.  

### Why we use MultiQC in TB analysis
- **Aggregated QC overview**: Summarizes all `fastp` results in one place.  
- **Consistency check**: Quickly detects outlier samples (e.g., unusually short reads, poor quality, or failed trimming).  
- **Scalable**: Handles hundreds or thousands of TB isolates efficiently.  
- **Standardized reporting**: Facilitates sharing results across teams or for publications.  

</details>

---

### Script: Run MultiQC
##### Step 1: **Open nano to create the script `run_multiqc.sh`
```bash
nano run_multiqc.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_50"
OUTPUT_DIR="multiqc/fastp_multiqc"

mkdir -p "$OUTPUT_DIR"

if [ ! -d "$INPUT_DIR" ]; then
    echo "Error: Input directory '$INPUT_DIR' does not exist!"
    exit 1
fi

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"

echo "MultiQC report generated in '$OUTPUT_DIR'."

```
<details>
<summary>📊 MultiQC Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors, undefined variables, or failed pipelines.  
- `INPUT_DIR="fastp_results_min_50"` → Directory containing fastp JSON/HTML outputs.  
- `OUTPUT_DIR="multiqc/fastp_multiqc"` → Directory where the aggregated MultiQC report will be saved.  
- `mkdir -p "$OUTPUT_DIR"` → Create `multiqc` and `fastp_multiqc` directories if they don’t exist.  
- `if [ ! -d "$INPUT_DIR" ]; then ... fi` → Check that the input directory exists; exit with an error if not.  
- `multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"` → Run MultiQC on all files in `INPUT_DIR` and save the combined report in `OUTPUT_DIR`.  

</details>

##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_multiqc.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate multiqc_env
./run_multiqc.sh
```
# 9️⃣ TB-Profiler
Before running TB-Profiler, we perform quality checks on raw FASTQ files using fastp and exclude samples with extremely low-quality reads. However, since TB-Profiler internally uses Trimmomatic to trim adapters and low-quality bases, it is not necessary to pre-trim the reads. Therefore, only quality-checked FASTQ files are provided as input to TB-Profiler, allowing it to handle trimming and variant calling internally.

<details>
<summary>🧬 TB-Profiler: Variant Calling, Lineage, and Drug Resistance</summary>

**TB-Profiler** is a specialized tool for *Mycobacterium tuberculosis* whole-genome sequencing (WGS) data. It performs **variant calling, lineage determination, and drug resistance prediction** in a single pipeline.

### Why we use TB-Profiler
- 🧪 **Drug Resistance Prediction** → Detects known resistance mutations for first- and second-line TB drugs.  
- 🌍 **Lineage Typing** → Classifies isolates into recognized TB lineages (e.g., Lineage 1–7).  
- 🔄 **Flexible Input** → Accepts FASTQ, BAM, or VCF files.  
- 📊 **Clear Outputs** → Produces human-readable (`.txt`) and machine-readable (`.json`) reports.  
- ⚡ **Speed & Integration** → Efficient and easily incorporated into TB genomics pipelines.  

### Why it matters
- Provides **actionable insights** for public health, including drug resistance and outbreak tracking.  
- Avoids manual cross-referencing with multiple TB resistance databases.  
- Ensures **standardized results** comparable across studies.  

</details>
 
---

## Steps

##### Step 1: Create or edit the script
```bash
nano run_tbprofiler.sh
```
##### Step 2: Paste the following code
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"

echo "📊 Starting TBProfiler runs for all samples in $FASTQ_DIR ..."

for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.fastq.gz)
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"

    if [[ ! -f "$R2" ]]; then
        echo "❌ Warning: missing paired file for $SAMPLE, skipping."
        continue
    fi

    echo "▶️ Processing sample: $SAMPLE"

    tb-profiler profile \
        -1 "$R1" \
        -2 "$R2" \
        --threads 8 \
        --prefix "$SAMPLE" \
        --txt \
        --spoligotype

    echo "✅ Finished $SAMPLE"
done

echo "📌 All samples processed!"


```
<details>
<summary>🧪 TB-Profiler Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors or undefined variables.  
- `FASTQ_DIR="raw_data"` → Folder containing paired-end FASTQ files.  
- `for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do ... done` → Loop through all R1 files.  
- `SAMPLE=$(basename "$R1" _1.fastq.gz)` → Extract sample name from filename.  
- `R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"` → Construct path for paired R2 file.  
- `if [[ ! -f "$R2" ]]; then ... fi` → Skip sample if paired R2 file is missing.  
- `tb-profiler profile -1 "$R1" -2 "$R2" --threads 8` → Run TBProfiler on paired reads using 8 threads; outputs are saved automatically in `results/tbprofiler_results/$SAMPLE/`.  
- `echo "✅ Finished $SAMPLE"` → Completion message per sample.  
- `echo "📌 All samples processed!"` → Final message after all samples are run.

</details>

If it is for **single read** we have replaced the following code instead of step 2
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"

echo "📊 Starting TBProfiler runs for all samples in $FASTQ_DIR ..."

for R1 in "$FASTQ_DIR"/*.fastq.gz; do
    SAMPLE=$(basename "$R1" | sed -E 's/_1\.fastq\.gz$//; s/\.fastq\.gz$//')
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "▶️ Processing paired sample: $SAMPLE"
        tb-profiler profile -1 "$R1" -2 "$R2" --threads 8
    else
        echo "▶️ Processing single-end sample: $SAMPLE"
        tb-profiler profile -1 "$R1" --threads 8
    fi

    echo "✅ Finished $SAMPLE"
done

echo "📌 All samples processed!"
```


##### Step 3: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano

##### Step 4: Make the script executable
```bash
chmod +x run_tbprofiler.sh
```
##### Step 5: Activate environment and run
```bash
conda activate tbprofiler_env
./run_tbprofiler.sh
```
##### Step 6: move the output from tbprofiler into new directory 
make directory tbprofiler_results
```bash
mkdir -p tbprofiler_results
```
then copy the tbprofiler output `bam`, `vcf` and `results` directories from current directory into tbprofiler_results directory 
##### Step 6: change directory to tbprofiler_results
```bash
cd ./tbprofiler_results
```
##### Step 7: Collate results
The results from numerous runs can be collated into one table using the following command:
```bash
tb-profiler collate
```
##### Step 8: Generate iTOL config for phylogeny visualization
If we want to visualize your phylogeny alongside drug-resistance types, lineages, and sublineages, run:
```bash
tb-profiler collate --itol
```

# 5️⃣ Snippy

<details>
<summary>🧬 Detailed Overview: Variant Calling with Snippy</summary>

`Snippy` is a rapid and reproducible pipeline designed for bacterial genome variant calling and consensus sequence generation. It is particularly well-suited for **Mycobacterium tuberculosis (TB) WGS analysis** due to its efficiency and accuracy.

### Key Features
- **Reference-based mapping**: Maps raw sequencing reads directly to a reference genome (commonly *M. tuberculosis* H37Rv), ensuring accurate alignment even in repetitive regions.  
- **High-confidence SNP calling**: Detects single nucleotide polymorphisms (SNPs) with stringent filtering criteria to minimize false positives.  
- **Consensus sequence generation**: Produces high-quality consensus FASTA files per sample, which can be used for phylogenetic analysis or comparative genomics.  
- **Reproducibility**: Standardized workflow ensures consistent results across multiple samples and datasets.  
- **Lightweight and scalable**: Efficient for processing large cohorts of TB isolates without requiring extensive computational resources.  
- **Standardized output files**: Outputs include VCF files for SNPs, consensus FASTA sequences, and optional alignment summaries, facilitating downstream analyses such as:
  - Phylogenetic tree reconstruction  
  - Drug resistance prediction using TBProfiler  
  - Comparative genomics across multiple TB strains  
- **Easy integration**: Works seamlessly with other bioinformatics tools and pipelines, allowing automated WGS analysis workflows.  

### Importance in TB Analysis
- Ensures accurate variant detection for **drug resistance prediction**.  
- Allows monitoring of **microevolution** within TB outbreaks.  
- Provides **reliable consensus sequences** for large-scale phylogenetic studies.  

</details>

---

##### Step 1: Create the script
```bash
nano run_snippy.sh
```
##### Step 2: Paste the following into `run_snippy.sh`
```bash
#!/bin/bash
set -euo pipefail

REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_50"
OUTDIR="snippy_results"
THREADS=8
BWA_THREADS=30
JOBS=4

mkdir -p "$OUTDIR"

run_snippy_sample() {
    SAMPLE="$1"
    R1="${FASTP_DIR}/${SAMPLE}_1.trim.fastq.gz"
    R2="${FASTP_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ ! -f "$R1" || ! -f "$R2" ]]; then
        echo "⚠ Missing R1/R2 for $SAMPLE"
        return
    fi

    echo "Running Snippy on sample: $SAMPLE"
    TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
    mkdir -p "$TMP_DIR"

    snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
           --R1 "$R1" --R2 "$R2" --force --bwaopt "-T $BWA_THREADS"

    if [[ -f "$TMP_DIR/snps.vcf" ]]; then
        mv "$TMP_DIR/snps.vcf" "${OUTDIR}/${SAMPLE}.vcf"
    fi

    for f in "$TMP_DIR"/*; do
        base=$(basename "$f")
        case "$base" in
            *.consensus.fa) mv "$f" "${OUTDIR}/${SAMPLE}.consensus.fa" ;;
            *.bam) mv "$f" "${OUTDIR}/${SAMPLE}.bam" ;;
            *.bam.bai) mv "$f" "${OUTDIR}/${SAMPLE}.bam.bai" ;;
            *.tab) mv "$f" "${OUTDIR}/${SAMPLE}.snps.tab" ;;
        esac
    done

    rm -rf "$TMP_DIR"

    [[ -f "${OUTDIR}/${SAMPLE}.vcf" ]] && echo "✅ Full VCF generated for $SAMPLE" || echo "⚠ No VCF produced for $SAMPLE"
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' \
    | parallel -j "$JOBS" run_snippy_sample {}

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' | sort > fastq_samples.txt
ls "${OUTDIR}"/*.vcf 2>/dev/null \
    | sed 's|.*/||; s/\.vcf//' | sort > snippy_samples.txt

echo "FASTQ pairs count: $(wc -l < fastq_samples.txt)"
echo "Snippy outputs count: $(wc -l < snippy_samples.txt)"

if diff fastq_samples.txt snippy_samples.txt >/dev/null; then
    echo "✅ All FASTQ pairs have corresponding Snippy results."
else
    echo "⚠ Missing samples detected:"
    diff fastq_samples.txt snippy_samples.txt || true
fi

rm -f fastq_samples.txt snippy_samples.txt

echo "🎯 All steps completed!"
echo "Snippy results are in: ${OUTDIR}/"

```
<details>
<summary>🌳 Snippy Pipeline Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors, undefined variables, or pipeline failures.  
- `REF="H37Rv.fasta"` → Reference genome.  
- `FASTP_DIR="fastp_results_min_50"` → Directory containing trimmed FASTQ files.  
- `OUTDIR="snippy_results"` → Directory to store Snippy outputs.  
- `THREADS=8` & `BWA_THREADS=30` → Threads for Snippy and BWA alignment.  
- `JOBS=4` → Number of samples to run in parallel.  
- `run_snippy_sample() { ... }` → Function for processing a single sample:  
  - Checks if paired FASTQ files exist.  
  - Runs Snippy with specified threads and BWA options.  
  - Moves key outputs (`.vcf`, `.consensus.fa`, `.bam`, `.bam.bai`, `.snps.tab`) to final directory.  
  - Deletes temporary Snippy directory.  
  - Prints confirmation if full VCF is generated.  
- `export -f run_snippy_sample` → Makes function available for GNU Parallel.  
- `ls ... | parallel -j "$JOBS" run_snippy_sample {}` → Runs multiple samples in parallel.  
- Verification section:  
  - Compares FASTQ sample list vs VCF output list.  
  - Prints warnings if any sample is missing.  
- `rm -f fastq_samples.txt snippy_samples.txt` → Cleans temporary lists.  
- `echo "🎯 All steps completed!"` → Final message indicating pipeline completion.  

</details>

If it is for **single read** we have replaced the following code instead of step 2
```bash
#!/bin/bash
set -euo pipefail

REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_50"
OUTDIR="snippy_results"
THREADS=8
BWA_THREADS=30
JOBS=4

mkdir -p "$OUTDIR"

run_snippy_sample() {
    SAMPLE="$1"
    R1="$FASTP_DIR/${SAMPLE}.trim.fastq.gz"

    if [[ ! -f "$R1" ]]; then
        echo "⚠ Missing FASTQ for $SAMPLE"
        return
    fi

    TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
    mkdir -p "$TMP_DIR"

    snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
           --se "$R1" --force --bwaopt "-T $BWA_THREADS"

    [[ -f "$TMP_DIR/snps.vcf" ]] && mv "$TMP_DIR/snps.vcf" "${OUTDIR}/${SAMPLE}.vcf"

    for f in "$TMP_DIR"/*; do
        case $(basename "$f") in
            *.consensus.fa) mv "$f" "${OUTDIR}/${SAMPLE}.consensus.fa" ;;
            *.bam) mv "$f" "${OUTDIR}/${SAMPLE}.bam" ;;
            *.bam.bai) mv "$f" "${OUTDIR}/${SAMPLE}.bam.bai" ;;
            *.tab) mv "$f" "${OUTDIR}/${SAMPLE}.snps.tab" ;;
        esac
    done

    rm -rf "$TMP_DIR"
    [[ -f "${OUTDIR}/${SAMPLE}.vcf" ]] && echo "✅ Full VCF generated for $SAMPLE" || echo "⚠ No VCF produced for $SAMPLE"
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

ls "$FASTP_DIR"/*.trim.fastq.gz \
    | sed 's|.*/||; s/\.trim\.fastq\.gz//' \
    | sort -u \
    | parallel -j "$JOBS" run_snippy_sample {}

```
##### Step 3: Save and exit nano
Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)

##### Step 4: Make the script executable
```bash
chmod +x run_snippy.sh
```
##### Step 5: Activate environment and run
```bash
conda activate snippy_env
./run_snippy.sh
```


Check Snippy VCFs Before Variant Filtering
- `tb_variant_filter` relies on a correctly formatted VCF with the `#CHROM` line to parse variants.  
- VCFs missing this line will **fail** during filtering, causing errors like:  
  `Missing line starting with "#CHROM"`.  
- Running this check before variant filtering saves time and ensures downstream analysis runs smoothly.

This script ensures that all VCF files generated by **Snippy** contain the required `#CHROM` header line.
```bash
#!/bin/bash
OUTDIR="snippy_results"

echo "Checking Snippy VCFs in $OUTDIR ..."

for vcf in "$OUTDIR"/*.vcf; do
    SAMPLE=$(basename "$vcf")
    if grep -q "^#CHROM" "$vcf"; then
        echo "✅ $SAMPLE contains #CHROM line"
    else
        echo "⚠ $SAMPLE is missing #CHROM line"
    fi
done
```

# 6️⃣ Qualimap BAM QC
<details>
<summary>📈 BAM Quality Assessment with Qualimap</summary>

After generating BAM files with Snippy, it is crucial to evaluate their quality before downstream analyses. **Qualimap** is widely used for this purpose.

### Why we use Qualimap in TB genomics
- **Mapping quality evaluation**: Assesses alignment accuracy and read placement.  
- **Coverage distribution and GC content**: Detects uneven coverage or biases across the genome.  
- **Visual reports**: Generates **HTML** and **PDF** reports with clear QC metrics.  
- **Identify problematic samples**: Flags samples with low coverage, poor alignment, or uneven depth, which may affect variant calling.  
- **Integration with MultiQC**: QC results can be aggregated for large TB cohorts for batch reporting.  

</details>

---

##### Step 1: Create the script
```bash
nano run_qualimap.sh
```
#####  Step 2: Paste the following into `run_qualimap.sh`
```bash
#!/bin/bash
set -euo pipefail

SNIPPY_DIR="snippy_results"
QUALIMAP_OUT="qualimap_reports"
mkdir -p "$QUALIMAP_OUT"

for bam in "$SNIPPY_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Running Qualimap BAM QC for sample: $sample"

    outdir="${QUALIMAP_OUT}/${sample}"
    mkdir -p "$outdir"

    qualimap bamqc \
        -bam "$bam" \
        -outdir "$outdir" \
        -outformat pdf:html \
        --java-mem-size=4G
done
```
<details>
<summary>📊 Qualimap BAM QC Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors or undefined variables.  
- `SNIPPY_DIR="all_bams"` → Directory with Snippy BAM files.  
- `QUALIMAP_OUT="qualimap_reports"` → Output directory for QC reports.  
- `mkdir -p "$QUALIMAP_OUT"` → Ensure output directory exists.  
- `for bam in "$SNIPPY_DIR"/*.bam; do ... done` → Loop over all BAM files.  
- `sample=$(basename "$bam" .bam)` → Extract sample name.  
- `outdir="${QUALIMAP_OUT}/${sample}"` → Unique folder per sample.  
- `qualimap bamqc -bam "$bam" -outdir "$outdir" -outformat pdf:html --java-mem-size=4G` → Run QC, generate PDF & HTML, allocate 4 GB memory.  
- `echo "Running Qualimap BAM QC for sample: $sample"` → Print progress.

</details>

##### Step 3: Save and exit nano

Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_qualimap.sh
```
##### Step 5: Activate environment and install GNU Parallel into your `qualimap_env`:
```bash
conda activate qualimap_env
```
##### Step 6: run:
```bash
./run_qualimap.sh
```

# 7️⃣ MultiQC after Qualimap

<details>
<summary>📊 Aggregating BAM QC with MultiQC</summary>

After running **Qualimap BAM QC**, each sample produces individual HTML and PDF reports. For large TB cohorts, opening these reports one by one is inefficient. **MultiQC** solves this by aggregating all results.

### Why we use MultiQC after Qualimap
- **Scans all Qualimap output folders**: Automatically detects per-sample QC reports.  
- **Aggregates QC metrics**: Combines mapping quality, depth, and coverage statistics across all samples.  
- **Quick overview of problematic samples**: Highlights low coverage, uneven depth, or poor alignment.  
- **Standardized reporting**: Ensures results are consistent, shareable, and suitable for team collaboration or publications.  

</details>

---

### Run MultiQC on Qualimap outputs

##### Step 1: **Open nano to create the script `run_multiqc_qualimap.sh`
```bash
nano run_multiqc_qualimap.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="qualimap_reports"
OUTPUT_DIR="multiqc/qualimap_multiqc"

mkdir -p "$OUTPUT_DIR"

if [ ! -d "$INPUT_DIR" ]; then
    echo "Error: Input directory '$INPUT_DIR' does not exist!"
    exit 1
fi

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"

echo "MultiQC report generated in '$OUTPUT_DIR'."

```
<details>
<summary>📊 MultiQC for Qualimap Reports – Script Explanation</summary>

- `#!/bin/bash` → Run the script with Bash.  
- `set -euo pipefail` → Exit on errors, undefined variables, or failed pipelines.  
- `INPUT_DIR="qualimap_reports"` → Directory containing Qualimap BAM QC reports.  
- `OUTPUT_DIR="multiqc/qualimap_multiqc"` → Directory where the aggregated MultiQC report will be saved.  
- `mkdir -p "$OUTPUT_DIR"` → Create `multiqc` and `qualimap_multiqc` directories if they don’t exist.  
- `if [ ! -d "$INPUT_DIR" ]; then ... fi` → Check that the input directory exists; exit with an error if not.  
- `multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"` → Run MultiQC on all files in `INPUT_DIR` and save the combined report in `OUTPUT_DIR`.  
- `echo "MultiQC report generated in '$OUTPUT_DIR'."` → Confirmation message after successful completion.

</details>


##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_multiqc_qualimap.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate multiqc_env
./run_multiqc_qualimap.sh
```

# 8️⃣ TB Variant Filter
<details>
<summary>🧬 TB-Specific Variant Filtering with <code>tb_variant_filter</code></summary>

The **tb_variant_filter** tool is designed specifically for **Mycobacterium tuberculosis (M. tb)** sequencing data. Unlike generic variant filters, it takes into account TB-specific genomic features and problematic regions in the **H37Rv reference genome**, ensuring that only **high-confidence variants** are retained.

### 🛠 Key Filtering Options

**tb_variant_filter** offers several ways to refine your VCF files:

1. **Region-based filtering**  
   Mask out variants in defined genomic regions. Region lists include:  
   - **RLC (Refined Low Confidence) regions** – Marin et al 2022 (default)  
   - **RLC + Low Mappability regions** – Marin et al 2022  
   - **PE/PPE genes** – Fishbein et al 2015  
   - **Antibiotic resistance genes** – TBProfiler and MTBseq lists  
   - **Repetitive loci** – UVP list  

   > ⚠️ **Default:** Use RLC regions. These are parts of the H37Rv genome where Illumina reads map poorly. For reads shorter than 100 bp or single-ended reads, consider using the **RLC + Low Mappability filter**. PE/PPE and UVP filters are mainly for backward compatibility, but they may exclude too much of the genome.

2. **Window around indels**  
   Masks variants within a set distance (default 5 bases) of insertions or deletions.

3. **Alternate allele percentage**  
   Removes variants with fewer than the minimum percentage (default 90%) of alternative alleles.

4. **Depth of aligned reads**  
   Filters variants based on sequencing depth to remove low-confidence calls.

5. **SNV-only filtering**  
   Optionally discard all variants that are not single nucleotide variants.

### ⚡ How Filters Work Together

When multiple filters are applied, they **stack**: a variant will be masked if **any filter** flags it. This ensures a conservative, high-confidence set of variants for downstream analyses.

</details>

  
---
## Steps

##### Step 1: Activate environment
```bash
conda activate tb_variant_filter_env
```
##### Step 2:  We need this directory to store all BED files containing genomic regions to be masked during variant filtering
```bash
mkdir -p /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions
```
#####  Step 3: Generate each BED file in that folder
<details>
<summary>🧬 BED Files for TB Variant Filtering</summary>

We need **Refined Low Confidence (RLC) regions** by default, but we may also need other regions. You can download and store the BED files in your directory to mask genomic regions during variant filtering, such as:

- **Refined Low Confidence (RLC) regions**  
- **Low mappability regions**  
- **PE/PPE genes**  
- **Antibiotic resistance gene lists** (TBProfiler, MTBseq)  
- **Repetitive loci** (UVP)

</details>

```bash
tb_region_list_to_bed --chromosome_name H37Rv farhat_rlc /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/RLC_Marin2022.bed

tb_region_list_to_bed --chromosome_name H37Rv farhat_rlc_lowmap /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/RLC_and_LowMappability_Marin2022.bed

tb_region_list_to_bed --chromosome_name H37Rv pe_ppe /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/PE_PPE_Fishbein2015.bed

tb_region_list_to_bed --chromosome_name H37Rv tbprofiler /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/TBProfiler_resistance_genes.bed

tb_region_list_to_bed --chromosome_name H37Rv mtbseq /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/MTBseq_resistance_genes.bed

tb_region_list_to_bed --chromosome_name H37Rv uvp /media/betselot_z/DATADRIVE0/betselot/TB/TB_project/PRJNA1247743/masking_regions/UVP_repetitive_loci.bed

```
<details>
<summary>🧬 tb_region_list_to_bed Commands Explanation</summary>

- `tb_region_list_to_bed` → Command from `tb_variant_filter` to export predefined genomic regions as BED files.  
- `--chromosome_name H37Rv` → Specifies the reference genome (M. tuberculosis H37Rv).  
- `{farhat_rlc, farhat_rlc_lowmap, pe_ppe, tbprofiler, mtbseq, uvp}` → Region list names available in `tb_variant_filter`:  
  - `farhat_rlc` → Refined Low Confidence regions (Marin et al., 2022).  
  - `farhat_rlc_lowmap` → RLC + Low Mappability regions (Marin et al., 2022).  
  - `pe_ppe` → PE/PPE gene regions (Fishbein et al., 2015).  
  - `tbprofiler` → TBProfiler antibiotic resistance genes.  
  - `mtbseq` → MTBseq antibiotic resistance genes.  
  - `uvp` → UVP repetitive loci in the genome.  
- Last argument → Output BED file path where the exported regions will be saved.  

</details>

##### Step 4: Create or edit the script  
```bash
nano run_tb_variant_filter.sh
```
#####  Step 5: Paste the following into `run_tb_variant_filter.sh`
```bash
#!/bin/bash
set -euo pipefail

CURDIR=$(pwd)
SNIPPY_DIR="$CURDIR/snippy_results"
OUTDIR="$CURDIR/tb_variant_filter_results"
mkdir -p "$OUTDIR"

REGION_FILTER="farhat_rlc"

for vcf in "$SNIPPY_DIR"/*.vcf; do
    sample=$(basename "$vcf")
    echo "Filtering $sample ..."
    
    tb_variant_filter --region_filter "$REGION_FILTER" \
        "$vcf" \
        "$OUTDIR/${sample%.vcf}.filtered.vcf"

        if [ ! -s "$OUTDIR/${sample%.vcf}.filtered.vcf" ]; then
        echo "⚠ $sample filtered VCF is empty."
    fi
done

echo "✅ All VCFs filtered using $REGION_FILTER and saved in $OUTDIR"

```
<details>
<summary>🧬 TB Variant Filter Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors or undefined variables.  
- `CURDIR=$(pwd)` → Save current working directory.  
- `SNIPPY_DIR="$CURDIR/snippy_results"` → Folder containing Snippy VCFs.  
- `OUTDIR="$CURDIR/tb_variant_filter_results"` → Output folder for filtered VCFs.  
- `mkdir -p "$OUTDIR"` → Ensure output directory exists.  
- `REGION_FILTER="farhat_rlc"` → Predefined region filter for TB variant filtering.  
- `for vcf in "$SNIPPY_DIR"/*.vcf; do ... done` → Loop through all Snippy VCFs.  
- `sample=$(basename "$vcf")` → Extract filename for naming filtered outputs.  
- `tb_variant_filter --region_filter "$REGION_FILTER" "$vcf" "$OUTDIR/${sample%.vcf}.filtered.vcf"` → Filter each VCF using the specified region filter and save result.  
- `echo "✅ All VCFs filtered using $REGION_FILTER and saved in $OUTDIR"` → Prints confirmation when all VCFs are filtered.

</details>



##### Step 6: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano

##### Step 7: Make the script executable
```bash
chmod +x run_tb_variant_filter.sh
```
##### Step 8: run
```bash
./run_tb_variant_filter.sh
```
<details>
<summary>📊 VCF QC Script Step-by-Step Guide</summary>

### Purpose

This script is designed to compare unfiltered Snippy VCFs with filtered VCFs produced by `tb_variant_filter`. The main goals are:

- Count the total number of variants in each VCF.
- Identify variants that meet a custom “PASS” criterion, based on user-defined quality thresholds.
- Calculate the PASS retention ratio, i.e., the fraction of high-quality variants retained after filtering.

This QC step ensures that your filtered VCFs retain high-confidence variants and helps detect potential over-filtering or loss of important variants before downstream analysis.

---

### Key Parameters

- `MIN_DP=20` → Minimum read depth required for a variant to be considered “high quality.”  
  - Read depth (DP in the VCF INFO field) indicates the number of reads supporting a variant.  
  - Variants with fewer than 20 supporting reads are considered low-confidence and are excluded from the PASS count.

- `MIN_QUAL=30` → Minimum variant quality score (QUAL in the VCF) to consider a variant as PASS.  
  - The QUAL score represents the confidence that the variant is real.  
  - Variants with QUAL < 30 are considered low-confidence and are excluded from the PASS count.

> These thresholds can be adjusted depending on your experimental design and sequencing quality.

---

### What the Script Does

For each sample:

**Unfiltered VCF analysis**

- Counts all variants (`Unfiltered_total`).
- Counts variants meeting `DP ≥ 20` and `QUAL ≥ 30` (`Unfiltered_PASS`).

**Filtered VCF analysis**

- Counts all variants in the filtered VCF (`Filtered_total`).
- Counts variants meeting `DP ≥ 20` and `QUAL ≥ 30` (`Filtered_PASS`).

**Calculate PASS retention ratio**

- `PASS_retention_ratio = Filtered_PASS / Unfiltered_PASS`  
- Measures how many high-quality variants remain after filtering.

---

### Output Table Columns

| Column               | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| Sample               | Name of the sample (derived from the VCF filename)                           |
| Unfiltered_total     | Total variants in the original Snippy VCF                                    |
| Unfiltered_PASS      | Variants meeting `DP ≥ 20` and `QUAL ≥ 30` in unfiltered VCF                |
| Filtered_total       | Total variants in tb_variant_filter output                                   |
| Filtered_PASS        | Variants meeting `DP ≥ 20` and `QUAL ≥ 30` in filtered VCF                  |
| PASS_retention_ratio | Fraction of high-quality variants retained after filtering                  |

</details>



##### Step 1: Open a new file in nano
```bash
nano compare_vcf_qc.sh
```
##### Step 2: Paste the script
```bash
#!/bin/bash
set -euo pipefail

SNIPPY_DIR="snippy_results"
FILTERED_DIR="tb_variant_filter_results"
OUTDIR="csv_output"
OUTFILE="$OUTDIR/variant_filter_summary.csv"

MIN_DP=20
MIN_QUAL=30

mkdir -p "$OUTDIR"
echo "Sample,Unfiltered_total,Unfiltered_PASS,Filtered_total,Filtered_PASS,PASS_retention_ratio" | tee "$OUTFILE"

count_pass_variants() {
    local vcf=$1
    awk -v min_dp=$MIN_DP -v min_qual=$MIN_QUAL '
        BEGIN{count=0}
        /^#/ {next}
        {
            qual=$6
            dp=0
            split($8, info_arr, ";")
            for(i in info_arr){
                if(info_arr[i] ~ /^DP=/){
                    split(info_arr[i], kv, "=")
                    dp=kv[2]
                }
            }
            if(qual >= min_qual && dp >= min_dp) count++
        }
        END{print count}
    ' "$vcf"
}

for vcf in "$SNIPPY_DIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)
    unfiltered_total=$(grep -v "^#" "$vcf" | wc -l)
    unfiltered_pass=$(count_pass_variants "$vcf")
    if [[ ! -f "$FILTERED_DIR/$sample.filtered.vcf" ]]; then
        echo "$sample,$unfiltered_total,$unfiltered_pass,NA,NA,NA" | tee -a "$OUTFILE"
        continue
    fi
    filtered_total=$(grep -v "^#" "$FILTERED_DIR/$sample.filtered.vcf" | wc -l)
    filtered_pass=$(count_pass_variants "$FILTERED_DIR/$sample.filtered.vcf")
    if [[ $unfiltered_pass -eq 0 ]]; then
        ratio="NA"
    else
        ratio=$(awk -v pass=$filtered_pass -v total=$unfiltered_pass 'BEGIN{printf "%.2f", pass/total}')
    fi
    echo "$sample,$unfiltered_total,$unfiltered_pass,$filtered_total,$filtered_pass,$ratio" | tee -a "$OUTFILE"
done

```

##### Step  3: Save and exit nano

  Press Ctrl+O → Enter to save.
   Press Ctrl+X → Exit nano.
##### Step  4:Make the script executable
```bash
chmod +x compare_vcf_qc.sh
```
##### Step 5: Run the script
```bash
./compare_vcf_qc.sh
```

# 1️⃣0️⃣ BCFTools Consensus Generation

<details>
<summary>🧬 Generate Sample-Specific Consensus Sequences</summary>

After filtering VCFs with **tb_variant_filter**, we generate **consensus FASTA sequences** for each sample. These sequences represent the **full genome of each isolate**, including only **high-confidence variants** relative to the reference genome (*H37Rv*).

### Why consensus sequences matter
- Provide a **single representative genome** per sample for downstream analyses.  
- Used in **phylogenetic reconstruction**, outbreak investigation, and comparative genomics.  
- Incorporate **only reliable SNPs and indels**, minimizing noise from sequencing errors.  
- Standardize genome representations across multiple isolates, ensuring comparability.  

</details>
---

##### Step 1: Compress and index each filtered VCF
```bash
for vcf in tb_variant_filter_results/*.vcf; do
   
    if [ ! -s "$vcf" ]; then
        echo "Skipping empty VCF: $vcf"
        continue
    fi

    gz_file="${vcf}.gz"
    echo "Compressing $vcf → $gz_file"
    bgzip -c "$vcf" > "$gz_file"

    echo "Indexing $gz_file"
    bcftools index "$gz_file"
done
```
##### Step 2: Create a script to generate consensus sequences
```bash
nano generate_consensus_all.sh
```
##### Step 3: Paste this code
```bash
#!/bin/bash
set -euo pipefail

CURDIR=$(pwd)
VCFDIR="$CURDIR/tb_variant_filter_results"
OUTDIR="$CURDIR/consensus_sequences"
mkdir -p "$OUTDIR"

for vcf in "$VCFDIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)

    if [ $(grep -v '^#' "$vcf" | wc -l) -eq 0 ]; then
        echo "$sample VCF is empty. Skipping."
        continue
    fi

    gz_file="${vcf}.gz"
    bgzip -c "$vcf" > "$gz_file"
    bcftools index "$gz_file"
    bcftools consensus -f "$CURDIR/H37Rv.fasta" "$gz_file" | sed "1s/.*/>$sample/" > "$OUTDIR/${sample}.consensus.fasta"
done

```
<details>
<summary>🧬 VCF-to-Consensus Script Explanation</summary>

- `#!/bin/bash` → Run script with Bash.  
- `set -euo pipefail` → Exit on errors or undefined variables.  
- `CURDIR=$(pwd)` → Save current working directory.  
- `VCFDIR="$CURDIR/tb_variant_filter_results"` → Folder with filtered VCFs.  
- `OUTDIR="$CURDIR/consensus_sequences"` → Folder for consensus FASTA sequences.  
- `mkdir -p "$OUTDIR"` → Ensure output directory exists.  
- `for vcf in "$VCFDIR"/*.vcf; do ... done` → Loop through all filtered VCF files.  
- `sample=$(basename "$vcf" .vcf)` → Extract sample name.  
- `bgzip -c "$vcf" > "$vcf.gz"` → Compress VCF with bgzip.  
- `bcftools index "$vcf.gz"` → Index compressed VCF.  
- `bcftools consensus -f "$CURDIR/H37Rv.fasta" "$vcf.gz" | sed "1s/.*/>$sample/" > "$OUTDIR/${sample}.consensus.fasta"` → Generate consensus FASTA and replace header with sample name.  
- `echo "✅ $sample consensus generated"` → Confirmation per sample.  
- `echo "🎉 All consensus sequences saved in $OUTDIR"` → Final message.  

**⚠ Note:** Activate the `tb_consensus_env` before running this script.

</details>


##### Step 4: Save and exit nano
Press Ctrl + O → Enter (to write the file)
Press Ctrl + X → Exit nano

##### Step 5: Make the script executable
```bash
chmod +x generate_consensus_all.sh
```
##### Step 6: Run the script
```bash
conda activate tb_consensus_env
./generate_consensus_all.sh
```

# 1️⃣1️⃣ Check Consensus FASTA Lengths

After generating consensus sequences, it's important to **verify the genome length** for each sample.  
This ensures no sequences are truncated or incomplete due to missing coverage or filtering.

---

### 📏 Calculating Consensus Genome Lengths

We can check the length of each consensus FASTA sequence to ensure completeness and consistency.  
This helps verify that consensus sequences cover the full *M. tuberculosis* genome (~4.4 Mbp) and can reveal missing regions.

### Rename the FASTA files

```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"

for f in "$FASTA_DIR"/*.filtered.consensus.fasta; do
    mv "$f" "${f/.filtered.consensus/}"
done

echo "✅ All consensus FASTA files have been renamed to .fasta."

```

<details>
<summary>📝 Rename Consensus FASTA Files</summary>

- `FASTA_DIR="consensus_sequences"` → Directory containing consensus FASTA files.  
- `for f in "$FASTA_DIR"/*.filtered.consensus.fasta; do ... done` → Loop over all FASTA files ending with `.filtered.consensus.fasta`.  
- `mv "$f" "${f/.filtered.consensus/}"` → Rename each file by removing `.filtered.consensus` from filename.  
- `echo "✅ All consensus FASTA files have been renamed to .fasta."` → Confirmation message after renaming.

</details>

###  Update headers inside the FASTA files
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    awk -v s="$sample" '/^>/{print ">" s; next} {print}' "$f" > "${f}.tmp" && mv "${f}.tmp" "$f"
    echo "✅ Updated header in: $(basename "$f")"
done
echo "🎉 All FASTA headers have been successfully updated."
```
<details>
<summary>📝 Update FASTA Headers with Sample Names</summary>

- `FASTA_DIR="consensus_sequences"` → Directory containing FASTA files.  
- `for f in "$FASTA_DIR"/*.fasta; do ... done` → Loop through all FASTA files.  
- `sample=$(basename "$f" .fasta)` → Extract sample name from filename.  
- `awk -v s="$sample" '/^>/{print ">" s; next} {print}' "$f" > "${f}.tmp" && mv "${f}.tmp" "$f"` → Replace FASTA header with `>sample`, keep sequence lines unchanged.  
- `echo "✅ Updated header in: $(basename "$f")"` → Logs each updated file.  
- `echo "🎉 All FASTA headers have been successfully updated."` → Completion message after all files processed.

</details>


### Using `grep` and `wc`
We remove the FASTA headers (`>` lines) and count the remaining nucleotides to get the total genome length:

```bash
for f in consensus_sequences/*.fasta; do
    sample=$(basename "$f")
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample : $length bp"
done
```

to save the result in csv file 
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"  
OUTPUT_CSV="${OUTDIR}/consensus_lengths.csv"

echo "Sample,Length_bp" > "$OUTPUT_CSV"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample,$length" >> "$OUTPUT_CSV"
done

echo "✅ Consensus genome lengths saved to $OUTPUT_CSV"

```
<details>
<summary>📖 Explanation of calculating consensus genome lengths and saving the result in CSV</summary>

- `FASTA_DIR="consensus_sequences"` → sets the directory containing consensus FASTA files.  
- `OUTDIR="csv_output"` → defines the directory where the CSV will be saved.  
- `mkdir -p "$OUTDIR"` → ensures the output directory exists before writing the file.  
- `OUTPUT_CSV="${OUTDIR}/consensus_lengths.csv"` → defines the CSV file path inside `OUTDIR`.  
- `echo "Sample,Length_bp" > "$OUTPUT_CSV"` → creates the CSV file and writes the header line.  
- `for f in "$FASTA_DIR"/*.fasta; do ... done` → loops over all FASTA files in the directory.  
- `sample=$(basename "$f" .fasta)` → extracts the sample name from the FASTA filename.  
- `length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)` → removes header lines (`>`), concatenates sequences into one line, and counts nucleotides.  
- `echo "$sample,$length" >> "$OUTPUT_CSV"` → appends the sample name and its sequence length to the CSV file.  
- `echo "✅ Consensus genome lengths saved to $OUTPUT_CSV"` → prints a completion message when finished.  

</details>



# 1️⃣2️⃣ Multiple Sequence Alignment with MAFFT

MAFFT v7.490 requires **a single FASTA file** as input.  
It **cannot take multiple FASTA files** on the command line directly, otherwise it interprets filenames as options.

---

Before performing phylogenetic analysis, it is crucial to include an **outgroup sequence**. The outgroup serves as a reference point to root the phylogenetic tree, providing directionality for evolutionary relationships.  

For our analysis, we searched the NCBI database for sequences belonging to **Lineage 8** of *Mycobacterium tuberculosis*. We selected the sequence `SRR10828835`, which originates from Rwanda, as our outgroup. This choice was intentional because Lineage 8 is **not present in Ethiopia**, ensuring it is sufficiently divergent from our study isolates and providing a robust root for the tree.  

The selected outgroup sequence was downloaded and saved in the `consensus_sequences/` directory alongside our aligned TB consensus sequences.

---
Perform multiple sequence alignment on the merged file:
Use a faster algorithm than --auto

--auto lets MAFFT decide, but for hundreds/thousands of sequences, it often picks a slower iterative refinement.
You can explicitly pick faster modes:

Since Mycobacterium tuberculosis genomes are highly conserved and we care more about speed than very small accuracy gains, we can safely drop the expensive iterative refinement steps in MAFFT.
fast and TB-suitable command



##### Step 1: Merge all consensus FASTAs
We combine all individual consensus sequences into one multi-FASTA file:
- **Aligning sequences**  
  - All sequences, including the outgroup, must be aligned together.  
  - This ensures that **homologous positions are matched across all sequences**, which is essential for correct tree inference.  
  - Example workflow:  
    1. Combine all consensus sequences and the outgroup FASTA into a single folder.  
    2. Run MAFFT on the combined sequences to produce a single aligned file (e.g., `aligned_consensus.fasta`).  
  - The outgroup sequence must already be included in the alignment.  
  - When running IQ-TREE, we need to specify the outgroup using the `-o` option with the **FASTA header name** of the outgroup:  
  - IQ-TREE will then **root the phylogenetic tree using this outgroup**.
    
```bash
cat consensus_sequences/*.fasta > consensus_sequences/all_consensus.fasta
```

##### Step 2: Open a new script file in nano
```bash
nano run_mafft.sh
```

##### Step 3: Paste the script into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="consensus_sequences"
OUTPUT_DIR="mafft_results"
mkdir -p "$OUTPUT_DIR"

THREADS=16
COMBINED_OUT="$OUTPUT_DIR/aligned_consensus.fasta"

[[ -f "$COMBINED_OUT" ]] && rm "$COMBINED_OUT"

for FILE in "$INPUT_DIR"/*.fasta "$INPUT_DIR"/*.fa; do
    [[ -f "$FILE" ]] || continue
    BASENAME=$(basename "$FILE" .fasta)
    BASENAME=$(basename "$BASENAME" .fa)
    TEMP_OUT="$OUTPUT_DIR/${BASENAME}_aligned_temp.fasta"
    mafft --thread "$THREADS" --auto --parttree "$FILE" > "$TEMP_OUT"
    cat "$TEMP_OUT" >> "$COMBINED_OUT"
    rm "$TEMP_OUT"
done

FILE="$COMBINED_OUT"

if [[ -f "$FILE" ]]; then
    SEQ_COUNT=$(grep -c ">" "$FILE")
    TOTAL_LENGTH=$(grep -v ">" "$FILE" | tr -d '\n' | wc -c)
    MIN_LENGTH=$(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"; min=0} NR>1{len=length($0); if(min==0 || len<min) min=len} END{print min}')
    MAX_LENGTH=$(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"; max=0} NR>1{len=length($0); if(len>max) max=len} END{print max}')
    AVG_LENGTH=$(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"; sum=0; count=0} NR>1{sum+=length($0); count++} END{if(count>0) print int(sum/count); else print 0}')

    if [[ $SEQ_COUNT -gt 0 ]]; then
        echo "✅ $FILE"
        echo "   Sequences: $SEQ_COUNT"
        echo "   Total length (bp): $TOTAL_LENGTH"
        echo "   Min length: $MIN_LENGTH"
        echo "   Max length: $MAX_LENGTH"
        echo "   Avg length: $AVG_LENGTH"
    else
        echo "⚠️ $FILE exists but contains no sequences!"
    fi
else
    echo "❌ $FILE not found!"
fi

echo "✅ Alignment and checks completed."

```
<details>
<summary>📖 Explanation of MAFFT alignment and combined consensus checks</summary>

- `INPUT_DIR="consensus_sequences"` → sets the directory containing input FASTA files.  
- `OUTPUT_DIR="mafft_results"` → sets the directory where aligned files will be saved.  
- `mkdir -p "$OUTPUT_DIR"` → ensures the output directory exists.  
- `THREADS=16` → sets the number of CPU threads MAFFT will use.  
- `COMBINED_OUT="$OUTPUT_DIR/aligned_consensus.fasta"` → defines the combined alignment output file.  
- `[[ -f "$COMBINED_OUT" ]] && rm "$COMBINED_OUT"` → removes any previous combined output file.  
- `for FILE in "$INPUT_DIR"/*.fasta "$INPUT_DIR"/*.fa; do ... done` → loops over all FASTA files in the input directory.  
- `[[ -f "$FILE" ]] || continue` → skips iteration if the file does not exist.  
- `BASENAME=$(basename "$FILE" .fasta)` → extracts the filename without `.fasta`.  
- `BASENAME=$(basename "$BASENAME" .fa)` → further removes `.fa` extension if present.  
- `TEMP_OUT="$OUTPUT_DIR/${BASENAME}_aligned_temp.fasta"` → temporary output file for individual alignment.  
- `mafft --thread "$THREADS" --auto --parttree "$FILE" > "$TEMP_OUT"` → runs MAFFT on the input file.  
- `cat "$TEMP_OUT" >> "$COMBINED_OUT"` → appends the aligned sequences to the combined output file.  
- `rm "$TEMP_OUT"` → removes the temporary file.  
- `FILE="$COMBINED_OUT"` → sets the variable for the combined file to check.  
- `if [[ -f "$FILE" ]]; then ... fi` → checks if the combined alignment file exists.  
- `SEQ_COUNT=$(grep -c ">" "$FILE")` → counts the number of sequences.  
- `TOTAL_LENGTH=$(grep -v ">" "$FILE" | tr -d '\n' | wc -c)` → counts total nucleotides.  
- `MIN_LENGTH` and `MAX_LENGTH` → compute the shortest and longest sequence lengths.  
- `AVG_LENGTH` → computes the average sequence length.  
- `if [[ $SEQ_COUNT -gt 0 ]]; then ... else ... fi` → prints success message with statistics if sequences exist, or a warning if none.  
- `echo "✅ Alignment and checks completed."` → prints completion message.  

</details>

##### Step 4: Save and exit nano

 Press Ctrl + O → hit Enter to save.
Press Ctrl + X → to exit nano.
##### Step 5: Make the script executable
```bash
chmod +x run_mafft.sh
```
##### Step 6: Run the script
```bash
conda activate mafft_env
./run_mafft.sh
```

##### Step 7: Verify the alignment

A. Quickly inspect the top of the aligned FASTA:
```bash
head consensus_sequences/aligned_consensus.fasta
```
B. Checking MAFFT alignment output to 
- Ensure the alignment file **exists** and contains all intended sequences.  
- Verify the **number of sequences** matches expectations.  
- Check **sequence lengths** (total, min, max, average) to confirm proper alignment.  
- Detect if sequences have **varying lengths**, which may indicate misalignment or excessive gaps.  
- A **good alignment** is essential for accurate phylogenetic tree inference with IQ-TREE or other downstream analyses.


```bash
#!/bin/bash
set -euo pipefail

FILE="mafft_results/aligned_consensus.fasta"

if [[ ! -f "$FILE" ]]; then
    echo "❌ $FILE not found!"
    exit 1
fi

SEQ_COUNT=$(grep -c ">" "$FILE")
LENGTHS=($(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"} NR>1{print length($0)}'))
TOTAL_LENGTH=$(IFS=+; echo "$((${LENGTHS[*]}))")
MIN_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | head -n1)
MAX_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | tail -n1)
AVG_LENGTH=$(( TOTAL_LENGTH / SEQ_COUNT ))

echo "✅ $FILE exists"
echo "Sequences: $SEQ_COUNT"
echo "Total length (bp): $TOTAL_LENGTH"
echo "Min length: $MIN_LENGTH"
echo "Max length: $MAX_LENGTH"
echo "Avg length: $AVG_LENGTH"

if [[ $MIN_LENGTH -eq $MAX_LENGTH ]]; then
    echo "✅ All sequences have equal length, alignment looks good."
else
    echo "⚠️ Sequence lengths vary; check for gaps or misalignment."
fi

```
<details>
<summary>📖 Explanation of MAFFT output check script</summary>

- `FILE="mafft_results/aligned_consensus.fasta"` → sets the path to the MAFFT alignment output file.  

- `if [[ ! -f "$FILE" ]]; then ... fi` → checks if the file exists; exits with a message if it does not.  

- `SEQ_COUNT=$(grep -c ">" "$FILE")` → counts the number of sequences in the alignment.  
  - In FASTA format, each sequence starts with a `>` header.  

- `LENGTHS=($(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"} NR>1{print length($0)}'))` → creates an array of sequence lengths.  
  - Removes header lines and calculates the length of each sequence.  

- `TOTAL_LENGTH=$(IFS=+; echo "$((${LENGTHS[*]}))")` → sums all sequence lengths to get the total number of base pairs.  

- `MIN_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | head -n1)` → finds the shortest sequence length.  

- `MAX_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | tail -n1)` → finds the longest sequence length.  

- `AVG_LENGTH=$(( TOTAL_LENGTH / SEQ_COUNT ))` → calculates the average sequence length.  

- `echo ...` → prints summary statistics for the alignment: number of sequences, total, min, max, and average lengths.  

- `if [[ $MIN_LENGTH -eq $MAX_LENGTH ]]; then ... else ... fi` → checks if all sequences are the same length.  
  - If yes → alignment looks good.  
  - If no → prints a warning that sequence lengths vary, indicating possible gaps or misalignment.  

</details>




# 1️⃣3️⃣ IQtree
<details>
<summary>📖 Overview of IQ-TREE and important concepts</summary>

- **What is IQ-TREE?**  
  - IQ-TREE is a **phylogenetic tree inference software** used to construct evolutionary trees from aligned sequences.  
  - It implements **maximum likelihood (ML) methods**, which estimate the tree that best explains the observed sequences under a substitution model.  

- **Input**  
  - Requires an **aligned sequence file** (FASTA, PHYLIP, or NEXUS).  
  - Sequences must be aligned so that homologous positions are in the same columns.  

- **Substitution models (-m)**  
  - Models describe how nucleotides (or amino acids) change over time.  
  - Common nucleotide models:  
    - **GTR**: General Time Reversible model, allows different rates for each nucleotide substitution.  
    - **+G**: Gamma distribution to account for rate variation across sites.  
    - **+I**: Proportion of invariant sites.  
  - IQ-TREE can **automatically select the best model** using `-m MFP` (ModelFinder Plus).  

- **Bootstrap support (-bb)**  
  - Ultrafast bootstrap replicates measure **confidence in the tree branches**.  
  - Example: `-bb 1000` runs 1000 replicates.  

- **Outgroup rooting (-o)**  
  - Specify the outgroup sequence (by its FASTA header) to **root the tree** correctly.  
  - The outgroup must be included in the alignment.  

- **Multithreading (-nt)**  
  - IQ-TREE can use multiple CPU threads for faster computation, e.g., `-nt 4`.  

- **Output files**  
  - `.treefile` → final tree in Newick format.  
  - `.log` → details of the run.  
  - `.iqtree` → summary of models, likelihoods, and bootstrap values.  

- **Key points**  
  - Input sequences must be properly aligned.  
  - The choice of substitution model affects tree accuracy.  
  - Use the outgroup for rooting but include it in the alignment.  
  - Bootstrapping provides branch support values for interpreting the tree.  

</details>

 
  Steps 
##### Step 1: activate iqtree environment
```bash
conda activate iqtree_env
```
##### Step 2: Run the following script
```bash
mkdir -p iqtree_results

iqtree2 -s mafft_results/aligned_consensus.fasta \
        -m GTR+G \
        -bb 1000 \
        -nt 4 \
        -o SRR10828835 \
        -pre iqtree_results/aligned_consensus

```
<details>
<summary>📖 Explanation of IQ-TREE command for aligned consensus sequences</summary>

- `mkdir -p iqtree_results` → creates the directory to store IQ-TREE output files if it doesn’t exist.  
- `iqtree2` → runs IQ-TREE version 2, a program for phylogenetic tree inference.  
- `-s mafft_results/aligned_consensus.fasta` → specifies the input alignment file generated by MAFFT.  
- `-m GTR+G` → sets the substitution model to GTR (General Time Reversible) with Gamma rate heterogeneity.  
- `-bb 1000` → performs 1000 ultrafast bootstrap replicates to assess branch support.  
- `-nt 4` → uses 4 CPU threads for faster computation.  
- `-pre iqtree_results/aligned_consensus` → sets the output file prefix and saves all IQ-TREE results in `iqtree_results/` with this prefix.  

</details>


# 📖 References

WHO. Catalogue of mutations in MTBC and their association with drug resistance, 2nd ed, 2023.

Ngabonziza JCS, et al. A sister lineage of the Mycobacterium tuberculosis complex discovered in Rwanda. Nat Commun, 2020 (Lineage 8).

Coscolla M, et al. Phylogenomics of Mycobacterium africanum lineage 9. Nat Commun, 2021 (Lineage 9).

Phelan J, et al. Whole-genome sequencing for TB: current standards and challenges. Nat Rev Microbiol.

Li H, et al. Fast and accurate short read alignment with Burrows–Wheeler transform. Bioinformatics
